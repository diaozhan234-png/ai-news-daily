#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ—¥æŠ¥æ¨é€è„šæœ¬ - å·¦å³åˆ†æ ä¸­è‹±å¯¹ç…§ç‰ˆï¼ˆä¿®å¤ç‰ˆï¼‰
æ ¸å¿ƒä¿®å¤ï¼šGist Raw URL æ”¹ä¸º htmlpreview.github.io æ¸²æŸ“é“¾æ¥
æ–°å¢æ¥æºï¼šopentools.aiã€VentureBeatã€Forbes
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ä»ä»“åº“Secretsè¯»å–ç¯å¢ƒå˜é‡
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
GIST_TOKEN = os.getenv("AI_NEWS_GIST_TOKEN", "")

# è¶…æ—¶ä¸é‡è¯•é…ç½®
GLOBAL_TIMEOUT = 15
MAX_RETRIES = 3
RANDOM_DELAY = (0.5, 1.2)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S"
)

# è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºæ ¼ï¼Œæ§åˆ¶é•¿åº¦"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.replace("\n", " ").replace("\r", "")
    return text[:800] if len(text) > 800 else text

def retry_wrapper(func):
    """é€šç”¨é‡è¯•è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        for retry in range(MAX_RETRIES):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logging.warning(f"[{func.__name__}] é‡è¯• {retry+1}/{MAX_RETRIES} å¤±è´¥: {str(e)[:50]}")
                time.sleep(random.uniform(*RANDOM_DELAY))
        logging.error(f"[{func.__name__}] æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
        return None
    return wrapper

@retry_wrapper
def baidu_translate(text):
    """ç™¾åº¦ç¿»è¯‘æ ¸å¿ƒå‡½æ•°"""
    if not text or len(text) < 2:
        return {"en": text, "zh": "æ— å†…å®¹"}

    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.warning("âš ï¸ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIï¼Œä½¿ç”¨å¤‡ç”¨ç¿»è¯‘")
        simple_trans = {
            "AI": "äººå·¥æ™ºèƒ½", "LLM": "å¤§è¯­è¨€æ¨¡å‹", "model": "æ¨¡å‹",
            "research": "ç ”ç©¶", "paper": "è®ºæ–‡", "technology": "æŠ€æœ¯",
            "Abstract": "æ‘˜è¦", "Introduction": "å¼•è¨€", "Method": "æ–¹æ³•",
        }
        zh_text = text
        for en, zh in simple_trans.items():
            zh_text = zh_text.replace(en, zh)
        return {"en": text, "zh": zh_text}

    url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    salt = str(random.randint(32768, 65536))
    sign = hashlib.md5((BAIDU_APP_ID + text + salt + BAIDU_SECRET_KEY).encode()).hexdigest()
    params = {
        "q": text, "from": "en", "to": "zh",
        "appid": BAIDU_APP_ID, "salt": salt, "sign": sign
    }
    try:
        resp = requests.get(url, params=params, timeout=GLOBAL_TIMEOUT, verify=False)
        res = resp.json()
        if "trans_result" in res and res["trans_result"]:
            zh_text = res["trans_result"][0]["dst"]
            logging.info(f"âœ… ç¿»è¯‘æˆåŠŸ: {text[:20]} -> {zh_text[:20]}")
            return {"en": text, "zh": zh_text}
        else:
            logging.error(f"âŒ ç¿»è¯‘APIå“åº”å¼‚å¸¸: {res}")
            return {"en": text, "zh": "ç¿»è¯‘å¤±è´¥ï¼Œæ˜¾ç¤ºåŸæ–‡"}
    except Exception as e:
        logging.error(f"âŒ ç¿»è¯‘è¯·æ±‚å¤±è´¥: {str(e)}")
        return {"en": text, "zh": "ç¿»è¯‘å¼‚å¸¸ï¼Œæ˜¾ç¤ºåŸæ–‡"}

@retry_wrapper
def fetch_article_content(url):
    """æŠ“å–æ–‡ç« æ­£æ–‡ï¼ˆå¤šç«™ç‚¹é€‚é…ï¼‰"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=GLOBAL_TIMEOUT, verify=False, allow_redirects=True)
        resp.encoding = resp.apparent_encoding
        soup = BeautifulSoup(resp.text, "html.parser")

        if "arxiv.org" in url:
            content = soup.find("blockquote", class_="abstract mathjax")
        elif "openai.com" in url:
            content = soup.find("div", class_="post-content") or soup.find("main")
        elif "venturebeat.com" in url:
            content = soup.find("div", class_="article-content") or soup.find("article")
        elif "forbes.com" in url:
            content = soup.find("div", class_="article-body") or soup.find("div", class_="content-body")
        elif "opentools.ai" in url:
            content = soup.find("div", class_="post-content") or soup.find("article")
        elif "techcrunch.com" in url:
            content = soup.find("article")
        elif "news.ycombinator.com" in url:
            content = soup.find("div", class_="storytext")
        else:
            paragraphs = soup.find_all("p")[:3]
            content = "\n".join([p.get_text() for p in paragraphs])

        return clean_text(content.get_text()) if hasattr(content, 'get_text') else clean_text(str(content)) if content else "Latest AI industry trends, stay tuned."
    except Exception as e:
        logging.error(f"âŒ æŠ“å–æ­£æ–‡å¤±è´¥: {e}")
        return "Latest AI industry trends, stay tuned."

# ===================== ç”Ÿæˆæ¸²æŸ“å‹å¥½çš„HTML =====================
def generate_bilingual_html(article, index):
    """
    ç”Ÿæˆå·¦å³åˆ†æ çš„ä¸­è‹±å¯¹ç…§HTMLã€‚
    æ³¨æ„ï¼šæ­¤HTMLå°†ä¸Šä¼ è‡³Gistï¼Œå¹¶é€šè¿‡ htmlpreview.github.io æ¸²æŸ“ï¼Œ
    æ‰€ä»¥å¿…é¡»æ˜¯å®Œæ•´è‡ªåŒ…å«çš„HTMLï¼ˆæ— å¤–éƒ¨ä¾èµ–æˆ–åªç”¨CDNå­—ä½“ï¼‰ã€‚
    """
    logging.info(f"\n=== ç”Ÿæˆç¬¬{index}æ¡èµ„è®¯HTML ===")
    title_en  = article.get("title",   {}).get("en", "No Title")
    title_zh  = article.get("title",   {}).get("zh", "æœªè·å–åˆ°ä¸­æ–‡æ ‡é¢˜")
    content_en = article.get("content", {}).get("en", "No Content")
    content_zh = article.get("content", {}).get("zh", "æœªè·å–åˆ°ä¸­æ–‡æ‘˜è¦")
    source    = article.get("source",    "Unknown Source")
    hot_score = article.get("hot_score", "N/A")
    link      = article.get("link",      "#")
    today     = get_today()

    html = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AIèµ„è®¯æ—¥æŠ¥ - {today} | ç¬¬{index}æ¡</title>
<style>
  *{{margin:0;padding:0;box-sizing:border-box}}
  body{{
    font-family:-apple-system,BlinkMacSystemFont,"Segoe UI","Microsoft YaHei","Helvetica Neue",Arial,sans-serif;
    background:#f0f2f5;color:#1a1a1a;line-height:1.8;min-height:100vh;
    display:flex;flex-direction:column;
  }}
  /* â”€â”€ é¡¶éƒ¨ header â”€â”€ */
  .header{{
    background:linear-gradient(135deg,#0052cc 0%,#0066ff 100%);
    color:#fff;padding:24px 32px;
  }}
  .header-inner{{max-width:1100px;margin:0 auto;}}
  .header h1{{font-size:22px;font-weight:700;margin-bottom:6px;letter-spacing:0.02em;}}
  .header-meta{{font-size:13px;opacity:.85;display:flex;gap:16px;flex-wrap:wrap;}}
  .badge{{
    background:rgba(255,255,255,0.22);border-radius:20px;
    padding:2px 10px;font-size:12px;
  }}
  /* â”€â”€ åˆ†æ å®¹å™¨ â”€â”€ */
  .main{{flex:1;max-width:1100px;width:100%;margin:24px auto;padding:0 16px 40px;}}
  .bilingual-wrapper{{
    display:grid;grid-template-columns:1fr 1fr;
    background:#fff;border-radius:12px;
    box-shadow:0 4px 24px rgba(0,0,0,0.10);
    overflow:hidden;
  }}
  /* â”€â”€ æ¯ä¸€åˆ— â”€â”€ */
  .col{{padding:28px 30px;}}
  .col.en{{background:#f8f9fc;border-right:1px solid #e8ecf0;}}
  .col.zh{{background:#ffffff;}}
  .col-lang-tag{{
    display:inline-flex;align-items:center;gap:6px;
    font-size:11px;font-weight:600;letter-spacing:0.12em;text-transform:uppercase;
    color:#0052cc;background:#e8efff;border-radius:4px;
    padding:3px 10px;margin-bottom:16px;
  }}
  .col.zh .col-lang-tag{{color:#c0392b;background:#fdecea;}}
  .col-title{{
    font-size:17px;font-weight:700;line-height:1.55;
    color:#111;margin-bottom:16px;
  }}
  .col-content{{
    font-size:15px;line-height:1.9;color:#444;
  }}
  /* â”€â”€ åº•éƒ¨ footer â”€â”€ */
  .footer{{
    max-width:1100px;width:100%;margin:0 auto;padding:0 16px 32px;
    display:flex;align-items:center;justify-content:space-between;flex-wrap:wrap;gap:12px;
  }}
  .btn{{
    display:inline-block;padding:10px 22px;border-radius:8px;
    font-size:14px;font-weight:600;text-decoration:none;cursor:pointer;
    transition:all .15s ease;
  }}
  .btn-primary{{background:#0052cc;color:#fff;}}
  .btn-primary:hover{{background:#003d99;}}
  .btn-ghost{{background:#fff;color:#444;border:1px solid #d0d5dd;}}
  .btn-ghost:hover{{background:#f5f5f5;}}
  .footer-note{{font-size:12px;color:#999;}}
  /* â”€â”€ å“åº”å¼ï¼šæ‰‹æœºç«–å±è‡ªåŠ¨å †å  â”€â”€ */
  @media(max-width:680px){{
    .bilingual-wrapper{{grid-template-columns:1fr;}}
    .col.en{{border-right:none;border-bottom:1px solid #e8ecf0;}}
    .header{{padding:18px 16px;}}
    .col{{padding:20px 18px;}}
  }}
</style>
</head>
<body>
  <div class="header">
    <div class="header-inner">
      <h1>ğŸ¤– AIèµ„è®¯æ—¥æŠ¥ Â· ä¸­è‹±åŒè¯­å¯¹ç…§</h1>
      <div class="header-meta">
        <span class="badge">ğŸ“… {today}</span>
        <span class="badge">ç¬¬ {index} æ¡</span>
        <span class="badge">ğŸ“¡ {source}</span>
        <span class="badge">ğŸ”¥ çƒ­åº¦ {hot_score}</span>
      </div>
    </div>
  </div>

  <div class="main">
    <div class="bilingual-wrapper">
      <!-- å·¦ï¼šè‹±æ–‡ -->
      <div class="col en">
        <div class="col-lang-tag">ğŸ“ English Original</div>
        <div class="col-title">{title_en}</div>
        <div class="col-content">{content_en}</div>
      </div>
      <!-- å³ï¼šä¸­æ–‡ -->
      <div class="col zh">
        <div class="col-lang-tag">ğŸ“ ä¸­æ–‡ç¿»è¯‘</div>
        <div class="col-title">{title_zh}</div>
        <div class="col-content">{content_zh}</div>
      </div>
    </div>
  </div>

  <div class="footer">
    <div style="display:flex;gap:10px;flex-wrap:wrap;">
      <a class="btn btn-primary" href="{link}" target="_blank">ğŸ”— æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
      <a class="btn btn-ghost" onclick="window.history.back()">â† è¿”å›</a>
    </div>
    <span class="footer-note">æ¥æºï¼š{source} Â· AIèµ„è®¯æ—¥æŠ¥è‡ªåŠ¨æ¨é€</span>
  </div>
</body>
</html>"""
    return html


# ===================== æ ¸å¿ƒä¿®å¤ï¼šGist ä¸Šä¼  + ç”Ÿæˆå¯æ¸²æŸ“ URL =====================
@retry_wrapper
def upload_to_gist(html, index):
    """
    âœ… æ ¸å¿ƒä¿®å¤ï¼š
    ä¸Šä¼ HTMLè‡³GitHub Giståï¼Œä¸å†ç›´æ¥ä½¿ç”¨ Raw URLï¼ˆä¼šæ˜¾ç¤ºæºç ï¼‰ï¼Œ
    è€Œæ˜¯è½¬æ¢ä¸º htmlpreview.github.io å‰ç¼€çš„æ¸²æŸ“é“¾æ¥ï¼Œç‚¹å‡»åç›´æ¥çœ‹åˆ°æ¸²æŸ“é¡µé¢ã€‚

    æ¸²æŸ“URLæ ¼å¼ï¼š
      https://htmlpreview.github.io/?https://gist.githubusercontent.com/{user}/{gist_id}/raw/{filename}
    """
    if GIST_TOKEN and len(GIST_TOKEN) > 10:
        try:
            file_name = f"ai_news_{index}_{get_today()}.html"
            gist_payload = {
                "files": {file_name: {"content": html}},
                "public": True,
                "description": f"AIèµ„è®¯æ—¥æŠ¥ç¬¬{index}æ¡ - {get_today()}"
            }
            gist_headers = {
                "Authorization": f"token {GIST_TOKEN}",
                "Accept": "application/vnd.github.v3+json",
                "User-Agent": "AI-News-Daily/1.0"
            }
            resp = requests.post(
                "https://api.github.com/gists",
                headers=gist_headers,
                json=gist_payload,
                timeout=20
            )
            if resp.status_code == 201:
                res = resp.json()
                gist_id   = res["id"]
                username  = res["owner"]["login"]
                # âœ… å…³é”®ä¿®å¤ï¼šRaw URL â†’ htmlpreview æ¸²æŸ“ URL
                raw_url      = f"https://gist.githubusercontent.com/{username}/{gist_id}/raw/{file_name}"
                rendered_url = f"https://htmlpreview.github.io/?{raw_url}"
                logging.info(f"âœ… Gistä¸Šä¼ æˆåŠŸï¼Œæ¸²æŸ“é“¾æ¥: {rendered_url}")
                return rendered_url
            else:
                logging.error(f"âŒ Gistä¸Šä¼ å¤±è´¥: {resp.status_code} - {resp.text[:150]}")
        except Exception as e:
            logging.error(f"âŒ Gistä¸Šä¼ å¼‚å¸¸: {e}")

    # å…œåº•ï¼šä½¿ç”¨ codepen é£æ ¼çš„ paste æœåŠ¡
    try:
        resp = requests.post(
            "https://api.paste.fo/",
            headers={"X-Auth-Token": "public"},
            json={"content": html, "title": f"AI_News_{index}_{get_today()}", "syntax": "html"},
            timeout=15
        )
        if resp.status_code == 200:
            paste_url = resp.json().get("url", "")
            if paste_url:
                logging.info(f"âœ… å…œåº•æ‰˜ç®¡æˆåŠŸ: {paste_url}")
                return paste_url
    except Exception as e:
        logging.error(f"âŒ å…œåº•æ‰˜ç®¡å¤±è´¥: {e}")

    # æœ€ç»ˆå…œåº•ï¼šè¿”å›ä¸€ä¸ªé€šç”¨é“¾æ¥
    return "https://htmlpreview.github.io/"


# ===================== å¤šæ¸ é“æŠ“å–å‡½æ•° =====================
def crawl_arxiv():
    """æŠ“å–arXiv AIè®ºæ–‡"""
    try:
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title   = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{"title": title, "content": content, "link": entry.link,
                 "source": "arXiv (AIå­¦æœ¯è®ºæ–‡)", "hot_score": round(random.uniform(87, 92), 1)}]
    except Exception as e:
        logging.error(f"âŒ arXivæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_openai():
    """æŠ“å–OpenAIåšå®¢"""
    try:
        feed = feedparser.parse("https://openai.com/blog/rss/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title   = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{"title": title, "content": content, "link": entry.link,
                 "source": "OpenAI Blog", "hot_score": round(random.uniform(85, 90), 1)}]
    except Exception as e:
        logging.error(f"âŒ OpenAIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_google_ai():
    """æŠ“å–Google AI"""
    try:
        feed = feedparser.parse("https://developers.google.com/feeds/ai.rss")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title   = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{"title": title, "content": content, "link": entry.link,
                 "source": "Google AI", "hot_score": round(random.uniform(84, 89), 1)}]
    except Exception as e:
        logging.error(f"âŒ Google AIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_opentools_ai():
    """
    æŠ“å– OpenTools AI â€” å·¥å…·èµ„è®¯èšåˆå¹³å°
    RSS: https://opentools.ai/rss  (å¤‡ç”¨: /feed)
    """
    try:
        feed = feedparser.parse("https://opentools.ai/rss")
        if not feed.entries:
            feed = feedparser.parse("https://opentools.ai/feed")
        if not feed.entries:
            logging.warning("âš ï¸ OpenTools AI RSS æ— æ¡ç›®")
            return []
        entry = feed.entries[0]
        title   = baidu_translate(clean_text(entry.title))
        summary = clean_text(getattr(entry, "summary", "Latest AI tools update"))
        content = baidu_translate(summary or fetch_article_content(entry.link))
        return [{"title": title, "content": content, "link": entry.link,
                 "source": "OpenTools AI", "hot_score": round(random.uniform(82, 87), 1)}]
    except Exception as e:
        logging.error(f"âŒ OpenTools AIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_venturebeat():
    """
    æŠ“å– VentureBeat AI é¢‘é“
    RSS: https://venturebeat.com/category/ai/feed/
    """
    try:
        feed = feedparser.parse("https://venturebeat.com/category/ai/feed/")
        if not feed.entries:
            # å¤‡ç”¨è·¯å¾„
            feed = feedparser.parse("https://venturebeat.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            logging.warning("âš ï¸ VentureBeat RSS æ— æ¡ç›®")
            return []
        entry = feed.entries[0]
        title   = baidu_translate(clean_text(entry.title))
        # ä¼˜å…ˆç”¨ RSS ä¸­çš„ summaryï¼Œå‡å°‘ä¸€æ¬¡ HTTP æŠ“å–
        summary = clean_text(getattr(entry, "summary", ""))
        if len(summary) < 80:
            summary = fetch_article_content(entry.link)
        content = baidu_translate(summary)
        return [{"title": title, "content": content, "link": entry.link,
                 "source": "VentureBeat", "hot_score": round(random.uniform(83, 88), 1)}]
    except Exception as e:
        logging.error(f"âŒ VentureBeatæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_forbes():
    """
    æŠ“å– Forbes AI é¢‘é“
    RSS: https://www.forbes.com/innovation/artificial-intelligence/feed/
    """
    try:
        # Forbes æä¾›å¤šæ¡ RSSï¼Œé€ä¸€å°è¯•
        rss_urls = [
            "https://www.forbes.com/innovation/artificial-intelligence/feed/",
            "https://www.forbes.com/technology/artificial-intelligence/feed/",
            "https://www.forbes.com/sites/technology/feed/",
        ]
        feed = None
        for rss in rss_urls:
            feed = feedparser.parse(rss)
            if feed.entries:
                break
        if not feed or not feed.entries:
            logging.warning("âš ï¸ Forbes RSS æ— æ¡ç›®")
            return []
        entry = feed.entries[0]
        title   = baidu_translate(clean_text(entry.title))
        summary = clean_text(getattr(entry, "summary", ""))
        if len(summary) < 80:
            summary = fetch_article_content(entry.link)
        content = baidu_translate(summary)
        return [{"title": title, "content": content, "link": entry.link,
                 "source": "Forbes", "hot_score": round(random.uniform(86, 91), 1)}]
    except Exception as e:
        logging.error(f"âŒ ForbesæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_hackernews():
    """æŠ“å– HackerNews AI ç›¸å…³çƒ­å¸–"""
    try:
        resp = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", timeout=GLOBAL_TIMEOUT)
        ids = resp.json()[:10]   # æ‰©å¤§æœç´¢èŒƒå›´ä»¥æé«˜å‘½ä¸­ç‡
        for story_id in ids:
            item = requests.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json",
                timeout=GLOBAL_TIMEOUT
            ).json()
            if "title" in item and any(kw in item["title"] for kw in ("AI", "LLM", "GPT", "model", "machine learning")):
                title = baidu_translate(clean_text(item["title"]))
                link  = item.get("url", f"https://news.ycombinator.com/item?id={story_id}")
                text  = clean_text(item.get("text", "Latest AI technology trends"))
                content = baidu_translate(text if text else "Trending AI discussion on HackerNews")
                return [{"title": title, "content": content, "link": link,
                         "source": "HackerNews", "hot_score": round(random.uniform(81, 86), 1)}]
        return []
    except Exception as e:
        logging.error(f"âŒ HackerNewsæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_techcrunch():
    """æŠ“å– TechCrunch AI é¢‘é“"""
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title   = baidu_translate(clean_text(entry.title))
        summary = clean_text(getattr(entry, "summary", ""))
        if len(summary) < 80:
            summary = fetch_article_content(entry.link)
        content = baidu_translate(summary)
        return [{"title": title, "content": content, "link": entry.link,
                 "source": "TechCrunch", "hot_score": round(random.uniform(82, 87), 1)}]
    except Exception as e:
        logging.error(f"âŒ TechCrunchæŠ“å–å¤±è´¥: {e}")
        return []


# ===================== é£ä¹¦æ¨é€å‡½æ•° =====================
def send_to_feishu(articles):
    """æ¨é€è‡³é£ä¹¦ç¾¤ï¼ˆå¡ç‰‡æ¶ˆæ¯ï¼‰"""
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhook")
        return False

    card_elements = []
    for idx, article in enumerate(articles, 1):
        bilingual_html = generate_bilingual_html(article, idx)
        rendered_url   = upload_to_gist(bilingual_html, idx)   # â† å·²æ˜¯æ¸²æŸ“é“¾æ¥

        card_elements.extend([
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": (
                        f"### {idx}. {article['title']['zh']}\n"
                        f"ğŸ“ˆ çƒ­åº¦: {article['hot_score']} | æ¥æº: {article['source']}\n\n"
                        f"**è‹±æ–‡æ ‡é¢˜**: {article['title']['en'][:80]}{'...' if len(article['title']['en'])>80 else ''}\n\n"
                        f"**ä¸­æ–‡æ‘˜è¦**: {article['content']['zh'][:120]}{'...' if len(article['content']['zh'])>120 else ''}"
                    )
                }
            },
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "ğŸ“„ æŸ¥çœ‹ä¸­è‹±å¯¹ç…§"},
                        "type": "primary",
                        "url": rendered_url          # âœ… ç›´æ¥æ‰“å¼€æ¸²æŸ“åçš„åŒè¯­é¡µé¢
                    },
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "ğŸ”— æŸ¥çœ‹è‹±æ–‡åŸæ–‡"},
                        "type": "default",
                        "url": article["link"]
                    }
                ]
            },
            {"tag": "hr"}
        ])

    # ç§»é™¤æœ€åå¤šä½™çš„åˆ†å‰²çº¿
    if card_elements and card_elements[-1].get("tag") == "hr":
        card_elements.pop()

    card = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": f"ğŸ¤– AIèµ„è®¯æ—¥æŠ¥ | {get_today()}"},
            "template": "blue"
        },
        "elements": card_elements
    }

    payload = {"msg_type": "interactive", "card": card}
    try:
        resp = requests.post(FEISHU_WEBHOOK, json=payload, timeout=GLOBAL_TIMEOUT)
        result = resp.json()
        # é£ä¹¦æˆåŠŸå“åº”ï¼šStatusCode==0 æˆ– code==0
        if resp.status_code == 200 and (result.get("StatusCode") == 0 or result.get("code") == 0):
            logging.info("âœ… é£ä¹¦æ¨é€æˆåŠŸ")
            return True
        logging.error(f"âŒ é£ä¹¦æ¨é€å¤±è´¥: {resp.text}")
        return False
    except Exception as e:
        logging.error(f"âŒ é£ä¹¦æ¨é€å¼‚å¸¸: {e}")
        return False


# ===================== ä¸»å‡½æ•° =====================
def main():
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡ŒAIèµ„è®¯æ—¥æŠ¥æ¨é€ä»»åŠ¡")

    # æ‰§è¡Œæ‰€æœ‰æ¸ é“æŠ“å–ï¼ˆé¡ºåº = ä¼˜å…ˆçº§ï¼‰
    crawlers = [
        crawl_arxiv,
        crawl_openai,
        crawl_venturebeat,    # âœ… æ–°å¢
        crawl_forbes,         # âœ… æ–°å¢
        crawl_opentools_ai,   # âœ… æ–°å¢ï¼ˆå·²æœ‰ï¼ŒåŠ å¼ºï¼‰
        crawl_google_ai,
        crawl_hackernews,
        crawl_techcrunch,
    ]

    all_articles = []
    for crawler in crawlers:
        try:
            results = crawler()
            if results:
                all_articles.extend(results)
                logging.info(f"âœ… [{crawler.__name__}] è·å– {len(results)} æ¡")
        except Exception as e:
            logging.error(f"âŒ [{crawler.__name__}] æŠ“å–å‡ºé”™: {e}")

    # è¿‡æ»¤æ— æ•ˆæ¡ç›®
    valid_articles = [a for a in all_articles if a and a.get("title")]

    if not valid_articles:
        logging.warning("âš ï¸ æœªæŠ“å–åˆ°æœ‰æ•ˆèµ„è®¯ï¼Œä½¿ç”¨é»˜è®¤å ä½å†…å®¹")
        valid_articles = [{
            "title":   {"en": "No AI news today", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯"},
            "content": {"en": "No AI news available today.", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯å¯æ¨é€ï¼Œè¯·æ˜å¤©å†æ¥æŸ¥çœ‹ã€‚"},
            "link":    "https://ai.google/",
            "source":  "AI Trends",
            "hot_score": 0.0
        }]

    # æœ€å¤šæ¨é€5æ¡ï¼ŒæŒ‰çƒ­åº¦é™åºæ’åˆ—
    valid_articles = sorted(valid_articles, key=lambda x: float(x.get("hot_score", 0)), reverse=True)[:5]
    logging.info(f"ğŸ“‹ å…±æ¨é€ {len(valid_articles)} æ¡èµ„è®¯")

    send_to_feishu(valid_articles)
    logging.info("ğŸ ä»»åŠ¡æ‰§è¡Œå®Œæˆ")


if __name__ == "__main__":
    main()
