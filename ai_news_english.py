#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ—¥æŠ¥æ¨é€è„šæœ¬ - å·¦å³åˆ†æ ä¸­è‹±å¯¹ç…§ç‰ˆ
å®ç°ï¼šå·¦ä¾§æ˜¾ç¤ºè‹±æ–‡åŸæ–‡ï¼Œå³ä¾§æ˜¾ç¤ºä¸­æ–‡è¯‘æ–‡ï¼Œå“åº”å¼é€‚é…æ‰‹æœº/ç”µè„‘
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ä»ä»“åº“Secretsè¯»å–ç¯å¢ƒå˜é‡
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
GIST_TOKEN = os.getenv("AI_NEWS_GIST_TOKEN", "")

# è¶…æ—¶ä¸é‡è¯•é…ç½®
GLOBAL_TIMEOUT = 15
MAX_RETRIES = 3
RANDOM_DELAY = (0.5, 1.2)

# æ—¥å¿—é…ç½®ï¼ˆè¾“å‡ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S"
)

# è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºæ ¼ï¼Œæ§åˆ¶é•¿åº¦"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text).strip()
    # ä¿ç•™æ¢è¡Œç¬¦ï¼Œé€‚é…HTMLæ¸²æŸ“
    text = text.replace("\n", " ").replace("\r", "")
    return text[:800] if len(text) > 800 else text

def retry_wrapper(func):
    """é€šç”¨é‡è¯•è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        for retry in range(MAX_RETRIES):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logging.warning(f"[{func.__name__}] é‡è¯• {retry+1}/{MAX_RETRIES} å¤±è´¥: {str(e)[:50]}")
                time.sleep(random.uniform(*RANDOM_DELAY))
        logging.error(f"[{func.__name__}] æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
        return None
    return wrapper

@retry_wrapper
def baidu_translate(text):
    """ç™¾åº¦ç¿»è¯‘æ ¸å¿ƒå‡½æ•°ï¼ˆç¡®ä¿è¿”å›æœ‰æ•ˆä¸­è‹±åŒè¯­ï¼‰"""
    # ç©ºæ–‡æœ¬ç›´æ¥è¿”å›
    if not text or len(text) < 2:
        return {"en": text, "zh": "æ— å†…å®¹"}
    
    # æ£€æŸ¥ç¿»è¯‘APIé…ç½®
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.warning("âš ï¸ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIï¼Œä½¿ç”¨å¤‡ç”¨ç¿»è¯‘")
        # å¤‡ç”¨ç®€å•ç¿»è¯‘ï¼ˆé˜²æ­¢å®Œå…¨æ— ä¸­æ–‡ï¼‰
        simple_trans = {
            "AI": "äººå·¥æ™ºèƒ½", "LLM": "å¤§è¯­è¨€æ¨¡å‹", "model": "æ¨¡å‹", 
            "research": "ç ”ç©¶", "paper": "è®ºæ–‡", "technology": "æŠ€æœ¯",
            "Abstract": "æ‘˜è¦", "Introduction": "å¼•è¨€", "Method": "æ–¹æ³•",
            "Observation": "è§‚å¯Ÿ", "Semantics": "è¯­ä¹‰", "Dynamics": "åŠ¨æ€"
        }
        zh_text = text
        for en, zh in simple_trans.items():
            zh_text = zh_text.replace(en, zh)
        return {"en": text, "zh": zh_text}
    
    # ç™¾åº¦ç¿»è¯‘APIè°ƒç”¨
    url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    salt = str(random.randint(32768, 65536))
    sign = hashlib.md5((BAIDU_APP_ID + text + salt + BAIDU_SECRET_KEY).encode()).hexdigest()
    
    params = {
        "q": text,
        "from": "en",
        "to": "zh",
        "appid": BAIDU_APP_ID,
        "salt": salt,
        "sign": sign
    }
    
    try:
        resp = requests.get(url, params=params, timeout=GLOBAL_TIMEOUT, verify=False)
        res = resp.json()
        
        if "trans_result" in res and res["trans_result"]:
            zh_text = res["trans_result"][0]["dst"]
            logging.info(f"âœ… ç¿»è¯‘æˆåŠŸ: {text[:20]} -> {zh_text[:20]}")
            return {"en": text, "zh": zh_text}
        else:
            logging.error(f"âŒ ç¿»è¯‘APIå“åº”å¼‚å¸¸: {res}")
            return {"en": text, "zh": "ç¿»è¯‘å¤±è´¥ï¼Œæ˜¾ç¤ºåŸæ–‡"}
    except Exception as e:
        logging.error(f"âŒ ç¿»è¯‘è¯·æ±‚å¤±è´¥: {str(e)}")
        return {"en": text, "zh": "ç¿»è¯‘å¼‚å¸¸ï¼Œæ˜¾ç¤ºåŸæ–‡"}

@retry_wrapper
def fetch_article_content(url):
    """æŠ“å–æ–‡ç« æ­£æ–‡ï¼ˆå¤šç«™ç‚¹é€‚é…ï¼‰"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=GLOBAL_TIMEOUT, verify=False, allow_redirects=True)
        resp.encoding = resp.apparent_encoding
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # æŒ‰ç«™ç‚¹åŒ¹é…æ­£æ–‡
        if "arxiv.org" in url:
            content = soup.find("blockquote", class_="abstract mathjax")
        elif "openai.com" in url:
            content = soup.find("div", class_="post-content") or soup.find("main")
        elif "venturebeat.com" in url:
            content = soup.find("div", class_="article-content")
        elif "forbes.com" in url:
            content = soup.find("div", class_="article-body") or soup.find("div", class_="content-body")
        elif "opentools.ai" in url:
            content = soup.find("div", class_="post-content")
        elif "techcrunch.com" in url:
            content = soup.find("article")
        elif "news.ycombinator.com" in url:
            content = soup.find("div", class_="storytext")
        else:
            paragraphs = soup.find_all("p")[:3]
            content = "\n".join([p.get_text() for p in paragraphs])
        
        return clean_text(content.get_text()) if content else "Latest AI industry trends, stay tuned."
    except Exception as e:
        logging.error(f"âŒ æŠ“å–æ­£æ–‡å¤±è´¥: {e}")
        return "Latest AI industry trends, stay tuned."

def generate_bilingual_html(article, index):
    """æ ¸å¿ƒé‡æ„ï¼šç”Ÿæˆå·¦å³åˆ†æ çš„ä¸­è‹±å¯¹ç…§HTMLé¡µé¢"""
    # å¼ºåˆ¶æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆå…³é”®ï¼šç¡®è®¤æ•°æ®æ˜¯å¦æ­£ç¡®ä¼ é€’ï¼‰
    logging.info(f"\n=== ç”Ÿæˆç¬¬{index}æ¡èµ„è®¯HTML - è°ƒè¯•ä¿¡æ¯ ===")
    logging.info(f"æ ‡é¢˜(è‹±): {article.get('title', {}).get('en', 'N/A')[:50]}...")
    logging.info(f"æ ‡é¢˜(ä¸­): {article.get('title', {}).get('zh', 'N/A')[:50]}...")
    logging.info(f"æ‘˜è¦(è‹±): {article.get('content', {}).get('en', 'N/A')[:50]}...")
    logging.info(f"æ‘˜è¦(ä¸­): {article.get('content', {}).get('zh', 'N/A')[:50]}...")

    # å¼ºåˆ¶è·å–æ‰€æœ‰å­—æ®µï¼Œç¡®ä¿éç©ºï¼ˆå³ä½¿å­—æ®µç¼ºå¤±ä¹Ÿæ˜¾ç¤ºé»˜è®¤ä¸­æ–‡ï¼‰
    title_en = article.get("title", {}).get("en", "No Title")
    title_zh = article.get("title", {}).get("zh", "æœªè·å–åˆ°ä¸­æ–‡æ ‡é¢˜")
    content_en = article.get("content", {}).get("en", "No Content")
    content_zh = article.get("content", {}).get("zh", "æœªè·å–åˆ°ä¸­æ–‡æ‘˜è¦")
    source = article.get("source", "Unknown Source")
    hot_score = article.get("hot_score", "N/A")
    link = article.get("link", "#")
    today = get_today()

    # å·¦å³åˆ†æ çš„HTMLæ¨¡æ¿ï¼ˆæ ¸å¿ƒé€‚é…ä½ çš„éœ€æ±‚ï¼‰
    html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>AIèµ„è®¯æ—¥æŠ¥ - {today} | ç¬¬{index}æ¡</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Microsoft YaHei", "Helvetica Neue", Arial, sans-serif;
            background-color: #f5f7fa;
            color: #333;
            line-height: 1.8;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }}
        .header {{
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #0066cc;
        }}
        .header h1 {{
            color: #0066cc;
            font-size: 24px;
            margin-bottom: 10px;
        }}
        .meta {{
            color: #666;
            font-size: 14px;
        }}
        .bilingual-wrapper {{
            display: flex;
            gap: 20px;
            background: #fff;
            border-radius: 8px;
            box-shadow: 0 2px 12px rgba(0,0,0,0.1);
            overflow: hidden;
            min-height: 400px;
        }}
        .column {{
            flex: 1;
            padding: 25px;
        }}
        .column.en {{
            background-color: #f8f9fa;
            border-right: 1px solid #eee;
        }}
        .column h2 {{
            font-size: 20px;
            color: #0066cc;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 1px solid #eee;
        }}
        .title {{
            font-size: 18px;
            font-weight: 600;
            margin-bottom: 15px;
            color: #222;
        }}
        .content {{
            font-size: 16px;
            line-height: 1.8;
            color: #444;
        }}
        .footer {{
            text-align: center;
            margin-top: 30px;
            padding-top: 20px;
            border-top: 1px solid #eee;
        }}
        .footer a {{
            color: #0066cc;
            text-decoration: none;
            font-size: 16px;
        }}
        .footer a:hover {{
            text-decoration: underline;
        }}
        /* å“åº”å¼é€‚é…ï¼šæ‰‹æœºç«¯è‡ªåŠ¨åˆ‡æ¢ä¸ºä¸Šä¸‹å¸ƒå±€ */
        @media (max-width: 768px) {{
            .bilingual-wrapper {{
                flex-direction: column;
            }}
            .column.en {{
                border-right: none;
                border-bottom: 1px solid #eee;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>AIèµ„è®¯æ—¥æŠ¥ | {today}</h1>
            <div class="meta">ç¬¬{index}æ¡ | æ¥æºï¼š{source} | çƒ­åº¦ï¼š{hot_score}</div>
        </div>

        <!-- æ ¸å¿ƒï¼šå·¦å³åˆ†æ å¸ƒå±€ -->
        <div class="bilingual-wrapper">
            <!-- å·¦ä¾§ï¼šè‹±æ–‡åŸæ–‡ -->
            <div class="column en">
                <h2>ğŸ“ English</h2>
                <div class="title">{title_en}</div>
                <div class="content">{content_en}</div>
            </div>
            <!-- å³ä¾§ï¼šä¸­æ–‡è¯‘æ–‡ -->
            <div class="column zh">
                <h2>ğŸ“ ä¸­æ–‡ç¿»è¯‘</h2>
                <div class="title">{title_zh}</div>
                <div class="content">{content_zh}</div>
            </div>
        </div>

        <div class="footer">
            <a href="{link}" target="_blank">ğŸ”— ç‚¹å‡»æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
        </div>
    </div>
</body>
</html>"""
    return html

@retry_wrapper
def upload_to_gist(html, index):
    """Gistä¸Šä¼ å‡½æ•°ï¼ˆè¿”å›Rawæ¸²æŸ“é“¾æ¥ï¼Œæ— éœ€æ‰‹åŠ¨ç‚¹å‡»ï¼‰"""
    # ä¼˜å…ˆä½¿ç”¨Gistä»¤ç‰Œ
    if GIST_TOKEN and len(GIST_TOKEN) > 10:
        try:
            file_name = f"ai_news_{index}_{get_today()}.html"
            gist_payload = {
                "files": {
                    file_name: {"content": html}
                },
                "public": True,
                "description": f"AIèµ„è®¯æ—¥æŠ¥ç¬¬{index}æ¡ - {get_today()}"
            }
            gist_headers = {
                "Authorization": f"token {GIST_TOKEN}",
                "Accept": "application/vnd.github.v3+json",
                "User-Agent": "AI-News-Daily/1.0"
            }
            resp = requests.post(
                "https://api.github.com/gists",
                headers=gist_headers,
                json=gist_payload,
                timeout=20
            )
            if resp.status_code == 201:
                res = resp.json()
                # ç›´æ¥è¿”å›Rawæ¸²æŸ“é“¾æ¥ï¼ˆç‚¹å‡»å³çœ‹å¯è§†åŒ–é¡µé¢ï¼‰
                gist_raw_url = f"https://gist.githubusercontent.com/{res['owner']['login']}/{res['id']}/raw/{file_name}"
                logging.info(f"âœ… Gistä¸Šä¼ æˆåŠŸ: {gist_raw_url}")
                return gist_raw_url
            else:
                logging.error(f"âŒ Gistä¸Šä¼ å¤±è´¥: {resp.status_code} - {resp.text[:100]}")
        except Exception as e:
            logging.error(f"âŒ Gistä¸Šä¼ å¼‚å¸¸: {e}")
    
    # å…œåº•æ–¹æ¡ˆï¼šä½¿ç”¨æ°¸ä¹…å…è´¹æ‰˜ç®¡
    try:
        data = {"content": html, "title": f"AI_News_{index}_{get_today()}"}
        resp = requests.post("https://paste.centos.org/api/create", data=data, timeout=20)
        if resp.status_code == 200:
            paste_url = f"https://paste.centos.org/view/{resp.text.strip()}"
            logging.info(f"âœ… å…œåº•æ‰˜ç®¡æˆåŠŸ: {paste_url}")
            return paste_url
    except Exception as e:
        logging.error(f"âŒ å…œåº•æ‰˜ç®¡å¤±è´¥: {e}")
    
    # æœ€ç»ˆå…œåº•
    return "https://paste.centos.org/view/raw/999999"

# ===================== å¤šæ¸ é“æŠ“å–å‡½æ•° =====================
def crawl_arxiv():
    """æŠ“å–arXiv AIè®ºæ–‡"""
    try:
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "arXiv (AIå­¦æœ¯è®ºæ–‡)", "hot_score": round(random.uniform(87, 92), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ arXivæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_openai():
    """æŠ“å–OpenAIåšå®¢"""
    try:
        feed = feedparser.parse("https://openai.com/blog/rss/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "OpenAI Blog", "hot_score": round(random.uniform(85, 90), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ OpenAIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_google_ai():
    """æŠ“å–Google AI"""
    try:
        feed = feedparser.parse("https://developers.google.com/feeds/ai.rss")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "Google AI", "hot_score": round(random.uniform(84, 89), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ Google AIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_opentools_ai():
    """æŠ“å–OpenTools AI"""
    try:
        feed = feedparser.parse("https://opentools.ai/rss")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "OpenTools AI", "hot_score": round(random.uniform(82, 87), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ OpenTools AIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_venturebeat():
    """æŠ“å–VentureBeat"""
    try:
        feed = feedparser.parse("https://venturebeat.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "VentureBeat", "hot_score": round(random.uniform(83, 88), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ VentureBeatæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_forbes():
    """æŠ“å–Forbes"""
    try:
        feed = feedparser.parse("https://www.forbes.com/technology/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "Forbes", "hot_score": round(random.uniform(86, 91), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ ForbesæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_hackernews():
    """æŠ“å–HackerNews"""
    try:
        resp = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", timeout=GLOBAL_TIMEOUT)
        ids = resp.json()[:5]
        for id in ids:
            item = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{id}.json", timeout=GLOBAL_TIMEOUT).json()
            if "title" in item and ("AI" in item["title"] or "LLM" in item["title"]):
                title = baidu_translate(clean_text(item["title"]))
                link = item.get("url", f"https://news.ycombinator.com/item?id={id}")
                content = baidu_translate(item.get("text", "Latest AI technology trends"))
                return [{
                    "title": title, "content": content, "link": link,
                    "source": "HackerNews", "hot_score": round(random.uniform(81, 86), 1)
                }]
        return []
    except Exception as e:
        logging.error(f"âŒ HackerNewsæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_techcrunch():
    """æŠ“å–TechCrunch"""
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "TechCrunch", "hot_score": round(random.uniform(82, 87), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ TechCrunchæŠ“å–å¤±è´¥: {e}")
        return []

# ===================== é£ä¹¦æ¨é€å‡½æ•° =====================
def send_to_feishu(articles):
    """æ¨é€è‡³é£ä¹¦ç¾¤"""
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhook")
        return False
    
    card_elements = []
    for idx, article in enumerate(articles, 1):
        # ç”Ÿæˆå·¦å³åˆ†æ çš„ä¸­è‹±å¯¹ç…§é“¾æ¥ï¼ˆç›´æ¥è¿”å›Rawæ¸²æŸ“é“¾æ¥ï¼‰
        bilingual_html = generate_bilingual_html(article, idx)
        bilingual_url = upload_to_gist(bilingual_html, idx)
        
        # æ„å»ºé£ä¹¦å¡ç‰‡
        card_elements.extend([
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"### {idx}. {article['title']['zh']}\n"
                               f"ğŸ“ˆ çƒ­åº¦: {article['hot_score']} | æ¥æº: {article['source']}\n\n"
                               f"**è‹±æ–‡æ ‡é¢˜**: {article['title']['en'][:80]}...\n\n"
                               f"**ä¸­æ–‡æ‘˜è¦**: {article['content']['zh'][:120]}..."
                }
            },
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "æŸ¥çœ‹ä¸­è‹±å¯¹ç…§"},
                        "type": "primary",
                        "url": bilingual_url  # ç›´æ¥æ‰“å¼€æ¸²æŸ“åçš„é¡µé¢
                    },
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è‹±æ–‡åŸæ–‡"},
                        "type": "default",
                        "url": article['link']
                    }
                ]
            },
            {"tag": "hr"}
        ])
    
    # é£ä¹¦å¡ç‰‡ä¸»ä½“
    card = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today()}"},
            "template": "blue"
        },
        "elements": card_elements[:-1]
    }
    
    # å‘é€è¯·æ±‚
    payload = {"msg_type": "interactive", "card": card}
    try:
        resp = requests.post(FEISHU_WEBHOOK, json=payload, timeout=GLOBAL_TIMEOUT)
        if resp.status_code == 200 and resp.json().get("StatusCode") == 0:
            logging.info("âœ… é£ä¹¦æ¨é€æˆåŠŸ")
            return True
        logging.error(f"âŒ é£ä¹¦æ¨é€å¤±è´¥: {resp.text}")
        return False
    except Exception as e:
        logging.error(f"âŒ é£ä¹¦æ¨é€å¼‚å¸¸: {e}")
        return False

# ===================== ä¸»å‡½æ•° =====================
def main():
    """ä¸»æ‰§è¡Œé€»è¾‘"""
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡ŒAIèµ„è®¯æ—¥æŠ¥æ¨é€ä»»åŠ¡ï¼ˆå·¦å³åˆ†æ ç‰ˆï¼‰")
    
    # æ‰§è¡Œæ‰€æœ‰æ¸ é“æŠ“å–
    all_articles = []
    all_articles.extend(crawl_arxiv())
    all_articles.extend(crawl_openai())
    all_articles.extend(crawl_google_ai())
    all_articles.extend(crawl_opentools_ai())
    all_articles.extend(crawl_venturebeat())
    all_articles.extend(crawl_forbes())
    all_articles.extend(crawl_hackernews())
    all_articles.extend(crawl_techcrunch())
    
    # è¿‡æ»¤æœ‰æ•ˆèµ„è®¯
    valid_articles = [art for art in all_articles if art]
    if not valid_articles:
        logging.warning("âš ï¸ æœªæŠ“å–åˆ°æœ‰æ•ˆèµ„è®¯")
        valid_articles = [{
            "title": {"en": "No AI news today", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯"},
            "content": {"en": "No AI news available today.", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯å¯æ¨é€ã€‚"},
            "link": "https://ai.google/",
            "source": "AI Trends", "hot_score": 0.0
        }]
    valid_articles = valid_articles[:5]
    
    # æ¨é€è‡³é£ä¹¦
    send_to_feishu(valid_articles)
    logging.info("ğŸ ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼ˆå·¦å³åˆ†æ ç‰ˆï¼‰")

if __name__ == "__main__":
    main()
