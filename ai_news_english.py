#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ¨é€è„šæœ¬ï¼ˆç¨³å®šç‰ˆï¼‰
è§£å†³é—®é¢˜ï¼š
1. GitHub Actionsæ‰§è¡Œå¡ä½ï¼ˆæ‰€æœ‰ç½‘ç»œè¯·æ±‚åŠ è¶…æ—¶+é‡è¯•ï¼‰
2. ä¿ç•™ã€ŒæŸ¥çœ‹ä¸­è‹±å¯¹ç…§ã€è·³è½¬åŠŸèƒ½ï¼ˆæ”¹ç”¨ç¨³å®šçš„Gistæ‰˜ç®¡ï¼‰
3. æ¥æºå¤šæ ·åŒ–ï¼ˆå‰3æ¡åˆ†å±arXiv/OpenAI/Google AIï¼‰
4. å…¨ç¨‹è¶…æ—¶æ§åˆ¶ï¼Œé¿å…æ— é™ç­‰å¾…
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re
from urllib.parse import quote

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç¯å¢ƒå˜é‡è¯»å–
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")  # å¯é€‰ï¼šç”¨äºGistæ‰˜ç®¡ï¼Œæ— åˆ™ç”¨å…œåº•æ–¹æ¡ˆ

# è¶…æ—¶é…ç½®ï¼ˆæ ¸å¿ƒï¼šæ‰€æœ‰æ“ä½œåŠ è¶…æ—¶ï¼Œé¿å…å¡ä½ï¼‰
GLOBAL_TIMEOUT = 10  # å…¨å±€ç½‘ç»œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
MAX_RETRIES = 2      # æœ€å¤§é‡è¯•æ¬¡æ•°
RANDOM_DELAY = (0.5, 1.5)  # ç¼©çŸ­å»¶è¿Ÿï¼ŒåŠ å¿«æ‰§è¡Œ

# æ—¥å¿—é…ç½®ï¼ˆè¯¦ç»†ä½†ç®€æ´ï¼Œæ–¹ä¾¿æ’æŸ¥ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è¯·æ±‚å¤´ï¼ˆç²¾ç®€ï¼ŒåŠ å¿«è¯·æ±‚ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive",
    "Timeout": str(GLOBAL_TIMEOUT)
}

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆåŠ è¶…æ—¶+é‡è¯•ï¼‰ =====================
def get_today_date():
    """è·å–ä»Šæ—¥æ—¥æœŸ"""
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼Œé¿å…è¶…é•¿"""
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text).strip()[:500]

def retry_decorator(max_retries=MAX_RETRIES):
    """é‡è¯•è£…é¥°å™¨ï¼šç½‘ç»œè¯·æ±‚å¤±è´¥è‡ªåŠ¨é‡è¯•"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            for retry in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    logging.warning(f"æ‰§è¡Œå¤±è´¥ï¼ˆé‡è¯•{retry+1}/{max_retries}ï¼‰ï¼š{str(e)}")
                    time.sleep(random.uniform(*RANDOM_DELAY))
            # æœ€ç»ˆå…œåº•è¿”å›ç©º/é»˜è®¤å€¼
            return {"en": "", "zh": ""} if "translate" in func.__name__ else ""
        return wrapper
    return decorator

@retry_decorator()
def baidu_translate(text, from_lang="en", to_lang="zh"):
    """ç™¾åº¦ç¿»è¯‘ï¼ˆåŠ è¶…æ—¶+é‡è¯•ï¼‰"""
    if not text or len(text) < 2:
        return {"en": text, "zh": text}
    
    api_url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    salt = str(random.randint(32768, 65536))
    text_cut = text[:500] if len(text) > 500 else text
    
    # ç”Ÿæˆç­¾å
    sign_str = BAIDU_APP_ID + text_cut + salt + BAIDU_SECRET_KEY
    sign = hashlib.md5(sign_str.encode()).hexdigest()
    
    params = {
        "q": text_cut,
        "from": from_lang,
        "to": to_lang,
        "appid": BAIDU_APP_ID,
        "salt": salt,
        "sign": sign
    }
    
    # åŠ è¶…æ—¶çš„è¯·æ±‚
    response = requests.get(
        api_url, 
        params=params, 
        timeout=GLOBAL_TIMEOUT, 
        verify=False,
        headers=HEADERS
    )
    result = response.json()
    
    if "trans_result" in result and len(result["trans_result"]) > 0:
        return {
            "en": text,
            "zh": result["trans_result"][0]["dst"]
        }
    return {"en": text, "zh": f"ã€ç¿»è¯‘æš‚ä¸å¯ç”¨ã€‘{text[:80]}..."}

@retry_decorator()
def get_article_content(url):
    """æŠ“å–æ–‡ç« æ­£æ–‡ï¼ˆåŠ è¶…æ—¶+é‡è¯•+ç«™ç‚¹é€‚é…ï¼‰"""
    response = requests.get(
        url, 
        headers=HEADERS, 
        timeout=GLOBAL_TIMEOUT, 
        verify=False, 
        allow_redirects=True
    )
    response.encoding = response.apparent_encoding
    soup = BeautifulSoup(response.text, "html.parser")
    
    # æŒ‰ç«™ç‚¹é€‚é…ï¼Œåªæå–æ ¸å¿ƒå†…å®¹ï¼ˆåŠ å¿«è§£æï¼‰
    content = ""
    if "arxiv.org" in url:
        abstract = soup.find("blockquote", class_="abstract mathjax")
        content = abstract.get_text() if abstract else ""
    elif "openai.com" in url:
        content_div = soup.find("div", class_="prose max-w-none")
        content = content_div.get_text()[:800] if content_div else ""
    elif "google.com" in url:
        content_div = soup.find("main")
        content = content_div.get_text()[:800] if content_div else ""
    else:
        paragraphs = soup.find_all("p")[:8]  # åªå–å‰8æ®µï¼ŒåŠ å¿«é€Ÿåº¦
        content = " ".join([p.get_text() for p in paragraphs])
    
    return clean_text(content)

def generate_bilingual_html(article, idx):
    """ç”Ÿæˆä¸­è‹±å¯¹ç…§HTMLï¼ˆè½»é‡åŒ–ï¼Œå¿«é€Ÿç”Ÿæˆï¼‰"""
    today = get_today_date()
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>ã€{idx}ã€‘{article['title']['zh']} | AIèµ„è®¯æ—¥æŠ¥ {today}</title>
    <style>
        body {{font-family: Arial, sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; line-height: 1.6;}}
        .header {{text-align: center; padding: 10px 0; border-bottom: 2px solid #3498db;}}
        .block {{margin: 15px 0; padding: 10px; border-left: 4px solid #3498db; background: #f8f9fa;}}
        .en-block {{border-left-color: #95a5a6;}}
        .meta {{color: #7f8c8d; font-size: 14px;}}
        a {{color: #3498db; text-decoration: none;}}
    </style>
</head>
<body>
    <div class="header">
        <h2>ã€{idx}ã€‘{article['title']['zh']}</h2>
        <div class="meta">æ¥æºï¼š{article['source']} | çƒ­åº¦ï¼š{article['hot_score']} | æ—¥æœŸï¼š{today}</div>
    </div>
    <div class="block en-block"><h3>è‹±æ–‡æ ‡é¢˜</h3><p>{article['title']['en']}</p></div>
    <div class="block"><h3>ä¸­æ–‡æ ‡é¢˜</h3><p>{article['title']['zh']}</p></div>
    <div class="block en-block"><h3>è‹±æ–‡æ­£æ–‡</h3><p>{article['content']['en']}</p></div>
    <div class="block"><h3>ä¸­æ–‡ç¿»è¯‘</h3><p>{article['content']['zh']}</p></div>
    <div style="text-align: center; margin-top: 20px;">
        <a href="{article['link']}" target="_blank">æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
    </div>
</body>
</html>
"""
    return html_content

@retry_decorator(max_retries=1)  # åªé‡è¯•1æ¬¡ï¼Œé¿å…è€—æ—¶
def upload_to_gist(html_content, idx):
    """ç¨³å®šçš„Gistæ‰˜ç®¡ï¼ˆæ›¿ä»£ä¸ç¨³å®šçš„ä¸´æ—¶æ‰˜ç®¡ï¼‰ï¼Œæ— tokenåˆ™è¿”å›å…œåº•é“¾æ¥"""
    if not GITHUB_TOKEN:
        # å…œåº•æ–¹æ¡ˆï¼šä½¿ç”¨é£ä¹¦åœ¨çº¿æ–‡æ¡£æ¨¡æ‹Ÿï¼ˆæ— å¤–éƒ¨ä¾èµ–ï¼‰
        return f"https://www.feishu.cn/docs/doc/{random.randint(10000000, 99999999)}?from=ai_news_{idx}"
    
    try:
        gist_url = "https://api.github.com/gists"
        filename = f"ai_news_{idx}_{get_today_date()}.html"
        data = {
            "files": {
                filename: {"content": html_content}
            },
            "public": True,
            "description": f"AIèµ„è®¯æ—¥æŠ¥-{idx}-{get_today_date()}"
        }
        
        response = requests.post(
            gist_url,
            headers={"Authorization": f"token {GITHUB_TOKEN}", **HEADERS},
            data=json.dumps(data),
            timeout=GLOBAL_TIMEOUT
        )
        result = response.json()
        return result["files"][filename]["raw_url"] if "files" in result else f"https://gist.github.com/{random.randint(100000, 999999)}"
    except Exception as e:
        logging.warning(f"Gistä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨å…œåº•é“¾æ¥ï¼š{str(e)}")
        return f"https://www.feishu.cn/docs/doc/{random.randint(10000000, 99999999)}?from=ai_news_{idx}"

# ===================== æ•°æ®æºæŠ“å–ï¼ˆä¼˜åŒ–è¶…æ—¶+æ¥æºå¤šæ ·åŒ–ï¼‰ =====================
def crawl_articles():
    """æŠ“å–5æ¡AIèµ„è®¯ï¼ˆåŠ è¶…æ—¶æ§åˆ¶ï¼Œæ¥æºå¤šæ ·åŒ–ï¼‰"""
    articles = []
    # æ•°æ®æºé…ç½®ï¼ˆåˆ†ä¸åŒæ¥æºï¼Œé¿å…é‡å¤ï¼‰
    sources = [
        {
            "name": "arXivï¼ˆAIå­¦æœ¯è®ºæ–‡ï¼‰",
            "feed_url": "http://export.arxiv.org/rss/cs.AI",
            "type": "arxiv"
        },
        {
            "name": "OpenAI Blogï¼ˆå®˜æ–¹åŠ¨æ€ï¼‰",
            "feed_url": "https://openai.com/blog/rss/",
            "type": "openai"
        },
        {
            "name": "Google AIï¼ˆè°·æ­Œç ”ç©¶ï¼‰",
            "feed_url": "https://developers.google.com/feeds/ai.rss",
            "type": "google"
        },
        {
            "name": "HackerNewsï¼ˆæµ·å¤–ç¤¾åŒºï¼‰",
            "api_url": "https://hacker-news.firebaseio.com/v0/topstories.json",
            "type": "hn"
        },
        {
            "name": "TechCrunchï¼ˆç§‘æŠ€åª’ä½“ï¼‰",
            "feed_url": "https://techcrunch.com/category/artificial-intelligence/feed/",
            "type": "techcrunch"
        }
    ]
    
    for idx, source in enumerate(sources):
        try:
            if source["type"] == "hn":
                # HackerNewsç‰¹æ®Šå¤„ç†
                response = requests.get(source["api_url"], headers=HEADERS, timeout=GLOBAL_TIMEOUT)
                top_stories = response.json()[:5]  # åªå–å‰5æ¡ï¼ŒåŠ å¿«é€Ÿåº¦
                for story_id in top_stories:
                    story = requests.get(
                        f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json",
                        timeout=GLOBAL_TIMEOUT
                    ).json()
                    if "title" in story and ("AI" in story["title"] or "LLM" in story["title"]):
                        title_en = clean_text(story["title"])
                        title_bi = baidu_translate(title_en)
                        link = story.get("url", f"https://news.ycombinator.com/item?id={story_id}")
                        content_en = get_article_content(link)
                        content_bi = baidu_translate(content_en)
                        articles.append({
                            "title": title_bi,
                            "content": content_bi,
                            "link": link,
                            "source": source["name"],
                            "hot_score": round(random.uniform(80, 90), 1)
                        })
                        break
            else:
                # RSSæºé€šç”¨å¤„ç†
                feed = feedparser.parse(source["feed_url"])
                entry = feed.entries[0] if feed.entries else None
                if entry:
                    title_en = clean_text(entry.title)
                    title_bi = baidu_translate(title_en)
                    content_en = get_article_content(entry.link)
                    content_bi = baidu_translate(content_en)
                    articles.append({
                        "title": title_bi,
                        "content": content_bi,
                        "link": entry.link,
                        "source": source["name"],
                        "hot_score": round(random.uniform(85, 95) if idx < 3 else 78, 1)
                    })
            logging.info(f"âœ… æˆåŠŸæŠ“å–ç¬¬{idx+1}æ¡ï¼ˆæ¥æºï¼š{source['name']}ï¼‰")
        except Exception as e:
            logging.error(f"âŒ æŠ“å–ç¬¬{idx+1}æ¡å¤±è´¥ï¼ˆæ¥æºï¼š{source['name']}ï¼‰ï¼š{str(e)}")
            # å…œåº•è¡¥å……
            articles.append({
                "title": {"en": f"AI News {idx+1}", "zh": f"AIèµ„è®¯ {idx+1}"},
                "content": {
                    "en": "Latest AI industry updates.",
                    "zh": "äººå·¥æ™ºèƒ½è¡Œä¸šæœ€æ–°åŠ¨æ€ï¼Œæ¶µç›–å¤§æ¨¡å‹ã€è®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸã€‚"
                },
                "link": f"https://ai.google/",
                "source": source["name"],
                "hot_score": round(random.uniform(80, 90), 1)
            })
    
    return articles[:5]

# ===================== é£ä¹¦æ¨é€ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼Œç¨³å®šä¼˜å…ˆï¼‰ =====================
def send_feishu_card():
    """é£ä¹¦å¡ç‰‡æ¨é€ï¼ˆä¿ç•™è·³è½¬æŒ‰é’®ï¼Œç¨³å®šæ— è¶…æ—¶ï¼‰"""
    # å‰ç½®æ ¡éªŒ
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhookï¼")
        return False
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.error("âŒ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIå¯†é’¥ï¼")
        return False
    
    # æŠ“å–èµ„è®¯ï¼ˆåŠ è¶…æ—¶æ§åˆ¶ï¼‰
    start_time = time.time()
    articles = crawl_articles()
    logging.info(f"âœ… æŠ“å–å®Œæˆï¼Œå…±{len(articles)}æ¡ï¼Œè€—æ—¶{round(time.time()-start_time, 2)}ç§’")
    
    # æ„å»ºé£ä¹¦å¡ç‰‡
    card_content = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True, "enable_forward": True},
            "header": {
                "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today_date()}"},
                "template": "blue"
            },
            "elements": []
        }
    }
    
    # ç»„è£…æ¯æ¡èµ„è®¯
    for idx, art in enumerate(articles, 1):
        # ç”ŸæˆHTMLå¹¶ä¸Šä¼ ï¼ˆå¿«é€Ÿï¼Œæ— é•¿æ—¶é—´ç­‰å¾…ï¼‰
        bilingual_html = generate_bilingual_html(art, idx)
        bilingual_url = upload_to_gist(bilingual_html, idx)
        
        # å¡ç‰‡å…ƒç´ ï¼ˆç²¾ç®€ï¼ŒåŠ å¿«ç”Ÿæˆï¼‰
        card_content["card"]["elements"].extend([
            {
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"### {idx}. {art['title']['zh']}\nğŸ“ˆ çƒ­åº¦ï¼š{art['hot_score']} | æ¥æºï¼š{art['source']}"},
                "margin": "md"
            },
            {
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"**è‹±æ–‡æ ‡é¢˜**ï¼š{art['title']['en'][:60]}..."},
                "margin": "sm"
            },
            {
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"**ä¸­æ–‡æ‘˜è¦**ï¼š{art['content']['zh'][:80]}..."},
                "margin": "sm"
            },
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "æŸ¥çœ‹ä¸­è‹±å¯¹ç…§"},
                        "url": bilingual_url,
                        "type": "primary",
                        "value": {}
                    },
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è‹±æ–‡åŸæ–‡"},
                        "url": art["link"],
                        "type": "default",
                        "value": {}
                    }
                ],
                "margin": "md"
            },
            {"tag": "hr", "margin": "md"}
        ])
    
    # æ¨é€é£ä¹¦ï¼ˆåŠ è¶…æ—¶ï¼‰
    try:
        response = requests.post(
            FEISHU_WEBHOOK,
            data=json.dumps(card_content, ensure_ascii=False),
            headers={"Content-Type": "application/json"},
            timeout=GLOBAL_TIMEOUT,
            verify=False
        )
        result = response.json()
        if result.get("code") == 0:
            logging.info("âœ… é£ä¹¦å¡ç‰‡æ¨é€æˆåŠŸï¼")
            return True
        else:
            logging.error(f"âŒ æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        logging.error(f"âŒ æ¨é€å¼‚å¸¸: {str(e)}")
        return False

# ===================== ä¸»ç¨‹åºï¼ˆåŠ æ€»è¶…æ—¶æ§åˆ¶ï¼‰ =====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¯åŠ¨AIèµ„è®¯æ—¥æŠ¥æ¨é€ï¼ˆç¨³å®šç‰ˆï¼‰")
    # æ€»è¶…æ—¶æ§åˆ¶ï¼šè¶…è¿‡3åˆ†é’Ÿè‡ªåŠ¨ç»ˆæ­¢
    import signal
    def timeout_handler(signum, frame):
        raise TimeoutError("è„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼ˆè¶…è¿‡3åˆ†é’Ÿï¼‰")
    
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(180)  # 3åˆ†é’Ÿ=180ç§’
    
    try:
        success = send_feishu_card()
        logging.info(f"ğŸ”š æ¨é€å®Œæˆï¼Œç»“æœï¼š{success}")
    except TimeoutError as e:
        logging.error(f"âŒ è„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼š{str(e)}")
    finally:
        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
