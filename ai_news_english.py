#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‹±æ–‡AIèµ„è®¯èšåˆæ¨é€è„šæœ¬ï¼ˆGitHubéƒ¨ç½²ç‰ˆï¼‰
åŠŸèƒ½ï¼šå¤šæºæŠ“å–+ä¸­è‹±å¯¹ç…§+é£ä¹¦æ¨é€
é€‚é…ï¼šGitHub Actionså®šæ—¶è¿è¡Œ/æ‰‹åŠ¨è§¦å‘
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser  # RSSè§£æåº“

# ===================== åŸºç¡€é…ç½® =====================
# å±è”½ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç¯å¢ƒå˜é‡è¯»å–ï¼ˆGitHub Secretsé…ç½®ï¼‰
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")  # é£ä¹¦Webhook
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")      # ç™¾åº¦ç¿»è¯‘APP ID
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")  # ç™¾åº¦ç¿»è¯‘å¯†é’¥

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé™ä½åçˆ¬ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

# éšæœºå»¶è¿Ÿï¼ˆ1-3ç§’ï¼‰ï¼Œé™ä½è¯·æ±‚é¢‘ç‡
RANDOM_DELAY = (1, 3)

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today_date():
    """è·å–ä»Šæ—¥æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰"""
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼ˆå»ç©ºæ ¼ã€æ¢è¡Œã€å¤šä½™ç¬¦å·ï¼‰"""
    if not text:
        return ""
    return text.replace("\n", "").replace("\r", "").replace("  ", "").strip()

def baidu_translate(text, from_lang="en", to_lang="zh"):
    """
    ç™¾åº¦ç¿»è¯‘APIï¼ˆä¸­è‹±å¯¹ç…§ï¼‰
    :param text: å¾…ç¿»è¯‘æ–‡æœ¬
    :param from_lang: æºè¯­è¨€ï¼ˆen/zhï¼‰
    :param to_lang: ç›®æ ‡è¯­è¨€ï¼ˆzh/enï¼‰
    :return: ç¿»è¯‘ç»“æœ {en: åŸæ–‡, zh: è¯‘æ–‡}
    """
    # ç©ºæ–‡æœ¬ç›´æ¥è¿”å›
    if not text or len(text) < 2:
        return {"en": text, "zh": text}
    
    # ç™¾åº¦ç¿»è¯‘APIå‚æ•°ç»„è£…
    api_url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    salt = str(random.randint(32768, 65536))
    sign_str = BAIDU_APP_ID + text + salt + BAIDU_SECRET_KEY
    sign = hashlib.md5(sign_str.encode()).hexdigest()
    
    params = {
        "q": text,
        "from": from_lang,
        "to": to_lang,
        "appid": BAIDU_APP_ID,
        "salt": salt,
        "sign": sign
    }
    
    try:
        # éšæœºå»¶è¿Ÿï¼Œé¿å…APIé™æµ
        time.sleep(random.uniform(*RANDOM_DELAY))
        response = requests.get(
            api_url,
            params=params,
            timeout=10,
            verify=False
        )
        result = response.json()
        
        # ç¿»è¯‘æˆåŠŸ
        if "trans_result" in result and len(result["trans_result"]) > 0:
            return {
                "en": text if from_lang == "en" else result["trans_result"][0]["dst"],
                "zh": result["trans_result"][0]["dst"] if from_lang == "en" else text
            }
        else:
            logging.warning(f"ç™¾åº¦ç¿»è¯‘è¿”å›å¼‚å¸¸: {result}")
            return {"en": text, "zh": f"ã€ç¿»è¯‘å¤±è´¥ã€‘{text}"}
    except Exception as e:
        logging.error(f"ç™¾åº¦ç¿»è¯‘è°ƒç”¨å¤±è´¥: {str(e)}")
        return {"en": text, "zh": f"ã€ç¿»è¯‘å¼‚å¸¸ã€‘{text}"}

# ===================== å¤šæºæŠ“å–å‡½æ•°ï¼ˆè‹±æ–‡ä¼˜å…ˆï¼‰ =====================
def crawl_academic():
    """ğŸ“š å­¦æœ¯å‰æ²¿ï¼ˆarXiv CS.AIä¸“æ  - RSSï¼‰"""
    try:
        # éšæœºå»¶è¿Ÿ
        time.sleep(random.uniform(*RANDOM_DELAY))
        # arXiv CS.AIæœ€æ–°è®ºæ–‡RSS
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        if feed.entries and len(feed.entries) > 0:
            entry = feed.entries[0]  # æœ€æ–°è®ºæ–‡
            title_bi = baidu_translate(clean_text(entry.title))
            summary_bi = baidu_translate(clean_text(entry.summary[:150]))  # æ‘˜è¦ä»…å–å‰150å­—ç¬¦
            
            return {
                "type": "ğŸ“š å­¦æœ¯å‰æ²¿ / Academic Frontier",
                "title": title_bi,
                "summary": summary_bi,
                "link": entry.link,
                "time": get_today_date()
            }
        else:
            return {
                "type": "ğŸ“š å­¦æœ¯å‰æ²¿ / Academic Frontier",
                "title": {"en": "No academic updates today", "zh": "ä»Šæ—¥æš‚æ— å­¦æœ¯å‰æ²¿æ›´æ–°"},
                "summary": {"en": "", "zh": ""},
                "link": "",
                "time": ""
            }
    except Exception as e:
        logging.error(f"æŠ“å–å­¦æœ¯å‰æ²¿å¤±è´¥: {str(e)}")
        return {
            "type": "ğŸ“š å­¦æœ¯å‰æ²¿ / Academic Frontier",
            "title": {"en": "Academic crawl failed", "zh": "å­¦æœ¯å‰æ²¿æŠ“å–å¤±è´¥"},
            "summary": {"en": "", "zh": ""},
            "link": "",
            "time": ""
        }

def crawl_official_blog():
    """ğŸ¢ å®˜æ–¹åšå®¢ï¼ˆOpenAI Blog - RSSï¼‰"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        # OpenAIå®˜æ–¹åšå®¢RSS
        feed = feedparser.parse("https://openai.com/blog/rss/")
        if feed.entries and len(feed.entries) > 0:
            entry = feed.entries[0]
            title_bi = baidu_translate(clean_text(entry.title))
            summary_bi = baidu_translate(clean_text(entry.summary[:150]))
            
            return {
                "type": "ğŸ¢ å®˜æ–¹åšå®¢ / Official Blog",
                "title": title_bi,
                "summary": summary_bi,
                "link": entry.link,
                "time": get_today_date()
            }
        else:
            return {
                "type": "ğŸ¢ å®˜æ–¹åšå®¢ / Official Blog",
                "title": {"en": "No official blog updates today", "zh": "ä»Šæ—¥æš‚æ— å®˜æ–¹åšå®¢æ›´æ–°"},
                "summary": {"en": "", "zh": ""},
                "link": "",
                "time": ""
            }
    except Exception as e:
        logging.error(f"æŠ“å–å®˜æ–¹åšå®¢å¤±è´¥: {str(e)}")
        return {
            "type": "ğŸ¢ å®˜æ–¹åšå®¢ / Official Blog",
            "title": {"en": "Official blog crawl failed", "zh": "å®˜æ–¹åšå®¢æŠ“å–å¤±è´¥"},
            "summary": {"en": "", "zh": ""},
            "link": "",
            "time": ""
        }

def crawl_community():
    """ğŸ’¬ æµ·å¤–ç¤¾åŒºï¼ˆHackerNews AIç›¸å…³ - APIï¼‰"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        # HackerNews Top AIç›¸å…³å¸–å­
        response = requests.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json",
            headers=HEADERS,
            timeout=10
        )
        top_stories = response.json()[:5]  # å–å‰5æ¡
        
        # æŠ“å–ç¬¬ä¸€æ¡AIç›¸å…³å¸–å­
        for story_id in top_stories:
            story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            story = requests.get(story_url, headers=HEADERS, timeout=5).json()
            if "title" in story and ("AI" in story["title"] or "LLM" in story["title"]):
                title_bi = baidu_translate(clean_text(story["title"]))
                link = story.get("url", f"https://news.ycombinator.com/item?id={story_id}")
                
                return {
                    "type": "ğŸ’¬ æµ·å¤–ç¤¾åŒº / Overseas Community",
                    "title": title_bi,
                    "summary": {"en": "Top AI discussion on HackerNews", "zh": "HackerNewsçƒ­é—¨AIè®¨è®º"},
                    "link": link,
                    "time": get_today_date()
                }
        
        return {
            "type": "ğŸ’¬ æµ·å¤–ç¤¾åŒº / Overseas Community",
            "title": {"en": "No AI community updates today", "zh": "ä»Šæ—¥æš‚æ— æµ·å¤–ç¤¾åŒºAIæ›´æ–°"},
            "summary": {"en": "", "zh": ""},
            "link": "",
            "time": ""
        }
    except Exception as e:
        logging.error(f"æŠ“å–æµ·å¤–ç¤¾åŒºå¤±è´¥: {str(e)}")
        return {
            "type": "ğŸ’¬ æµ·å¤–ç¤¾åŒº / Overseas Community",
            "title": {"en": "Community crawl failed", "zh": "æµ·å¤–ç¤¾åŒºæŠ“å–å¤±è´¥"},
            "summary": {"en": "", "zh": ""},
            "link": "",
            "time": ""
        }

def crawl_social():
    """ğŸ“± ç¤¾åª’èšåˆï¼ˆTwitter AIè¶‹åŠ¿ - Nitter RSSï¼‰"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        # Nitterï¼ˆTwitteré•œåƒï¼‰OpenAI RSS
        feed = feedparser.parse("https://nitter.net/OpenAI/rss")
        if feed.entries and len(feed.entries) > 0:
            entry = feed.entries[0]
            title_bi = baidu_translate(clean_text(entry.title))
            link = entry.link.replace("nitter.net", "twitter.com")  # æ›¿æ¢ä¸ºåŸTwitteré“¾æ¥
            
            return {
                "type": "ğŸ“± ç¤¾åª’èšåˆ / Social Media",
                "title": title_bi,
                "summary": {"en": "Latest AI trend on Twitter", "zh": "Twitteræœ€æ–°AIè¶‹åŠ¿"},
                "link": link,
                "time": get_today_date()
            }
        else:
            return {
                "type": "ğŸ“± ç¤¾åª’èšåˆ / Social Media",
                "title": {"en": "No social media updates today", "zh": "ä»Šæ—¥æš‚æ— ç¤¾åª’AIæ›´æ–°"},
                "summary": {"en": "", "zh": ""},
                "link": "",
                "time": ""
            }
    except Exception as e:
        logging.error(f"æŠ“å–ç¤¾åª’èšåˆå¤±è´¥: {str(e)}")
        return {
            "type": "ğŸ“± ç¤¾åª’èšåˆ / Social Media",
            "title": {"en": "Social media crawl failed", "zh": "ç¤¾åª’èšåˆæŠ“å–å¤±è´¥"},
            "summary": {"en": "", "zh": ""},
            "link": "",
            "time": ""
        }

# ===================== æ„å»ºåŒè¯­æ¨é€å†…å®¹ =====================
def build_feishu_content():
    """æ„å»ºé£ä¹¦åŒè¯­æ¨é€å†…å®¹"""
    # æŠ“å–å››ç±»ä¿¡æ¯
    academic = crawl_academic()
    official_blog = crawl_official_blog()
    community = crawl_community()
    social = crawl_social()
    
    # ç»„è£…åŒè¯­å†…å®¹
    content = f"ğŸ“® Daily AI Digest / æ¯æ—¥AIè‹±æ–‡ç²¾é€‰ï¼ˆ{get_today_date()}ï¼‰\n\n"
    
    for idx, item in enumerate([academic, official_blog, community, social], 1):
        content += f"{idx}. ã€{item['type']}ã€‘\n"
        content += f"   English Title: {item['title']['en']}\n"
        content += f"   ä¸­æ–‡æ ‡é¢˜ï¼š{item['title']['zh']}\n"
        if item['summary']['en']:
            content += f"   English Summary: {item['summary']['en'][:100]}...\n"
            content += f"   ä¸­æ–‡æ‘˜è¦ï¼š{item['summary']['zh'][:100]}...\n"
        if item['link']:
            content += f"   Source Link / æ¥æºé“¾æ¥ï¼š{item['link']}\n"
        content += "\n"
    
    return content.strip()

# ===================== é£ä¹¦æ¨é€ =====================
def send_to_feishu():
    """æ¨é€åŒè¯­å†…å®¹åˆ°é£ä¹¦"""
    # æ ¡éªŒå¿…è¦é…ç½®
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhookï¼")
        return False
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.error("âŒ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIå¯†é’¥ï¼")
        return False
    
    try:
        payload = {
            "msg_type": "post",
            "content": {
                "post": {
                    "zh_cn": {
                        "title": f"Daily AI Digest / æ¯æ—¥AIè‹±æ–‡ç²¾é€‰ï¼ˆ{get_today_date()}ï¼‰",
                        "content": [[{"tag": "text", "text": build_feishu_content()}]]
                    }
                }
            }
        }
        response = requests.post(
            FEISHU_WEBHOOK,
            data=json.dumps(payload, ensure_ascii=False),
            headers={"Content-Type": "application/json"},
            timeout=10,
            verify=False
        )
        result = response.json()
        if result.get("code") == 0:
            logging.info("âœ… é£ä¹¦åŒè¯­æ¨é€æˆåŠŸï¼")
            return True
        else:
            logging.error(f"âŒ æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        logging.error(f"âŒ æ¨é€å¼‚å¸¸: {str(e)}")
        return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡Œè‹±æ–‡AIèµ„è®¯èšåˆæ¨é€ä»»åŠ¡")
    success = send_to_feishu()
    logging.info("ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå®Œæˆ" if success else "ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
