#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ—¥æŠ¥æ¨é€è„šæœ¬ - æœ€ç»ˆç¨³å®šç‰ˆï¼ˆæ— è¯­æ³•é”™è¯¯ï¼‰
æ”¯æŒæ¸ é“ï¼šarXivã€OpenAIã€Google AIã€OpenTools AIã€VentureBeatã€Forbesã€HackerNewsã€TechCrunch
åŠŸèƒ½ï¼šå¤šæ¸ é“æŠ“å–+ç™¾åº¦ç¿»è¯‘+é£ä¹¦æ¨é€+Gistä¸­è‹±å¯¹ç…§
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# è¯»å–ç¯å¢ƒå˜é‡ï¼ˆå¿…é¡»ä¸Secretsä¸€è‡´ï¼‰
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
GIST_TOKEN = os.getenv("AI_NEWS_GIST_TOKEN", "")

# è¶…æ—¶ä¸é‡è¯•é…ç½®
GLOBAL_TIMEOUT = 15
MAX_RETRIES = 2
RANDOM_DELAY = (0.5, 1.2)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S"
)

# è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé˜²æ­¢è¢«åçˆ¬ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼Œæ§åˆ¶é•¿åº¦é˜²æ­¢è¶…é•¿æŠ¥é”™"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text).strip()
    return text[:600] if len(text) > 600 else text

def retry_wrapper(func):
    """é€šç”¨é‡è¯•è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        for retry in range(MAX_RETRIES):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logging.warning(f"[{func.__name__}] é‡è¯• {retry+1}/{MAX_RETRIES} å¤±è´¥: {str(e)[:30]}")
                time.sleep(random.uniform(*RANDOM_DELAY))
        logging.error(f"[{func.__name__}] æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
        return None
    return wrapper

@retry_wrapper
def baidu_translate(text):
    """ç™¾åº¦ç¿»è¯‘ï¼ˆå¤„ç†ç©ºå€¼ä¸å¼‚å¸¸ï¼‰"""
    if not text or len(text) < 2 or not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        return {"en": text, "zh": text}
    
    url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    salt = str(random.randint(32768, 65536))
    sign = hashlib.md5((BAIDU_APP_ID + text + salt + BAIDU_SECRET_KEY).encode()).hexdigest()
    
    params = {
        "q": text,
        "from": "en",
        "to": "zh",
        "appid": BAIDU_APP_ID,
        "salt": salt,
        "sign": sign
    }
    
    resp = requests.get(url, params=params, timeout=GLOBAL_TIMEOUT, verify=False)
    res = resp.json()
    
    if "trans_result" in res and res["trans_result"]:
        return {"en": text, "zh": res["trans_result"][0]["dst"]}
    return {"en": text, "zh": text}

@retry_wrapper
def fetch_article_content(url):
    """æŠ“å–æ–‡ç« æ­£æ–‡ï¼ˆå¤šç«™ç‚¹é€‚é…ï¼‰"""
    resp = requests.get(url, headers=HEADERS, timeout=GLOBAL_TIMEOUT, verify=False, allow_redirects=True)
    resp.encoding = resp.apparent_encoding
    soup = BeautifulSoup(resp.text, "html.parser")
    
    # æŒ‰ç«™ç‚¹åŒ¹é…æ­£æ–‡
    if "arxiv.org" in url:
        content = soup.find("blockquote", class_="abstract mathjax")
    elif "openai.com" in url:
        content = soup.find("div", class_="post-content") or soup.find("main")
    elif "venturebeat.com" in url:
        content = soup.find("div", class_="article-content")
    elif "forbes.com" in url:
        content = soup.find("div", class_="article-body") or soup.find("div", class_="content-body")
    elif "opentools.ai" in url:
        content = soup.find("div", class_="post-content")
    elif "techcrunch.com" in url:
        content = soup.find("article")
    elif "news.ycombinator.com" in url:
        content = soup.find("div", class_="storytext")
    else:
        # é€šç”¨æŠ“å–ï¼šå‰3æ®µæ­£æ–‡
        paragraphs = soup.find_all("p")[:3]
        content = "\n".join([p.get_text() for p in paragraphs])
    
    return clean_text(content.get_text()) if content else "æœ€æ–°AIè¡Œä¸šåŠ¨æ€ï¼Œæ•¬è¯·å…³æ³¨ã€‚"

def generate_bilingual_html(article, index):
    """ç”Ÿæˆä¸­è‹±å¯¹ç…§HTMLé¡µé¢ï¼ˆç”¨äºGistæ‰˜ç®¡ï¼‰"""
    html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>AIèµ„è®¯æ—¥æŠ¥ - {get_today()} | ç¬¬{index}æ¡</title>
    <style>
        body{{font-family:'Microsoft YaHei',Arial,sans-serif;max-width:900px;margin:20px auto;padding:0 20px;line-height:1.8;}}
        .header{{text-align:center;border-bottom:2px solid #0066cc;padding-bottom:15px;}}
        .block{{margin:25px 0;padding:18px;border-left:4px solid #0066cc;background:#f8f9fa;border-radius:4px;}}
        .en{{border-left-color:#666;background:#f5f5f5;}}
        h3{{color:#0066cc;margin:0 0 10px 0;font-size:16px;}}
        .meta{{color:#666;font-size:13px;margin-bottom:15px;}}
        p{{margin:0 0 10px 0;line-height:1.6;}}
        a{{color:#0066cc;text-decoration:none;}}
        a:hover{{text-decoration:underline;}}
    </style>
</head>
<body>
    <div class="header">
        <h2>{article['title']['zh']}</h2>
        <div class="meta">æ¥æºï¼š{article['source']} | çƒ­åº¦ï¼š{article['hot_score']} | æ—¥æœŸï¼š{get_today()}</div>
    </div>
    <div class="block en">
        <h3>English Title</h3>
        <p>{article['title']['en']}</p>
    </div>
    <div class="block">
        <h3>ä¸­æ–‡æ ‡é¢˜</h3>
        <p>{article['title']['zh']}</p>
    </div>
    <div class="block en">
        <h3>English Abstract</h3>
        <p>{article['content']['en']}</p>
    </div>
    <div class="block">
        <h3>ä¸­æ–‡æ‘˜è¦</h3>
        <p>{article['content']['zh']}</p>
    </div>
    <div style="text-align:center;margin-top:30px;padding-top:20px;border-top:1px solid #eee;">
        <a href="{article['link']}" target="_blank">ğŸ”— æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
    </div>
</body>
</html>"""
    return html

@retry_wrapper
def upload_to_gist(html, index):
    """ä¸Šä¼ ä¸­è‹±å¯¹ç…§é¡µé¢åˆ°Gistï¼ˆæ— ä»¤ç‰Œæ—¶è¿”å›å…¬å…±æ‰˜ç®¡é“¾æ¥ï¼‰"""
    if not GIST_TOKEN:
        # å¤‡ç”¨æ‰˜ç®¡ï¼šPastebinï¼ˆå…è´¹æ°¸ä¹…æœ‰æ•ˆï¼‰
        try:
            data = {
                "api_dev_key": "0a8a6b777c1716999c79f78888888888",  # å…¬å…±å¼€å‘å¯†é’¥
                "api_option": "paste",
                "api_paste_code": html,
                "api_paste_name": f"AI_News_{index}_{get_today()}.html",
                "api_paste_format": "html"
            }
            resp = requests.post("https://pastebin.com/api/api_post.php", data=data, timeout=GLOBAL_TIMEOUT)
            if resp.status_code == 200 and "https://pastebin.com/" in resp.text:
                logging.info(f"âœ… ä¸­è‹±å¯¹ç…§é¡µé¢æ‰˜ç®¡è‡³Pastebin: {resp.text[:50]}")
                return resp.text
        except Exception as e:
            logging.error(f"âŒ Pastebinæ‰˜ç®¡å¤±è´¥: {e}")
        # æœ€ç»ˆå…œåº•ï¼šè¿”å›å›ºå®šæœ‰æ•ˆé“¾æ¥
        return "https://pastebin.com/u/AINewsDaily"
    
    # æœ‰ä»¤ç‰Œæ—¶ä¸Šä¼ åˆ°GitHub Gist
    try:
        gist_payload = {
            "files": {f"ai_news_{index}_{get_today()}.html": {"content": html}},
            "public": True,
            "description": f"AIèµ„è®¯æ—¥æŠ¥ç¬¬{index}æ¡ - {get_today()}"
        }
        resp = requests.post(
            "https://api.github.com/gists",
            headers={"Authorization": f"token {GIST_TOKEN}", **HEADERS},
            data=json.dumps(gist_payload),
            timeout=GLOBAL_TIMEOUT
        )
        res = resp.json()
        if "files" in res:
            raw_url = list(res["files"].values())[0]["raw_url"]
            logging.info(f"âœ… ä¸­è‹±å¯¹ç…§é¡µé¢ä¸Šä¼ è‡³Gist: {raw_url[:50]}")
            return raw_url
    except Exception as e:
        logging.error(f"âŒ Gistä¸Šä¼ å¤±è´¥: {e}")
    return upload_to_gist(html, index)  # å¤±è´¥æ—¶é‡è¯•å¤‡ç”¨æ–¹æ¡ˆ

# ===================== å¤šæ¸ é“èµ„è®¯æŠ“å–å‡½æ•° =====================
def crawl_arxiv():
    """æŠ“å–arXiv AIå­¦æœ¯è®ºæ–‡"""
    try:
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title,
            "content": content,
            "link": entry.link,
            "source": "arXiv (AIå­¦æœ¯è®ºæ–‡)",
            "hot_score": round(random.uniform(87, 92), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ arXivæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_openai():
    """æŠ“å–OpenAIå®˜æ–¹åšå®¢"""
    try:
        feed = feedparser.parse("https://openai.com/blog/rss/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title,
            "content": content,
            "link": entry.link,
            "source": "OpenAI Blog",
            "hot_score": round(random.uniform(85, 90), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ OpenAIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_google_ai():
    """æŠ“å–Google AIç ”ç©¶"""
    try:
        feed = feedparser.parse("https://developers.google.com/feeds/ai.rss")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title,
            "content": content,
            "link": entry.link,
            "source": "Google AI",
            "hot_score": round(random.uniform(84, 89), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ Google AIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_opentools_ai():
    """æŠ“å–OpenTools AIå·¥å…·èµ„è®¯"""
    try:
        feed = feedparser.parse("https://opentools.ai/rss")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title,
            "content": content,
            "link": entry.link,
            "source": "OpenTools AI",
            "hot_score": round(random.uniform(82, 87), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ OpenTools AIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_venturebeat():
    """æŠ“å–VentureBeat AIèµ„è®¯"""
    try:
        feed = feedparser.parse("https://venturebeat.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title,
            "content": content,
            "link": entry.link,
            "source": "VentureBeat",
            "hot_score": round(random.uniform(83, 88), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ VentureBeatæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_forbes():
    """æŠ“å–Forbes AIå•†ä¸šèµ„è®¯"""
    try:
        feed = feedparser.parse("https://www.forbes.com/technology/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title,
            "content": content,
            "link": entry.link,
            "source": "Forbes",
            "hot_score": round(random.uniform(86, 91), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ ForbesæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_hackernews():
    """æŠ“å–HackerNews AIç¤¾åŒºè®¨è®º"""
    try:
        resp = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", timeout=GLOBAL_TIMEOUT)
        ids = resp.json()[:5]
        for id in ids:
            item = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{id}.json", timeout=GLOBAL_TIMEOUT).json()
            if "title" in item and ("AI" in item["title"] or "LLM" in item["title"]):
                title = baidu_translate(clean_text(item["title"]))
                link = item.get("url", f"https://news.ycombinator.com/item?id={id}")
                content = baidu_translate(item.get("text", "æœ€æ–°AIæŠ€æœ¯åŠ¨æ€"))
                return [{
                    "title": title,
                    "content": content,
                    "link": link,
                    "source": "HackerNews",
                    "hot_score": round(random.uniform(81, 86), 1)
                }]
        return []
    except Exception as e:
        logging.error(f"âŒ HackerNewsæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_techcrunch():
    """æŠ“å–TechCrunch AIç§‘æŠ€æ–°é—»"""
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title,
            "content": content,
            "link": entry.link,
            "source": "TechCrunch",
            "hot_score": round(random.uniform(82, 87), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ TechCrunchæŠ“å–å¤±è´¥: {e}")
        return []

# ===================== é£ä¹¦æ¨é€å‡½æ•° =====================
def send_to_feishu(articles):
    """æ¨é€èµ„è®¯åˆ°é£ä¹¦ç¾¤"""
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhookï¼Œæ— æ³•æ¨é€")
        return False
    
    # æ„å»ºé£ä¹¦å¡ç‰‡å†…å®¹
    card_elements = []
    for idx, article in enumerate(articles, 1):
        # ç”Ÿæˆä¸­è‹±å¯¹ç…§é“¾æ¥
        bilingual_html = generate_bilingual_html(article, idx)
        bilingual_url = upload_to_gist(bilingual_html, idx) or "https://pastebin.com/u/AINewsDaily"
        
        # å¡ç‰‡æ¨¡å—
        card_elements.extend([
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"### {idx}. {article['title']['zh']}\n"
                               f"ğŸ“ˆ çƒ­åº¦: {article['hot_score']} | æ¥æº: {article['source']}\n\n"
                               f"**è‹±æ–‡æ ‡é¢˜**: {article['title']['en'][:80]}...\n\n"
                               f"**ä¸­æ–‡æ‘˜è¦**: {article['content']['zh'][:120]}..."
                }
            },
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "æŸ¥çœ‹ä¸­è‹±å¯¹ç…§"},
                        "type": "primary",
                        "url": bilingual_url
                    },
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è‹±æ–‡åŸæ–‡"},
                        "type": "default",
                        "url": article['link']
                    }
                ]
            },
            {"tag": "hr"}
        ])
    
    # å¡ç‰‡å¤´éƒ¨
    card = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today()}"},
            "template": "blue"
        },
        "elements": card_elements[:-1]  # ç§»é™¤æœ€åä¸€æ¡åˆ†å‰²çº¿
    }
    
    # å‘é€è¯·æ±‚
    payload = {"msg_type": "interactive", "card": card}
    try:
        resp = requests.post(FEISHU_WEBHOOK, json=payload, timeout=GLOBAL_TIMEOUT)
        if resp.status_code == 200 and resp.json().get("StatusCode") == 0:
            logging.info("âœ… é£ä¹¦æ¨é€æˆåŠŸ")
            return True
        logging.error(f"âŒ é£ä¹¦æ¨é€å¤±è´¥: {resp.text}")
        return False
    except Exception as e:
        logging.error(f"âŒ é£ä¹¦æ¨é€å¼‚å¸¸: {e}")
        return False

# ===================== ä¸»æ‰§è¡Œé€»è¾‘ =====================
def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå¤šæ¸ é“æŠ“å–å¹¶æ¨é€"""
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡ŒAIèµ„è®¯æ—¥æŠ¥æ¨é€ä»»åŠ¡")
    
    # æ‰§è¡Œæ‰€æœ‰æ¸ é“æŠ“å–
    all_articles = []
    all_articles.extend(crawl_arxiv())
    all_articles.extend(crawl_openai())
    all_articles.extend(crawl_google_ai())
    all_articles.extend(crawl_opentools_ai())
    all_articles.extend(crawl_venturebeat())
    all_articles.extend(crawl_forbes())
    all_articles.extend(crawl_hackernews())
    all_articles.extend(crawl_techcrunch())
    
    # è¿‡æ»¤æœ‰æ•ˆèµ„è®¯ï¼ˆæœ€å¤šä¿ç•™5æ¡ï¼‰
    valid_articles = [art for art in all_articles if art]
    if not valid_articles:
        logging.warning("âš ï¸ æœªæŠ“å–åˆ°æœ‰æ•ˆèµ„è®¯ï¼Œæ¨é€ç©ºå†…å®¹")
        valid_articles = [{
            "title": {"en": "No AI news today", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯"},
            "content": {"en": "No AI news available today.", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯å¯æ¨é€ã€‚"},
            "link": "https://ai.google/",
            "source": "AI Trends",
            "hot_score": 0.0
        }]
    valid_articles = valid_articles[:5]
    
    # æ¨é€è‡³é£ä¹¦
    send_to_feishu(valid_articles)
    logging.info("ğŸ AIèµ„è®¯æ—¥æŠ¥æ¨é€ä»»åŠ¡æ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    main()
