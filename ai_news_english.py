#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ¨é€è„šæœ¬ï¼ˆç›®æ ‡æ ·å¼ç‰ˆï¼‰
åŠŸèƒ½ï¼šTopçƒ­ç‚¹ç­›é€‰+é£ä¹¦å¡ç‰‡æ ¼å¼+è‹±æ–‡å…¨æ–‡åŒè¯­å¯¹ç…§
é€‚é…GitHub Actionséƒ¨ç½²
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç¯å¢ƒå˜é‡è¯»å–
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

RANDOM_DELAY = (1, 3)

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today_date():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text).strip()[:200]  # ç²¾ç®€æ–‡æœ¬é•¿åº¦

def baidu_translate(text, from_lang="en", to_lang="zh"):
    """ç™¾åº¦ç¿»è¯‘ï¼ˆæ”¯æŒé•¿æ–‡æœ¬åˆ†æ®µï¼‰"""
    if not text or len(text) < 2:
        return {"en": text, "zh": text}
    
    # åˆ†æ®µç¿»è¯‘ï¼ˆé¿å…è¶…è¿‡APIå­—ç¬¦é™åˆ¶ï¼‰
    max_len = 500
    text_segments = [text[i:i+max_len] for i in range(0, len(text), max_len)]
    en_segments = []
    zh_segments = []
    
    for seg in text_segments:
        api_url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
        salt = str(random.randint(32768, 65536))
        sign_str = BAIDU_APP_ID + seg + salt + BAIDU_SECRET_KEY
        sign = hashlib.md5(sign_str.encode()).hexdigest()
        
        params = {
            "q": seg,
            "from": from_lang,
            "to": to_lang,
            "appid": BAIDU_APP_ID,
            "salt": salt,
            "sign": sign
        }
        
        try:
            time.sleep(random.uniform(*RANDOM_DELAY))
            response = requests.get(api_url, params=params, timeout=10, verify=False)
            result = response.json()
            if "trans_result" in result and len(result["trans_result"]) > 0:
                en_segments.append(seg)
                zh_segments.append(result["trans_result"][0]["dst"])
            else:
                en_segments.append(seg)
                zh_segments.append(f"ã€ç¿»è¯‘å¤±è´¥ã€‘{seg}")
        except Exception as e:
            logging.error(f"ç¿»è¯‘åˆ†æ®µå¤±è´¥: {str(e)}")
            en_segments.append(seg)
            zh_segments.append(f"ã€ç¿»è¯‘å¼‚å¸¸ã€‘{seg}")
    
    return {
        "en": "".join(en_segments),
        "zh": "".join(zh_segments)
    }

def get_article_content(url):
    """æŠ“å–è‹±æ–‡æ–‡ç« æ­£æ–‡ï¼ˆç®€åŒ–ç‰ˆï¼Œé€‚é…ä¸»æµè‹±æ–‡ç«™ï¼‰"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        response = requests.get(url, headers=HEADERS, timeout=15, verify=False, allow_redirects=True)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æå–æ­£æ–‡ï¼ˆé€‚é…arXiv/OpenAI Blog/HackerNews/Twitterï¼‰
        if "arxiv.org" in url:
            content = soup.find("blockquote", class_="abstract mathjax")
        elif "openai.com" in url:
            content = soup.find("div", class_="prose max-w-none")
        elif "hackernews.com" in url:
            content = soup.find("div", class_="comment-tree")
        elif "twitter.com" in url or "nitter.net" in url:
            content = soup.find("div", class_="tweet-content")
        else:
            content = soup.find("main") or soup.find("article")
        
        if content:
            return clean_text(content.get_text())
        else:
            return "No content available"
    except Exception as e:
        logging.error(f"æŠ“å–æ–‡ç« æ­£æ–‡å¤±è´¥: {str(e)}")
        return "Content crawl failed"

def generate_bilingual_page(articles):
    """ç”ŸæˆåŒè¯­å¯¹ç…§å†…å®¹ï¼ˆæ¨¡æ‹Ÿç½‘é¡µé“¾æ¥æ ¼å¼ï¼‰"""
    bilingual_content = f"<h1>AIèµ„è®¯æ—¥æŠ¥ | {get_today_date()}</h1>"
    for idx, art in enumerate(articles, 1):
        bilingual_content += f"""
        <h2>{idx}. {art['title']['en']}</h2>
        <p><b>ä¸­æ–‡æ ‡é¢˜ï¼š</b>{art['title']['zh']}</p>
        <h3>è‹±æ–‡åŸæ–‡</h3>
        <p>{art['content']['en']}</p>
        <h3>ä¸­æ–‡ç¿»è¯‘</h3>
        <p>{art['content']['zh']}</p>
        <p><b>æ¥æºé“¾æ¥ï¼š</b><a href="{art['link']}">{art['link']}</a></p>
        <hr>
        """
    # ç®€åŒ–ï¼šè¿”å›æ ¼å¼åŒ–æ–‡æœ¬ï¼ˆå¦‚éœ€çœŸå®ç½‘é¡µå¯å¯¹æ¥GitHub Pagesï¼Œæ­¤å¤„å…ˆé€‚é…é£ä¹¦å±•ç¤ºï¼‰
    return bilingual_content

# ===================== å¤šæºæŠ“å–+çƒ­ç‚¹ç­›é€‰ =====================
def crawl_and_rank_articles():
    """æŠ“å–å¹¶ç­›é€‰Top 2çƒ­ç‚¹èµ„è®¯ï¼ˆåŒ¹é…æ¡ˆä¾‹æ ·å¼ï¼‰"""
    # 1. å­¦æœ¯å‰æ²¿ï¼ˆarXivï¼‰
    academic = crawl_academic()
    # 2. OpenAIåšå®¢
    official = crawl_official_blog()
    # 3. HackerNewsç¤¾åŒº
    community = crawl_community()
    # 4. ç¤¾åª’èšåˆ
    social = crawl_social()
    
    # æ•´åˆæ‰€æœ‰æœ‰æ•ˆèµ„è®¯
    all_articles = []
    for art in [academic, official, community, social]:
        if art["link"] and art["title"]["en"] != "No updates today":
            # æŠ“å–æ­£æ–‡å¹¶ç¿»è¯‘
            content_en = get_article_content(art["link"])
            content_bi = baidu_translate(content_en)
            # éšæœºç”Ÿæˆçƒ­åº¦å€¼ï¼ˆæ¨¡æ‹Ÿæ¡ˆä¾‹ï¼‰
            hot_score = round(random.uniform(80, 95), 1)
            
            all_articles.append({
                "type": art["type"],
                "title": art["title"],
                "content": content_bi,
                "link": art["link"],
                "hot_score": hot_score,
                "source": art["source"]
            })
    
    # æŒ‰çƒ­åº¦ç­›é€‰Top 2
    all_articles.sort(key=lambda x: x["hot_score"], reverse=True)
    return all_articles[:2]

def crawl_academic():
    """ğŸ“š å­¦æœ¯å‰æ²¿"""
    try:
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        if feed.entries:
            entry = feed.entries[0]
            title_bi = baidu_translate(clean_text(entry.title))
            return {
                "type": "å­¦æœ¯å‰æ²¿",
                "title": title_bi,
                "link": entry.link,
                "source": "arXiv",
                "title_en": entry.title
            }
        return {"type": "å­¦æœ¯å‰æ²¿", "title": {"en": "No updates today", "zh": "æš‚æ— æ›´æ–°"}, "link": "", "source": ""}
    except Exception as e:
        logging.error(f"å­¦æœ¯æŠ“å–å¤±è´¥: {e}")
        return {"type": "å­¦æœ¯å‰æ²¿", "title": {"en": "Crawl failed", "zh": "æŠ“å–å¤±è´¥"}, "link": "", "source": ""}

def crawl_official_blog():
    """ğŸ¢ å®˜æ–¹åšå®¢"""
    try:
        feed = feedparser.parse("https://openai.com/blog/rss/")
        if feed.entries:
            entry = feed.entries[0]
            title_bi = baidu_translate(clean_text(entry.title))
            return {
                "type": "å®˜æ–¹åšå®¢",
                "title": title_bi,
                "link": entry.link,
                "source": "OpenAI Blog"
            }
        return {"type": "å®˜æ–¹åšå®¢", "title": {"en": "No updates today", "zh": "æš‚æ— æ›´æ–°"}, "link": "", "source": ""}
    except Exception as e:
        logging.error(f"åšå®¢æŠ“å–å¤±è´¥: {e}")
        return {"type": "å®˜æ–¹åšå®¢", "title": {"en": "Crawl failed", "zh": "æŠ“å–å¤±è´¥"}, "link": "", "source": ""}

def crawl_community():
    """ğŸ’¬ æµ·å¤–ç¤¾åŒº"""
    try:
        response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", headers=HEADERS, timeout=10)
        top_stories = response.json()[:5]
        for story_id in top_stories:
            story = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json", timeout=5).json()
            if "title" in story and ("AI" in story["title"] or "LLM" in story["title"]):
                title_bi = baidu_translate(clean_text(story["title"]))
                link = story.get("url", f"https://news.ycombinator.com/item?id={story_id}")
                return {
                    "type": "æµ·å¤–ç¤¾åŒº",
                    "title": title_bi,
                    "link": link,
                    "source": "HackerNews"
                }
        return {"type": "æµ·å¤–ç¤¾åŒº", "title": {"en": "No updates today", "zh": "æš‚æ— æ›´æ–°"}, "link": "", "source": ""}
    except Exception as e:
        logging.error(f"ç¤¾åŒºæŠ“å–å¤±è´¥: {e}")
        return {"type": "æµ·å¤–ç¤¾åŒº", "title": {"en": "Crawl failed", "zh": "æŠ“å–å¤±è´¥"}, "link": "", "source": ""}

def crawl_social():
    """ğŸ“± ç¤¾åª’èšåˆ"""
    try:
        feed = feedparser.parse("https://nitter.net/OpenAI/rss")
        if feed.entries:
            entry = feed.entries[0]
            title_bi = baidu_translate(clean_text(entry.title))
            link = entry.link.replace("nitter.net", "twitter.com")
            return {
                "type": "ç¤¾åª’èšåˆ",
                "title": title_bi,
                "link": link,
                "source": "Twitter/OpenAI"
            }
        return {"type": "ç¤¾åª’èšåˆ", "title": {"en": "No updates today", "zh": "æš‚æ— æ›´æ–°"}, "link": "", "source": ""}
    except Exception as e:
        logging.error(f"ç¤¾åª’æŠ“å–å¤±è´¥: {e}")
        return {"type": "ç¤¾åª’èšåˆ", "title": {"en": "Crawl failed", "zh": "æŠ“å–å¤±è´¥"}, "link": "", "source": ""}

# ===================== é£ä¹¦å¯Œæ–‡æœ¬æ¨é€ï¼ˆåŒ¹é…ç›®æ ‡æ ·å¼ï¼‰ =====================
def send_feishu_card():
    """é£ä¹¦å¡ç‰‡å¼æ¨é€ï¼ˆåŒ¹é…æ¡ˆä¾‹æ ·å¼ï¼‰"""
    if not FEISHU_WEBHOOK or not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.error("é…ç½®ç¼ºå¤±ï¼")
        return False
    
    # æŠ“å–Top 2çƒ­ç‚¹
    top_articles = crawl_and_rank_articles()
    if not top_articles:
        logging.warning("æ— çƒ­ç‚¹èµ„è®¯å¯æ¨é€")
        return False
    
    # æ„å»ºé£ä¹¦å¡ç‰‡å†…å®¹
    card_content = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            "header": {
                "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today_date()}"},
                "template": "blue"
            },
            "elements": []
        }
    }
    
    # æ·»åŠ Topèµ„è®¯æ¡ç›®
    for idx, art in enumerate(top_articles, 1):
        # æ¡ç›®1ï¼šæ ‡é¢˜+çƒ­åº¦
        element1 = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"{idx}. **{art['title']['zh']}** \n ğŸ“ˆ çƒ­åº¦ï¼š{art['hot_score']} | æ¥æºï¼š{art['source']}"
            }
        }
        # æ¡ç›®2ï¼šè‹±æ–‡æ ‡é¢˜+æŸ¥çœ‹è¯¦æƒ…é“¾æ¥
        element2 = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"ğŸ“ è‹±æ–‡åŸæ–‡ï¼š{art['title']['en'][:50]}... \n ğŸ”— [æŸ¥çœ‹è¯¦æƒ…ï¼ˆä¸­è‹±å¯¹ç…§ï¼‰]({art['link']})"
            }
        }
        # åˆ†å‰²çº¿
        element3 = {"tag": "hr"}
        
        card_content["card"]["elements"].extend([element1, element2, element3])
    
    # æ·»åŠ å…¨æ–‡å¯¹ç…§é“¾æ¥ï¼ˆæ¨¡æ‹Ÿç½‘é¡µï¼‰
    bilingual_page = generate_bilingual_page(top_articles)
    card_content["card"]["elements"].append({
        "tag": "div",
        "text": {
            "tag": "lark_md",
            "content": f"ğŸ“– [æŸ¥çœ‹å®Œæ•´ä¸­è‹±æ–‡å¯¹ç…§ç½‘é¡µ](https://your-github-pages-url/{get_today_date()}.html)"
        }
    })
    
    try:
        response = requests.post(
            FEISHU_WEBHOOK,
            data=json.dumps(card_content, ensure_ascii=False),
            headers={"Content-Type": "application/json"},
            timeout=10,
            verify=False
        )
        result = response.json()
        if result.get("code") == 0:
            logging.info("âœ… é£ä¹¦å¡ç‰‡æ¨é€æˆåŠŸï¼")
            return True
        else:
            logging.error(f"æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        logging.error(f"æ¨é€å¼‚å¸¸: {e}")
        return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡ŒAIèµ„è®¯æ—¥æŠ¥æ¨é€ï¼ˆç›®æ ‡æ ·å¼ç‰ˆï¼‰")
    send_feishu_card()
    logging.info("ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
