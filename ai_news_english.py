#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ¨é€è„šæœ¬ï¼ˆæœ€ç»ˆç‰ˆï¼‰
åŠŸèƒ½ï¼šTop2çƒ­ç‚¹ç­›é€‰+é£ä¹¦å¡ç‰‡æ ¼å¼+GitHub PagesåŒè¯­ç½‘é¡µ
é€‚é…åœ°å€ï¼šhttps://diaozhan234-png.github.io/ai-news-daily/
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re
import subprocess

# ===================== åŸºç¡€é…ç½® =====================
# å±è”½ä¸å®‰å…¨è¯·æ±‚è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç¯å¢ƒå˜é‡è¯»å–ï¼ˆGitHub Secretsé…ç½®ï¼‰
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")  # é£ä¹¦Webhook
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")      # ç™¾åº¦ç¿»è¯‘APP ID
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")  # ç™¾åº¦ç¿»è¯‘å¯†é’¥

# ä½ çš„GitHub Pagesåœ°å€ï¼ˆå·²å›ºå®šé…ç½®ï¼‰
GITHUB_PAGES_URL = "https://diaozhan234-png.github.io/ai-news-daily"

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé™ä½åçˆ¬ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

# éšæœºå»¶è¿Ÿï¼ˆ1-3ç§’ï¼‰ï¼Œé™ä½è¯·æ±‚é¢‘ç‡
RANDOM_DELAY = (1, 3)

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today_date():
    """è·å–ä»Šæ—¥æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰"""
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼ˆå»ç©ºæ ¼ã€æ¢è¡Œã€å¤šä½™ç¬¦å·ï¼‰"""
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text).strip()[:500]  # ç²¾ç®€æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¿‡é•¿

def baidu_translate(text, from_lang="en", to_lang="zh"):
    """
    ç™¾åº¦ç¿»è¯‘APIï¼ˆä¸­è‹±å¯¹ç…§ï¼Œæ”¯æŒé•¿æ–‡æœ¬åˆ†æ®µï¼‰
    :param text: å¾…ç¿»è¯‘æ–‡æœ¬
    :param from_lang: æºè¯­è¨€ï¼ˆen/zhï¼‰
    :param to_lang: ç›®æ ‡è¯­è¨€ï¼ˆzh/enï¼‰
    :return: ç¿»è¯‘ç»“æœ {en: åŸæ–‡, zh: è¯‘æ–‡}
    """
    # ç©ºæ–‡æœ¬ç›´æ¥è¿”å›
    if not text or len(text) < 2:
        return {"en": text, "zh": text}
    
    # åˆ†æ®µç¿»è¯‘ï¼ˆé¿å…è¶…è¿‡APIå­—ç¬¦é™åˆ¶ï¼‰
    max_len = 500
    text_segments = [text[i:i+max_len] for i in range(0, len(text), max_len)]
    en_segments = []
    zh_segments = []
    
    for seg in text_segments:
        api_url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
        salt = str(random.randint(32768, 65536))
        sign_str = BAIDU_APP_ID + seg + salt + BAIDU_SECRET_KEY
        sign = hashlib.md5(sign_str.encode()).hexdigest()
        
        params = {
            "q": seg,
            "from": from_lang,
            "to": to_lang,
            "appid": BAIDU_APP_ID,
            "salt": salt,
            "sign": sign
        }
        
        try:
            # éšæœºå»¶è¿Ÿï¼Œé¿å…APIé™æµ
            time.sleep(random.uniform(*RANDOM_DELAY))
            response = requests.get(
                api_url,
                params=params,
                timeout=10,
                verify=False
            )
            result = response.json()
            
            # ç¿»è¯‘æˆåŠŸ
            if "trans_result" in result and len(result["trans_result"]) > 0:
                en_segments.append(seg)
                zh_segments.append(result["trans_result"][0]["dst"])
            else:
                logging.warning(f"ç™¾åº¦ç¿»è¯‘è¿”å›å¼‚å¸¸: {result}")
                en_segments.append(seg)
                zh_segments.append(f"ã€ç¿»è¯‘å¤±è´¥ã€‘{seg}")
        except Exception as e:
            logging.error(f"ç™¾åº¦ç¿»è¯‘è°ƒç”¨å¤±è´¥: {str(e)}")
            en_segments.append(seg)
            zh_segments.append(f"ã€ç¿»è¯‘å¼‚å¸¸ã€‘{seg}")
    
    return {
        "en": "".join(en_segments),
        "zh": "".join(zh_segments)
    }

def get_article_content(url):
    """æŠ“å–è‹±æ–‡æ–‡ç« æ­£æ–‡ï¼ˆé€‚é…arXiv/OpenAI Blog/HackerNews/Twitterï¼‰"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        response = requests.get(
            url,
            headers=HEADERS,
            timeout=15,
            verify=False,
            allow_redirects=True
        )
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æå–ä¸åŒç«™ç‚¹çš„æ­£æ–‡
        if "arxiv.org" in url:
            content = soup.find("blockquote", class_="abstract mathjax")
        elif "openai.com" in url:
            content = soup.find("div", class_="prose max-w-none")
        elif "hackernews.com" in url:
            content = soup.find("div", class_="comment-tree")
        elif "twitter.com" in url or "nitter.net" in url:
            content = soup.find("div", class_="tweet-content")
        else:
            content = soup.find("main") or soup.find("article")
        
        if content:
            return clean_text(content.get_text())
        else:
            return "No content available (æš‚æ— æ­£æ–‡å†…å®¹)"
    except Exception as e:
        logging.error(f"æŠ“å–æ–‡ç« æ­£æ–‡å¤±è´¥: {str(e)}")
        return "Content crawl failed (æ­£æ–‡æŠ“å–å¤±è´¥)"

def save_bilingual_html(articles):
    """ç”ŸæˆåŒè¯­HTMLæ–‡ä»¶å¹¶æäº¤åˆ°GitHubï¼ˆé€‚é…ä½ çš„Pagesåœ°å€ï¼‰"""
    today = get_today_date()
    html_filename = f"{today}.html"
    html_path = html_filename  # ä¿å­˜åˆ°ä»“åº“æ ¹ç›®å½•
    
    # ç”Ÿæˆç¾è§‚çš„åŒè¯­HTMLå†…å®¹
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>AIèµ„è®¯æ—¥æŠ¥ ä¸­è‹±å¯¹ç…§ | {today}</title>
    <style>
        body {{ 
            font-family: "Microsoft YaHei", Arial, sans-serif; 
            max-width: 900px; 
            margin: 0 auto; 
            padding: 30px; 
            line-height: 1.6;
            color: #333;
        }}
        h1 {{ 
            color: #2c3e50; 
            border-bottom: 2px solid #3498db; 
            padding-bottom: 10px;
            text-align: center;
        }}
        h2 {{ 
            color: #3498db; 
            margin-top: 40px;
        }}
        .en-block {{ 
            background-color: #f8f9fa; 
            padding: 15px; 
            border-left: 4px solid #7f8c8d; 
            margin: 10px 0;
        }}
        .zh-block {{ 
            background-color: #e8f4fd; 
            padding: 15px; 
            border-left: 4px solid #3498db; 
            margin: 10px 0;
        }}
        .source-link {{ 
            margin: 20px 0; 
            color: #2980b9; 
            font-weight: bold;
        }}
        hr {{ 
            border: 0; 
            border-top: 1px solid #eee; 
            margin: 30px 0;
        }}
    </style>
</head>
<body>
    <h1>AIèµ„è®¯æ—¥æŠ¥ å®Œæ•´ä¸­è‹±å¯¹ç…§ | {today}</h1>
"""
    # æ‹¼æ¥æ¯ç¯‡èµ„è®¯çš„åŒè¯­å†…å®¹
    for idx, art in enumerate(articles, 1):
        html_content += f"""
    <h2>{idx}. {art['title']['zh']}</h2>
    <div class="en-block"><strong>English Title:</strong> {art['title']['en']}</div>
    <div class="zh-block"><strong>ä¸­æ–‡æ ‡é¢˜:</strong> {art['title']['zh']}</div>
    
    <h3>æ­£æ–‡å†…å®¹</h3>
    <div class="en-block"><strong>English Content:</strong> {art['content']['en']}</div>
    <div class="zh-block"><strong>ä¸­æ–‡ç¿»è¯‘:</strong> {art['content']['zh']}</div>
    
    <div class="source-link"><strong>Source Link / æ¥æºé“¾æ¥:</strong> <a href="{art['link']}" target="_blank">{art['link']}</a></div>
    <hr>
"""
    html_content += """
</body>
</html>
"""
    
    # ä¿å­˜HTMLæ–‡ä»¶åˆ°æœ¬åœ°
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    # æäº¤HTMLæ–‡ä»¶åˆ°GitHubï¼ˆé€‚é…GitHub Actionsæƒé™ï¼‰
    try:
        # é…ç½®gitç”¨æˆ·ä¿¡æ¯ï¼ˆActionsè¿è¡Œæ—¶éœ€è¦ï¼‰
        subprocess.run(["git", "config", "--global", "user.name", "GitHub Actions"], check=True)
        subprocess.run(["git", "config", "--global", "user.email", "actions@github.com"], check=True)
        
        # æäº¤æ–‡ä»¶
        subprocess.run(["git", "add", html_path], check=True)
        subprocess.run(["git", "commit", "-m", f"Add bilingual HTML: {html_filename}"], check=True)
        subprocess.run(["git", "push", "origin", "main"], check=True)
        
        logging.info(f"âœ… åŒè¯­HTMLæ–‡ä»¶ {html_filename} æäº¤æˆåŠŸ")
        # è¿”å›å¯è®¿é—®çš„Pagesé“¾æ¥
        return f"{GITHUB_PAGES_URL}/{html_filename}"
    except Exception as e:
        logging.error(f"æäº¤HTMLæ–‡ä»¶å¤±è´¥: {str(e)}")
        # å…œåº•ï¼šè¿”å›åŸèµ„è®¯é“¾æ¥
        return articles[0]["link"] if articles else "#"

# ===================== å¤šæºæŠ“å–+çƒ­ç‚¹ç­›é€‰ =====================
def crawl_academic():
    """ğŸ“š å­¦æœ¯å‰æ²¿ï¼ˆarXiv CS.AIä¸“æ ï¼‰"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        if feed.entries and len(feed.entries) > 0:
            entry = feed.entries[0]  # æœ€æ–°è®ºæ–‡
            title_bi = baidu_translate(clean_text(entry.title))
            return {
                "type": "å­¦æœ¯å‰æ²¿",
                "title": title_bi,
                "link": entry.link,
                "source": "arXiv",
                "hot_score": round(random.uniform(85, 95), 1)  # æ¨¡æ‹Ÿçƒ­åº¦å€¼
            }
        else:
            return {
                "type": "å­¦æœ¯å‰æ²¿",
                "title": {"en": "No academic updates today", "zh": "ä»Šæ—¥æš‚æ— å­¦æœ¯å‰æ²¿æ›´æ–°"},
                "link": "",
                "source": "",
                "hot_score": 0
            }
    except Exception as e:
        logging.error(f"æŠ“å–å­¦æœ¯å‰æ²¿å¤±è´¥: {str(e)}")
        return {
            "type": "å­¦æœ¯å‰æ²¿",
            "title": {"en": "Academic crawl failed", "zh": "å­¦æœ¯å‰æ²¿æŠ“å–å¤±è´¥"},
            "link": "",
            "source": "",
            "hot_score": 0
        }

def crawl_official_blog():
    """ğŸ¢ å®˜æ–¹åšå®¢ï¼ˆOpenAI Blogï¼‰"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        feed = feedparser.parse("https://openai.com/blog/rss/")
        if feed.entries and len(feed.entries) > 0:
            entry = feed.entries[0]
            title_bi = baidu_translate(clean_text(entry.title))
            return {
                "type": "å®˜æ–¹åšå®¢",
                "title": title_bi,
                "link": entry.link,
                "source": "OpenAI Blog",
                "hot_score": round(random.uniform(88, 98), 1)
            }
        else:
            return {
                "type": "å®˜æ–¹åšå®¢",
                "title": {"en": "No official blog updates today", "zh": "ä»Šæ—¥æš‚æ— å®˜æ–¹åšå®¢æ›´æ–°"},
                "link": "",
                "source": "",
                "hot_score": 0
            }
    except Exception as e:
        logging.error(f"æŠ“å–å®˜æ–¹åšå®¢å¤±è´¥: {str(e)}")
        return {
            "type": "å®˜æ–¹åšå®¢",
            "title": {"en": "Official blog crawl failed", "zh": "å®˜æ–¹åšå®¢æŠ“å–å¤±è´¥"},
            "link": "",
            "source": "",
            "hot_score": 0
        }

def crawl_community():
    """ğŸ’¬ æµ·å¤–ç¤¾åŒºï¼ˆHackerNews AIç›¸å…³ï¼‰"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        response = requests.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json",
            headers=HEADERS,
            timeout=10
        )
        top_stories = response.json()[:5]  # å–å‰5æ¡
        
        # æŠ“å–ç¬¬ä¸€æ¡AIç›¸å…³å¸–å­
        for story_id in top_stories:
            story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
            story = requests.get(story_url, headers=HEADERS, timeout=5).json()
            if "title" in story and ("AI" in story["title"] or "LLM" in story["title"]):
                title_bi = baidu_translate(clean_text(story["title"]))
                link = story.get("url", f"https://news.ycombinator.com/item?id={story_id}")
                return {
                    "type": "æµ·å¤–ç¤¾åŒº",
                    "title": title_bi,
                    "link": link,
                    "source": "HackerNews",
                    "hot_score": round(random.uniform(80, 90), 1)
                }
        
        return {
            "type": "æµ·å¤–ç¤¾åŒº",
            "title": {"en": "No AI community updates today", "zh": "ä»Šæ—¥æš‚æ— æµ·å¤–ç¤¾åŒºAIæ›´æ–°"},
            "link": "",
            "source": "",
            "hot_score": 0
        }
    except Exception as e:
        logging.error(f"æŠ“å–æµ·å¤–ç¤¾åŒºå¤±è´¥: {str(e)}")
        return {
            "type": "æµ·å¤–ç¤¾åŒº",
            "title": {"en": "Community crawl failed", "zh": "æµ·å¤–ç¤¾åŒºæŠ“å–å¤±è´¥"},
            "link": "",
            "source": "",
            "hot_score": 0
        }

def crawl_social():
    """ğŸ“± ç¤¾åª’èšåˆï¼ˆTwitter/OpenAIï¼‰"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        feed = feedparser.parse("https://nitter.net/OpenAI/rss")
        if feed.entries and len(feed.entries) > 0:
            entry = feed.entries[0]
            title_bi = baidu_translate(clean_text(entry.title))
            link = entry.link.replace("nitter.net", "twitter.com")  # æ›¿æ¢ä¸ºåŸTwitteré“¾æ¥
            return {
                "type": "ç¤¾åª’èšåˆ",
                "title": title_bi,
                "link": link,
                "source": "Twitter/OpenAI",
                "hot_score": round(random.uniform(82, 92), 1)
            }
        else:
            return {
                "type": "ç¤¾åª’èšåˆ",
                "title": {"en": "No social media updates today", "zh": "ä»Šæ—¥æš‚æ— ç¤¾åª’AIæ›´æ–°"},
                "link": "",
                "source": "",
                "hot_score": 0
            }
    except Exception as e:
        logging.error(f"æŠ“å–ç¤¾åª’èšåˆå¤±è´¥: {str(e)}")
        return {
            "type": "ç¤¾åª’èšåˆ",
            "title": {"en": "Social media crawl failed", "zh": "ç¤¾åª’èšåˆæŠ“å–å¤±è´¥"},
            "link": "",
            "source": "",
            "hot_score": 0
        }

def crawl_and_rank_articles():
    """æŠ“å–å¹¶ç­›é€‰Top 2çƒ­ç‚¹èµ„è®¯"""
    # æŠ“å–å››ç±»ä¿¡æ¯
    academic = crawl_academic()
    official_blog = crawl_official_blog()
    community = crawl_community()
    social = crawl_social()
    
    # æ•´åˆæ‰€æœ‰æœ‰æ•ˆèµ„è®¯ï¼ˆè¿‡æ»¤æ— é“¾æ¥/æ— çƒ­åº¦çš„ï¼‰
    all_articles = []
    for art in [academic, official_blog, community, social]:
        if art["link"] and art["hot_score"] > 0:
            # æŠ“å–æ­£æ–‡å¹¶ç¿»è¯‘
            content_en = get_article_content(art["link"])
            content_bi = baidu_translate(content_en)
            art["content"] = content_bi  # æ–°å¢æ­£æ–‡åŒè¯­å†…å®¹
            all_articles.append(art)
    
    # æŒ‰çƒ­åº¦æ’åºï¼Œå–Top 2
    all_articles.sort(key=lambda x: x["hot_score"], reverse=True)
    return all_articles[:2]

# ===================== é£ä¹¦å¡ç‰‡å¼æ¨é€ =====================
def send_feishu_card():
    """é£ä¹¦äº¤äº’å¼å¡ç‰‡æ¨é€ï¼ˆåŒ¹é…ç›®æ ‡æ ·å¼ï¼‰"""
    # æ ¡éªŒå¿…è¦é…ç½®
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhookï¼")
        return False
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.error("âŒ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIå¯†é’¥ï¼")
        return False
    
    # æŠ“å–Top 2çƒ­ç‚¹
    top_articles = crawl_and_rank_articles()
    if not top_articles:
        logging.warning("âš ï¸ æ— çƒ­ç‚¹èµ„è®¯å¯æ¨é€")
        return False
    
    # ç”ŸæˆåŒè¯­HTMLæ–‡ä»¶å¹¶è·å–Pagesé“¾æ¥
    bilingual_html_url = save_bilingual_html(top_articles)
    
    # æ„å»ºé£ä¹¦å¡ç‰‡å†…å®¹
    card_content = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},  # å®½å±æ¨¡å¼
            "header": {
                "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today_date()}"},
                "template": "blue"  # å¡ç‰‡å¤´éƒ¨è“è‰²æ ·å¼
            },
            "elements": []
        }
    }
    
    # æ·»åŠ Topèµ„è®¯æ¡ç›®
    for idx, art in enumerate(top_articles, 1):
        # æ¡ç›®1ï¼šæ ‡é¢˜+çƒ­åº¦+æ¥æº
        element_title = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"{idx}. **{art['title']['zh']}** \n ğŸ“ˆ çƒ­åº¦ï¼š{art['hot_score']} | æ¥æºï¼š{art['source']}"
            }
        }
        
        # æ¡ç›®2ï¼šè‹±æ–‡æ ‡é¢˜+æŸ¥çœ‹è¯¦æƒ…é“¾æ¥
        element_english = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"ğŸ“ è‹±æ–‡åŸæ–‡ï¼š{art['title']['en'][:50]}... \n ğŸ”— [æŸ¥çœ‹è¯¦æƒ…ï¼ˆä¸­è‹±å¯¹ç…§ï¼‰]({art['link']})"
            }
        }
        
        # åˆ†å‰²çº¿
        element_hr = {"tag": "hr"}
        
        # æ·»åŠ åˆ°å¡ç‰‡
        card_content["card"]["elements"].extend([element_title, element_english, element_hr])
    
    # æ·»åŠ å®Œæ•´åŒè¯­ç½‘é¡µé“¾æ¥
    element_bilingual = {
        "tag": "div",
        "text": {
            "tag": "lark_md",
            "content": f"ğŸ“– [æŸ¥çœ‹å®Œæ•´ä¸­è‹±æ–‡å¯¹ç…§ç½‘é¡µ]({bilingual_html_url})"
        }
    }
    card_content["card"]["elements"].append(element_bilingual)
    
    # æ¨é€å¡ç‰‡åˆ°é£ä¹¦
    try:
        response = requests.post(
            FEISHU_WEBHOOK,
            data=json.dumps(card_content, ensure_ascii=False),
            headers={"Content-Type": "application/json"},
            timeout=10,
            verify=False
        )
        result = response.json()
        if result.get("code") == 0:
            logging.info("âœ… é£ä¹¦å¡ç‰‡æ¨é€æˆåŠŸï¼")
            return True
        else:
            logging.error(f"âŒ æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        logging.error(f"âŒ æ¨é€å¼‚å¸¸: {str(e)}")
        return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡ŒAIèµ„è®¯æ—¥æŠ¥æ¨é€ï¼ˆæœ€ç»ˆç‰ˆï¼‰")
    success = send_feishu_card()
    logging.info("ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå®Œæˆ" if success else "ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
