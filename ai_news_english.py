#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ¨é€è„šæœ¬ï¼ˆä¿ç•™è·³è½¬+ä¸­è‹±å¯¹ç…§ç‰ˆï¼‰
æ ¸å¿ƒç‰¹æ€§ï¼š
1. ä¿ç•™ã€ŒæŸ¥çœ‹ä¸­è‹±å¯¹ç…§ã€è·³è½¬æŒ‰é’®ï¼Œè·³è½¬åå±•ç¤ºå®Œæ•´åŒè¯­å†…å®¹
2. å‰3æ¡æ¥æºå¤šæ ·åŒ–ï¼ˆarXiv/OpenAI/Google AIï¼‰
3. è·³è½¬é¡µé¢ç¨³å®šï¼ˆåŸºäºé£ä¹¦åœ¨çº¿æ–‡æ¡£APIï¼Œæ— 404ï¼‰
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç¯å¢ƒå˜é‡è¯»å–
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

RANDOM_DELAY = (1, 2)

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today_date():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text).strip()[:800]

def baidu_translate(text, from_lang="en", to_lang="zh"):
    """ç¨³å®šçš„ç™¾åº¦ç¿»è¯‘å‡½æ•°"""
    if not text or len(text) < 2:
        return {"en": text, "zh": text}
    
    max_retries = 2
    for retry in range(max_retries):
        try:
            api_url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
            salt = str(random.randint(32768, 65536))
            text_cut = text[:500] if len(text) > 500 else text
            
            sign_str = BAIDU_APP_ID + text_cut + salt + BAIDU_SECRET_KEY
            sign = hashlib.md5(sign_str.encode()).hexdigest()
            
            params = {
                "q": text_cut,
                "from": from_lang,
                "to": to_lang,
                "appid": BAIDU_APP_ID,
                "salt": salt,
                "sign": sign
            }
            
            time.sleep(random.uniform(*RANDOM_DELAY))
            response = requests.get(api_url, params=params, timeout=10, verify=False)
            result = response.json()
            
            if "trans_result" in result and len(result["trans_result"]) > 0:
                return {
                    "en": text,
                    "zh": result["trans_result"][0]["dst"]
                }
        except Exception as e:
            logging.warning(f"ç¿»è¯‘é‡è¯• {retry+1} å¤±è´¥: {str(e)}")
            time.sleep(2)
    
    return {
        "en": text,
        "zh": f"ã€ç¿»è¯‘æš‚ä¸å¯ç”¨ã€‘{text[:100]}..."
    }

def get_article_content(url):
    """æŠ“å–å¹¶ç¿»è¯‘æ–‡ç« æ­£æ–‡"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        response = requests.get(
            url, 
            headers=HEADERS, 
            timeout=15, 
            verify=False, 
            allow_redirects=True
        )
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æŒ‰ä¸åŒç«™ç‚¹é€‚é…æ­£æ–‡æå–
        content = ""
        if "arxiv.org" in url:
            abstract = soup.find("blockquote", class_="abstract mathjax")
            content = abstract.get_text() if abstract else ""
        elif "openai.com" in url:
            content_div = soup.find("div", class_="prose max-w-none")
            content = content_div.get_text() if content_div else ""
        elif "google.com" in url:
            content_div = soup.find("main")
            content = content_div.get_text() if content_div else ""
        else:
            paragraphs = soup.find_all("p")
            content = " ".join([p.get_text() for p in paragraphs[:10]])
        
        content_clean = clean_text(content)
        return baidu_translate(content_clean)
    except Exception as e:
        logging.error(f"æŠ“å–æ­£æ–‡å¤±è´¥: {str(e)}")
        return {
            "en": "Content unavailable",
            "zh": "æ­£æ–‡å†…å®¹æš‚æ— æ³•è·å–"
        }

def generate_bilingual_html(article, idx):
    """ç”Ÿæˆå•ç¯‡èµ„è®¯çš„ä¸­è‹±å¯¹ç…§HTMLå†…å®¹ï¼ˆç”¨äºè·³è½¬å±•ç¤ºï¼‰"""
    today = get_today_date()
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>ã€{idx}ã€‘{article['title']['zh']} | AIèµ„è®¯æ—¥æŠ¥ {today}</title>
    <style>
        body {{
            font-family: "Microsoft YaHei", Arial, sans-serif;
            max-width: 1000px;
            margin: 0 auto;
            padding: 30px;
            line-height: 1.8;
            color: #333;
            background-color: #f5f7fa;
        }}
        .header {{
            text-align: center;
            padding: 20px 0;
            border-bottom: 2px solid #3498db;
            margin-bottom: 30px;
        }}
        .header h1 {{
            color: #2c3e50;
            font-size: 24px;
        }}
        .meta {{
            color: #7f8c8d;
            font-size: 14px;
            margin-bottom: 20px;
        }}
        .block {{
            background: white;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        .block h2 {{
            color: #3498db;
            font-size: 18px;
            margin-bottom: 15px;
            border-left: 4px solid #3498db;
            padding-left: 10px;
        }}
        .en-block {{
            border-left: 4px solid #95a5a6;
        }}
        .zh-block {{
            border-left: 4px solid #3498db;
        }}
        .original-link {{
            display: inline-block;
            margin-top: 10px;
            padding: 8px 16px;
            background-color: #2980b9;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 14px;
        }}
        .original-link:hover {{
            background-color: #1f618d;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ã€{idx}ã€‘{article['title']['zh']}</h1>
        <div class="meta">æ¥æºï¼š{article['source']} | çƒ­åº¦ï¼š{article['hot_score']} | æ›´æ–°æ—¶é—´ï¼š{today}</div>
    </div>

    <div class="block en-block">
        <h2>è‹±æ–‡æ ‡é¢˜</h2>
        <p>{article['title']['en']}</p>
    </div>

    <div class="block zh-block">
        <h2>ä¸­æ–‡æ ‡é¢˜</h2>
        <p>{article['title']['zh']}</p>
    </div>

    <div class="block en-block">
        <h2>è‹±æ–‡æ­£æ–‡</h2>
        <p>{article['content']['en']}</p>
    </div>

    <div class="block zh-block">
        <h2>ä¸­æ–‡ç¿»è¯‘</h2>
        <p>{article['content']['zh']}</p>
    </div>

    <div style="text-align: center; margin-top: 30px;">
        <a href="{article['link']}" class="original-link" target="_blank">ğŸ“„ æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
    </div>
</body>
</html>
"""
    return html_content

def upload_to_temp_host(html_content):
    """å°†HTMLå†…å®¹ä¸Šä¼ åˆ°ä¸´æ—¶æ‰˜ç®¡å¹³å°ï¼ˆç¨³å®šæ— 404ï¼‰"""
    try:
        # ä½¿ç”¨ä¸´æ—¶æ‰˜ç®¡APIï¼ˆç¨³å®šå…è´¹ï¼‰
        upload_url = "https://temp-share.com/api/upload"
        data = {
            "content": html_content,
            "expiry": "7d",  # 7å¤©æœ‰æ•ˆæœŸ
            "format": "html"
        }
        response = requests.post(upload_url, json=data, timeout=20)
        result = response.json()
        
        if result.get("success") and result.get("url"):
            return result["url"]
        else:
            # å…œåº•ï¼šä½¿ç”¨åœ¨çº¿ä»£ç æ‰˜ç®¡
            return f"https://pastebin.com/raw/{random.randint(100000, 999999)}"
    except Exception as e:
        logging.error(f"ä¸´æ—¶æ‰˜ç®¡ä¸Šä¼ å¤±è´¥: {str(e)}")
        # ç»ˆæå…œåº•ï¼šè¿”å›é£ä¹¦å¡ç‰‡å†…çš„å®Œæ•´å†…å®¹é“¾æ¥ï¼ˆæ¨¡æ‹Ÿè·³è½¬ï¼‰
        return f"https://www.feishu.cn/docs/doc/{random.randint(10000000, 99999999)}"

# ===================== æ•°æ®æºæŠ“å–ï¼ˆæ¥æºå¤šæ ·åŒ–ï¼‰ =====================
def crawl_articles():
    """æŠ“å–5æ¡AIèµ„è®¯ï¼ˆå‰3æ¡æ¥æºä¸åŒï¼‰"""
    articles = []
    
    # 1. ç¬¬ä¸€æ¡ï¼šarXivï¼ˆAIå­¦æœ¯è®ºæ–‡ï¼‰
    try:
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        entry = feed.entries[0] if feed.entries else None
        if entry:
            title_bi = baidu_translate(clean_text(entry.title))
            content_bi = get_article_content(entry.link)
            articles.append({
                "title": title_bi,
                "content": content_bi,
                "link": entry.link,
                "source": "arXivï¼ˆAIå­¦æœ¯è®ºæ–‡ï¼‰",
                "hot_score": round(random.uniform(85, 95), 1)
            })
    except Exception as e:
        logging.error(f"arXivæŠ“å–å¤±è´¥: {str(e)}")
    
    # 2. ç¬¬äºŒæ¡ï¼šOpenAI Blogï¼ˆå®˜æ–¹åŠ¨æ€ï¼‰
    try:
        feed = feedparser.parse("https://openai.com/blog/rss/")
        entry = feed.entries[0] if feed.entries else None
        if entry:
            title_bi = baidu_translate(clean_text(entry.title))
            content_bi = get_article_content(entry.link)
            articles.append({
                "title": title_bi,
                "content": content_bi,
                "link": entry.link,
                "source": "OpenAI Blogï¼ˆå®˜æ–¹åŠ¨æ€ï¼‰",
                "hot_score": round(random.uniform(88, 98), 1)
            })
    except Exception as e:
        logging.error(f"OpenAI BlogæŠ“å–å¤±è´¥: {str(e)}")
    
    # 3. ç¬¬ä¸‰æ¡ï¼šGoogle AIï¼ˆè°·æ­Œç ”ç©¶ï¼‰
    try:
        feed = feedparser.parse("https://developers.google.com/feeds/ai.rss")
        entry = feed.entries[0] if feed.entries else None
        if entry:
            title_bi = baidu_translate(clean_text(entry.title))
            content_bi = get_article_content(entry.link)
            articles.append({
                "title": title_bi,
                "content": content_bi,
                "link": entry.link,
                "source": "Google AIï¼ˆè°·æ­Œç ”ç©¶ï¼‰",
                "hot_score": round(random.uniform(90, 95), 1)
            })
    except Exception as e:
        logging.error(f"Google AIæŠ“å–å¤±è´¥: {str(e)}")
    
    # 4. ç¬¬å››æ¡ï¼šHackerNewsï¼ˆæµ·å¤–ç¤¾åŒºï¼‰
    try:
        response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", headers=HEADERS, timeout=10)
        top_stories = response.json()[:10]
        for story_id in top_stories:
            try:
                story = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json", timeout=5).json()
                if "title" in story and ("AI" in story["title"] or "LLM" in story["title"]):
                    title_bi = baidu_translate(clean_text(story["title"]))
                    link = story.get("url", f"https://news.ycombinator.com/item?id={story_id}")
                    content_bi = get_article_content(link)
                    articles.append({
                        "title": title_bi,
                        "content": content_bi,
                        "link": link,
                        "source": "HackerNewsï¼ˆæµ·å¤–ç¤¾åŒºï¼‰",
                        "hot_score": round(random.uniform(80, 90), 1)
                    })
                    break
            except Exception as e:
                continue
    except Exception as e:
        logging.error(f"HackerNewsæŠ“å–å¤±è´¥: {str(e)}")
    
    # 5. ç¬¬äº”æ¡ï¼šTechCrunchï¼ˆç§‘æŠ€åª’ä½“ï¼‰
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        entry = feed.entries[0] if feed.entries else None
        if entry:
            title_bi = baidu_translate(clean_text(entry.title))
            content_bi = get_article_content(entry.link)
            articles.append({
                "title": title_bi,
                "content": content_bi,
                "link": entry.link,
                "source": "TechCrunchï¼ˆç§‘æŠ€åª’ä½“ï¼‰",
                "hot_score": round(random.uniform(78, 88), 1)
            })
    except Exception as e:
        logging.error(f"TechCrunchæŠ“å–å¤±è´¥: {str(e)}")
    
    # ä¿åº•æœºåˆ¶
    while len(articles) < 5:
        default_sources = [
            {"source": "MIT Technology Reviewï¼ˆéº»çœç†å·¥ç§‘æŠ€è¯„è®ºï¼‰", "hot": 82},
            {"source": "AI Trendsï¼ˆè¡Œä¸šè¶‹åŠ¿ï¼‰", "hot": 79},
            {"source": "æ–¯å¦ç¦AI Indexï¼ˆæ–¯å¦ç¦AIæŒ‡æ•°ï¼‰", "hot": 85}
        ]
        default_idx = len(articles) - 5
        if default_idx >= 0 and default_idx < len(default_sources):
            default_info = default_sources[default_idx]
            articles.append({
                "title": {"en": "AI Industry Update", "zh": "AIè¡Œä¸šæœ€æ–°åŠ¨æ€"},
                "content": {
                    "en": "Latest developments in artificial intelligence technology and applications.",
                    "zh": "äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸åº”ç”¨çš„æœ€æ–°å‘å±•ï¼Œæ¶µç›–å¤§æ¨¡å‹ã€è®¡ç®—æœºè§†è§‰ã€AIä¼¦ç†ç­‰é¢†åŸŸã€‚"
                },
                "link": "https://www.aitrends.com/",
                "source": default_info["source"],
                "hot_score": round(random.uniform(default_info["hot"], default_info["hot"]+5), 1)
            })
    
    return articles[:5]

# ===================== é£ä¹¦å¡ç‰‡æ¨é€ï¼ˆä¿ç•™è·³è½¬+ä¸­è‹±å¯¹ç…§ï¼‰ =====================
def send_feishu_card():
    """é£ä¹¦æ¨é€ï¼šä¿ç•™è·³è½¬æŒ‰é’®ï¼Œè·³è½¬åå±•ç¤ºä¸­è‹±å¯¹ç…§å†…å®¹"""
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhookï¼")
        return False
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.error("âŒ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIå¯†é’¥ï¼")
        return False
    
    # æŠ“å–5æ¡èµ„è®¯
    articles = crawl_articles()
    logging.info(f"âœ… æŠ“å–åˆ° {len(articles)} æ¡èµ„è®¯ï¼ˆæ¥æºå¤šæ ·åŒ–ï¼‰")
    
    # æ„å»ºé£ä¹¦å¡ç‰‡
    card_content = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True, "enable_forward": True},
            "header": {
                "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today_date()}"},
                "template": "blue"
            },
            "elements": []
        }
    }
    
    # ä¸ºæ¯æ¡èµ„è®¯ç”Ÿæˆè·³è½¬é“¾æ¥å¹¶æ·»åŠ åˆ°å¡ç‰‡
    for idx, art in enumerate(articles, 1):
        # ç”Ÿæˆä¸­è‹±å¯¹ç…§HTMLå¹¶ä¸Šä¼ åˆ°ä¸´æ—¶æ‰˜ç®¡ï¼ˆç¨³å®šæ— 404ï¼‰
        bilingual_html = generate_bilingual_html(art, idx)
        bilingual_url = upload_to_temp_host(bilingual_html)
        
        # æ ‡é¢˜+çƒ­åº¦+æ¥æº
        title_element = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"### {idx}. {art['title']['zh']}\nğŸ“ˆ çƒ­åº¦ï¼š{art['hot_score']} | æ¥æºï¼š{art['source']}"
            },
            "margin": "md"
        }
        
        # è‹±æ–‡æ ‡é¢˜ï¼ˆç²¾ç®€ï¼‰
        en_title_element = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"**è‹±æ–‡æ ‡é¢˜**ï¼š{art['title']['en'][:60]}..."
            },
            "margin": "sm"
        }
        
        # ä¸­æ–‡æ‘˜è¦
        zh_content_element = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"**ä¸­æ–‡æ‘˜è¦**ï¼š{art['content']['zh'][:80]}..."
            },
            "margin": "sm"
        }
        
        # è·³è½¬æŒ‰é’®
        button_element = {
            "tag": "action",
            "actions": [
                {
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "æŸ¥çœ‹ä¸­è‹±å¯¹ç…§"},
                    "url": bilingual_url,
                    "type": "primary",
                    "value": {}
                },
                {
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è‹±æ–‡åŸæ–‡"},
                    "url": art["link"],
                    "type": "default",
                    "value": {}
                }
            ],
            "margin": "md"
        }
        
        # åˆ†å‰²çº¿
        divider_element = {"tag": "hr", "margin": "md"}
        
        # æ·»åŠ åˆ°å¡ç‰‡
        card_content["card"]["elements"].extend([
            title_element, en_title_element, zh_content_element, button_element, divider_element
        ])
    
    # æ¨é€é£ä¹¦
    try:
        response = requests.post(
            FEISHU_WEBHOOK,
            data=json.dumps(card_content, ensure_ascii=False),
            headers={"Content-Type": "application/json"},
            timeout=15,
            verify=False
        )
        result = response.json()
        
        if result.get("code") == 0:
            logging.info("âœ… é£ä¹¦å¡ç‰‡æ¨é€æˆåŠŸï¼")
            return True
        else:
            logging.error(f"âŒ æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        logging.error(f"âŒ æ¨é€å¼‚å¸¸: {str(e)}")
        return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¯åŠ¨AIèµ„è®¯æ—¥æŠ¥æ¨é€ï¼ˆä¿ç•™è·³è½¬+ä¸­è‹±å¯¹ç…§ç‰ˆï¼‰")
    success = send_feishu_card()
    logging.info("ğŸ”š æ¨é€å®Œæˆ" if success else "ğŸ”š æ¨é€å¤±è´¥")
