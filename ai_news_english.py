#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ—¥æŠ¥æ¨é€è„šæœ¬ v5
==============================================================
v5 æ–°å¢ä¿®å¤ï¼š
  Fix-A  TechCrunch/VentureBeat/Forbes ç­‰æˆªæ–­å‹ç«™ç‚¹ â†’
         å¼ºåˆ¶æŠ“å–åŸæ–‡é¡µé¢ï¼Œä¸ä¾èµ–ä»…æœ‰2å¥çš„ RSS summary
  Fix-B  MIT Tech Review éAIæ–‡ç«  â†’
         ä¸¥æ ¼å…³é”®è¯è¿‡æ»¤å‰15æ¡ï¼Œæ— åŒ¹é…ç›´æ¥è·³è¿‡
  Fix-C  é£ä¹¦å¡ç‰‡æ ‡é¢˜"###" â†’ æ•°å­—emoji + æ¥æºå›¾æ ‡ï¼Œæ›´ç¾è§‚

ç»§æ‰¿ v4 ä¿®å¤ï¼š
  safe_translate / translate_long_textï¼ˆç¿»è¯‘ä¸å´©æºƒ + åˆ†æ®µç¿»è¯‘ï¼‰
  fetch_article_contentï¼ˆç²¾å‡†æ®µè½æå– + å»å¹¿å‘Šå™ªå£°ï¼‰
  generate_bilingual_htmlï¼ˆå­—æ®µå…¨é¢ç©ºå€¼é˜²æŠ¤ï¼‰
  htmlpreview.github.io æ¸²æŸ“é“¾æ¥ï¼ˆéRawæºç é“¾æ¥ï¼‰
  window.close() å…³é—­æŒ‰é’®
"""

import requests
import os
import datetime
import time
import random
import hashlib
import re
import logging
import urllib3
import feedparser
from bs4 import BeautifulSoup

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

FEISHU_WEBHOOK  = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID    = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
GIST_TOKEN      = os.getenv("AI_NEWS_GIST_TOKEN", "")

GLOBAL_TIMEOUT  = 20
MAX_RETRIES     = 3
RANDOM_DELAY    = (0.8, 1.5)
TRANSLATE_MAX   = 1800   # ç™¾åº¦ç¿»è¯‘å•æ¬¡æœ€å¤§å­—ç¬¦æ•°
CONTENT_MAX     = 6000   # æ­£æ–‡æŠ“å–æœ€å¤§ä¿ç•™å­—ç¬¦ï¼ˆè¶³å¤Ÿå®Œæ•´ï¼Œä¸æˆªæ–­æ–‡ç« ï¼‰
CONTENT_MIN_LEN = 80     # å†…å®¹ä½äºæ­¤é•¿åº¦åˆ™ç»§ç»­å°è¯•ä¸‹ä¸€çº§

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S"
)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
    "Cache-Control": "no-cache",
}

# ===================== å·¥å…·å‡½æ•° =====================
def get_today():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text, max_len=None):
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€‚max_len=None æ—¶ä¸æˆªæ–­ï¼ˆæ­£æ–‡è·å–ç”¨ï¼‰"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', str(text)).strip()
    if max_len and len(text) > max_len:
        # åœ¨å¥å­è¾¹ç•Œæˆªæ–­ï¼Œé¿å…åŠå¥è¯
        truncated = text[:max_len]
        last_period = max(truncated.rfind('. '), truncated.rfind('ã€‚'))
        return truncated[:last_period + 1] if last_period > max_len * 0.7 else truncated
    return text

def clean_content(text):
    """æ­£æ–‡æ¸…ç†ï¼šä¿ç•™å®Œæ•´å†…å®¹ï¼Œæœ€å¤š CONTENT_MAX å­—ç¬¦ï¼ˆåœ¨å¥å­è¾¹ç•Œæˆªæ–­ï¼‰"""
    return clean_text(text, max_len=CONTENT_MAX)

def clean_title(text):
    """æ ‡é¢˜æ¸…ç†ï¼šé™åˆ¶åœ¨åˆç†é•¿åº¦"""
    return clean_text(text, max_len=300)

def strip_html(raw_html):
    """å°† HTML å­—ç¬¦ä¸²è½¬ä¸ºçº¯æ–‡æœ¬ï¼ˆä¸æˆªæ–­ï¼‰"""
    if not raw_html:
        return ""
    return clean_text(BeautifulSoup(str(raw_html), "html.parser").get_text())

def retry(func):
    """é‡è¯•è£…é¥°å™¨ï¼Œå¤±è´¥è¿”å› None"""
    def wrapper(*args, **kwargs):
        for i in range(MAX_RETRIES):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logging.warning(f"[{func.__name__}] ç¬¬{i+1}æ¬¡å¤±è´¥: {str(e)[:60]}")
                time.sleep(random.uniform(*RANDOM_DELAY))
        logging.error(f"[{func.__name__}] å…¨éƒ¨é‡è¯•å¤±è´¥")
        return None
    return wrapper


# ===================== Fix-1 + Fix-2ï¼šç¿»è¯‘å‡½æ•° =====================
def _call_baidu_api(text):
    """
    å•æ¬¡è°ƒç”¨ç™¾åº¦ç¿»è¯‘ APIï¼Œè¿”å›ä¸­æ–‡å­—ç¬¦ä¸²æˆ– Noneã€‚
    text é•¿åº¦è°ƒç”¨æ–¹ä¿è¯ <= TRANSLATE_MAXã€‚
    ç™¾åº¦é”™è¯¯ç è¯´æ˜ï¼š52003=æœªæˆæƒ 54001=ç­¾åé”™è¯¯ 54004=ä½™é¢ä¸è¶³
    error_code=-27 ç­‰éæ ‡å‡†é”™è¯¯ä¹Ÿéœ€è¦æ‹¦æˆªã€‚
    """
    url  = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    salt = str(random.randint(32768, 65536))
    sign = hashlib.md5((BAIDU_APP_ID + text + salt + BAIDU_SECRET_KEY).encode()).hexdigest()
    params = {"q": text, "from": "en", "to": "zh",
              "appid": BAIDU_APP_ID, "salt": salt, "sign": sign}
    resp = requests.get(url, params=params, timeout=GLOBAL_TIMEOUT, verify=False)
    res  = resp.json()

    # æ˜ç¡®æœ‰ç¿»è¯‘ç»“æœæ‰è¿”å›
    if "trans_result" in res and res["trans_result"]:
        translated = res["trans_result"][0]["dst"]
        # è¿‡æ»¤ç™¾åº¦è¿”å›çš„é”™è¯¯æç¤ºæ–‡å­—ï¼ˆé”™è¯¯æ—¶æœ‰æ—¶ä¼šæŠŠé”™è¯¯ä¿¡æ¯ç¿»è¯‘å‡ºæ¥ï¼‰
        ERROR_PATTERNS = ["æœåŠ¡é”™è¯¯", "æœåŠ¡ç›®å‰ä¸å¯ç”¨", "é‚£æ˜¯ä¸ªé”™è¯¯", "é”™è¯¯-", "error_code"]
        if any(p in translated for p in ERROR_PATTERNS):
            logging.error(f"ç™¾åº¦ç¿»è¯‘è¿”å›é”™è¯¯æ–‡æœ¬: {translated[:50]}")
            return None
        return translated

    # æœ‰ error_code å­—æ®µè¯´æ˜ç¿»è¯‘å¤±è´¥
    if "error_code" in res:
        logging.error(f"ç™¾åº¦ç¿»è¯‘é”™è¯¯ç : {res.get('error_code')} - {res.get('error_msg', '')}")
        return None

    logging.error(f"ç™¾åº¦ç¿»è¯‘å¼‚å¸¸å“åº”: {res}")
    return None


def translate_long_text(text):
    """
    Fix-2ï¼šè¶…é•¿æ–‡æœ¬æŒ‰å¥å­åˆ†æ®µç¿»è¯‘ï¼ˆä¸è¶…è¿‡ TRANSLATE_MAXï¼‰ï¼Œç»“æœæ‹¼æ¥è¿”å›ã€‚
    """
    if not text or not text.strip():
        return ""

    # æŒ‰å¥å·/é—®å·/æ„Ÿå¹å·åˆ†å¥ï¼Œå°½é‡ä¿æŒè¯­ä¹‰å®Œæ•´
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    chunks, cur = [], ""
    for sent in sentences:
        if len(cur) + len(sent) + 1 <= TRANSLATE_MAX:
            cur = (cur + " " + sent).strip()
        else:
            if cur:
                chunks.append(cur)
            # å•å¥è¶…é•¿åˆ™å¼ºåˆ¶æˆªæ–­
            cur = sent[:TRANSLATE_MAX]
    if cur:
        chunks.append(cur)

    zh_parts = []
    for chunk in chunks:
        zh = _call_baidu_api(chunk)
        if zh:
            zh_parts.append(zh)
        else:
            zh_parts.append(chunk)   # ç¿»è¯‘å¤±è´¥ä¿ç•™åŸæ–‡æ®µ
        time.sleep(random.uniform(0.3, 0.6))   # é¿å… API é¢‘ç‡é™åˆ¶

    return "".join(zh_parts)


def safe_translate(text):
    """
    å®‰å…¨ç¿»è¯‘å‡½æ•°ï¼Œå§‹ç»ˆè¿”å› {"en": ..., "zh": ...}ï¼Œç»ä¸è¿”å› Noneã€‚
    - en å­—æ®µä¿å­˜å®Œæ•´åŸæ–‡
    - zh å­—æ®µæ˜¯å®Œæ•´ç¿»è¯‘
    """
    en_text = clean_text(text) if text else ""

    if not en_text or len(en_text) < 3:
        return {"en": en_text, "zh": en_text or "æš‚æ— å†…å®¹"}

    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.warning("âš ï¸ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIï¼Œä¸­æ–‡æ æ˜¾ç¤ºè‹±æ–‡åŸæ–‡")
        return {"en": en_text, "zh": en_text}

    try:
        zh_text = translate_long_text(en_text)
        if zh_text and zh_text.strip():
            logging.info(f"âœ… ç¿»è¯‘å®Œæˆ({len(en_text)}å­—â†’{len(zh_text)}å­—): {en_text[:20]}...")
            return {"en": en_text, "zh": zh_text}
        else:
            logging.warning("âš ï¸ ç¿»è¯‘ç»“æœä¸ºç©ºï¼Œä½¿ç”¨åŸæ–‡")
            return {"en": en_text, "zh": en_text}
    except Exception as e:
        logging.error(f"âŒ ç¿»è¯‘å¼‚å¸¸: {e}")
        return {"en": en_text, "zh": en_text}


# ===================== Fix-3ï¼šç²¾å‡†æ­£æ–‡æŠ“å– =====================
@retry
def fetch_article_content(url):
    """
    ç²¾å‡†æ­£æ–‡æŠ“å–ã€‚
    é˜²çº¿1ï¼šHTTP çŠ¶æ€ç é 200 ç›´æ¥è¿”å›ç©ºï¼ˆä¸å¤„ç†é”™è¯¯é¡µé¢ï¼‰
    é˜²çº¿2ï¼šè¯†åˆ«å¸¸è§é”™è¯¯é¡µé¢ç‰¹å¾æ–‡å­—ï¼Œè¿”å›ç©º
    """
    # é”™è¯¯é¡µé¢ç‰¹å¾ï¼ˆé¿å…æŠŠæœåŠ¡å™¨é”™è¯¯é¡µå½“æ­£æ–‡ï¼‰
    ERROR_PAGE_SIGNS = [
        "503", "502", "500", "404",
        "that's an error", "service error", "not available at this time",
        "access denied", "forbidden", "cloudflare", "just a moment",
        "please enable cookies", "enable javascript",
        "our systems have detected unusual traffic",
    ]
    try:
        resp = requests.get(
            url, headers=HEADERS, timeout=GLOBAL_TIMEOUT,
            verify=False, allow_redirects=True
        )

        # é˜²çº¿1ï¼šçŠ¶æ€ç æ£€æŸ¥
        if resp.status_code != 200:
            logging.warning(f"âš ï¸ æŠ“å–è¿”å› {resp.status_code}: {url[:60]}")
            return ""

        resp.encoding = resp.apparent_encoding
        soup = BeautifulSoup(resp.text, "html.parser")

        # é˜²çº¿2ï¼šæ£€æµ‹é”™è¯¯é¡µé¢ç‰¹å¾ï¼ˆåœ¨æ­£æ–‡æå–å‰ï¼‰
        page_text_sample = soup.get_text()[:500].lower()
        if any(sign in page_text_sample for sign in ERROR_PAGE_SIGNS):
            logging.warning(f"âš ï¸ æ£€æµ‹åˆ°é”™è¯¯é¡µé¢: {url[:60]}")
            return ""

        # ç§»é™¤å¹²æ‰°å…ƒç´ ï¼ˆå¹¿å‘Šã€å¯¼èˆªã€ä¾§è¾¹æ ã€è„šæ³¨ï¼‰
        for tag in soup.find_all(["script", "style", "nav", "header",
                                   "footer", "aside", "figure", "figcaption",
                                   "noscript", "iframe"]):
            tag.decompose()
        for tag in soup.find_all(class_=re.compile(
            r"(ad|ads|advert|sponsor|promo|related|recommend|sidebar|"
            r"newsletter|subscribe|comment|social|share|cookie|banner)",
            re.I
        )):
            tag.decompose()

        # æŒ‰ç«™ç‚¹ç²¾å‡†é€‰æ‹©æ­£æ–‡å®¹å™¨
        content_el = None
        if "arxiv.org" in url:
            content_el = soup.find("blockquote", class_="abstract mathjax")
        elif "openai.com" in url:
            content_el = (soup.find("div", class_=re.compile(r"post.?content", re.I))
                          or soup.find("main"))
        elif "venturebeat.com" in url:
            content_el = (soup.find("div", class_=re.compile(r"article.?content|entry.?content", re.I))
                          or soup.find("article"))
        elif "forbes.com" in url:
            content_el = (soup.find("div", class_=re.compile(r"article.?body|body.?text", re.I))
                          or soup.find("article"))
        elif "opentools.ai" in url:
            content_el = (soup.find("div", class_=re.compile(r"post.?content|entry.?content", re.I))
                          or soup.find("article"))
        elif "techcrunch.com" in url:
            article = soup.find("article")
            if article:
                paras = [p.get_text(" ", strip=True) for p in article.find_all("p")
                         if len(p.get_text(strip=True)) > 40]
                return clean_content(" ".join(paras))   # âœ… å–å…¨éƒ¨æ®µè½ï¼Œä¸é™8æ®µ
        elif "technologyreview.com" in url:
            content_el = (soup.find("div", class_=re.compile(r"article.?body|content.?body", re.I))
                          or soup.find("article"))
        elif "news.ycombinator.com" in url:
            content_el = soup.find("div", class_="storytext")
        elif "reuters.com" in url or "bloomberg.com" in url:
            content_el = soup.find("div", attrs={"data-testid": re.compile(r"body|article", re.I)})

        # æœ‰ç²¾å‡†å®¹å™¨ â†’ å–å…¨éƒ¨æ®µè½
        if content_el:
            paras = [p.get_text(" ", strip=True) for p in content_el.find_all("p")
                     if len(p.get_text(strip=True)) > 30]
            text  = " ".join(paras) if paras else content_el.get_text(" ", strip=True)
            return clean_content(text)   # âœ… ç”¨ clean_contentï¼Œåœ¨å¥å­è¾¹ç•Œæˆªæ–­

        # é€šç”¨å…œåº•ï¼šå…¨æ–‡æœç´¢ <p>ï¼Œè¿‡æ»¤çŸ­æ®µï¼Œå–å‰10æ®µ
        paras = [p.get_text(" ", strip=True) for p in soup.find_all("p")
                 if len(p.get_text(strip=True)) > 40][:10]
        return clean_content(" ".join(paras))

    except Exception as e:
        logging.error(f"âŒ æŠ“å–æ­£æ–‡å¤±è´¥ [{url[:50]}]: {e}")
        return ""


# ===================== Fix-4ï¼šå¤šçº§å†…å®¹è·å– =====================
def resolve_google_news_url(url):
    """
    Google News RSS çš„é“¾æ¥æ˜¯è·³è½¬é“¾æ¥ï¼Œéœ€è¦è§£æå‡ºçœŸå®æ–‡ç«  URLã€‚
    åŒæ—¶æ£€æµ‹è½åœ°é¡µè¯­è¨€ï¼Œä¸­æ–‡é¡µé¢è¿”å› None è¡¨ç¤ºè·³è¿‡ã€‚
    """
    if "news.google.com" not in url:
        return url
    try:
        resp = requests.get(
            url, headers=HEADERS, timeout=GLOBAL_TIMEOUT,
            allow_redirects=True, verify=False
        )
        final_url = resp.url
        if "google.com" in final_url:
            return url  # é‡å®šå‘æœªæˆåŠŸï¼Œè¿”å›åŸé“¾æ¥

        # æ£€æµ‹è½åœ°é¡µæ˜¯å¦ä¸ºä¸­æ–‡é¡µé¢
        if is_chinese_url(final_url):
            logging.warning(f"  [URLè¿‡æ»¤] ä¸­æ–‡è½åœ°é¡µè·³è¿‡: {final_url[:60]}")
            return None  # None è¡¨ç¤ºè·³è¿‡è¿™ç¯‡æ–‡ç« 

        logging.info(f"  [URLè§£æ] Google News â†’ {final_url[:80]}")
        return final_url
    except Exception as e:
        logging.warning(f"  [URLè§£æ] å¤±è´¥: {e}")
    return url


def get_rich_content(entry, url):
    """
    å¤šçº§å…œåº•è·å–æ­£æ–‡ï¼Œç¡®ä¿ç¿»è¯‘æœ‰å®è´¨å†…å®¹ã€‚

    Fix-A æ ¸å¿ƒé€»è¾‘ï¼š
    - å¯¹"æˆªæ–­å‹"ç«™ç‚¹ï¼ˆTechCrunch/VentureBeat/Forbes/MIT Tech Reviewï¼‰ï¼Œ
      RSS summary é€šå¸¸åªæœ‰1-2å¥ï¼Œç›´æ¥è·³è¿‡ summaryï¼Œå¼ºåˆ¶æŠ“å–åŸæ–‡é¡µé¢ã€‚
    - Google News é“¾æ¥å…ˆè§£æçœŸå® URL å†æŠ“å–ã€‚
    - å…¶ä»–ç«™ç‚¹èµ°æ­£å¸¸ä¼˜å…ˆçº§ï¼šfull content â†’ summary â†’ æŠ“å– â†’ å…œåº•ã€‚
    """
    # Google News é“¾æ¥å…ˆè§£æçœŸå® URLï¼ŒNone è¡¨ç¤ºä¸­æ–‡é¡µé¢
    real_url = resolve_google_news_url(url)
    if real_url is None:
        real_url = url  # é™çº§ç”¨åŸé“¾æ¥ï¼ˆç†è®ºä¸Šå…¬å¸çˆ¬è™«å·²æå‰è¿‡æ»¤ï¼‰

    # æˆªæ–­å‹ç«™ç‚¹ï¼šRSS summary ä¸å¯ä¿¡ï¼Œç›´æ¥æŠ“å–åŸæ–‡
    FORCE_FETCH_DOMAINS = [
        "techcrunch.com", "venturebeat.com", "forbes.com",
        "technologyreview.com", "reuters.com", "bloomberg.com",
        "news.google.com",  # Google News ç»Ÿä¸€å¼ºåˆ¶æŠ“å–
    ]
    force_fetch = any(d in url for d in FORCE_FETCH_DOMAINS)

    if not force_fetch:
        # 1ï¸âƒ£ RSS content:encodedï¼ˆéƒ¨åˆ†ç«™ç‚¹æä¾›å…¨æ–‡ï¼Œå¦‚ arXivï¼‰
        if hasattr(entry, "content") and entry.content:
            raw  = entry.content[0].get("value", "")
            text = strip_html(raw)
            if len(text) >= CONTENT_MIN_LEN:
                logging.info(f"  [å†…å®¹] RSS full content ({len(text)}å­—)")
                return text

        # 2ï¸âƒ£ RSS summaryï¼ˆHTMLå‰¥ç¦»åéœ€è¦è¶³å¤Ÿé•¿ï¼‰
        raw_summary = getattr(entry, "summary", "") or getattr(entry, "description", "")
        summary = strip_html(raw_summary)
        # åªæœ‰ summary è¶³å¤Ÿé•¿ï¼ˆ>= 200å­—ï¼‰æ‰ç›´æ¥ä½¿ç”¨ï¼Œé¿å…æˆªæ–­å†…å®¹
        if len(summary) >= 200:
            logging.info(f"  [å†…å®¹] RSS summary ({len(summary)}å­—)")
            return summary

    # 3ï¸âƒ£ å¼ºåˆ¶æŠ“å–åŸæ–‡é¡µé¢ï¼ˆæˆªæ–­å‹ç«™ç‚¹æˆ–summaryä¸è¶³ï¼‰
    fetch_target = real_url if real_url != url else url
    logging.info(f"  [å†…å®¹] æŠ“å–åŸæ–‡é¡µé¢: {fetch_target[:60]}")
    fetched = fetch_article_content(fetch_target) or ""
    if len(fetched) >= CONTENT_MIN_LEN:
        logging.info(f"  [å†…å®¹] æŠ“å–æˆåŠŸ ({len(fetched)}å­—)")
        return fetched

    # 4ï¸âƒ£ é™çº§å› RSS summaryï¼ˆæŠ“å–ä¹Ÿå¤±è´¥æ—¶ï¼‰
    raw_summary = getattr(entry, "summary", "") or getattr(entry, "description", "")
    summary = strip_html(raw_summary)
    if summary:
        logging.warning(f"  [å†…å®¹] é™çº§ç”¨RSS summary ({len(summary)}å­—)")
        return summary

    # 5ï¸âƒ£ æ ‡é¢˜å…œåº•ï¼ˆç»å¯¹ä¿åº•ï¼‰
    title    = clean_text(getattr(entry, "title", ""))
    fallback = f"{title}. Visit the original article for more details." if title else "AI industry latest update."
    logging.warning(f"  [å†…å®¹] æ ‡é¢˜å…œåº•")
    return fallback


# ===================== HTML ç”Ÿæˆ =====================
def generate_bilingual_html(article, index):
    """
    Fix-5ï¼šæ‰€æœ‰å­—æ®µå¢åŠ ç©ºå€¼å…œåº•ï¼Œç¡®ä¿ä»»ä½•æƒ…å†µä¸‹é¡µé¢éƒ½èƒ½æ­£å¸¸æ¸²æŸ“ã€‚
    """
    # å®‰å…¨å–å€¼ï¼ˆé˜²æ­¢ content ä¸º None å¯¼è‡´ .get å´©æºƒï¼‰
    def safe_get(obj, *keys, default=""):
        val = obj
        for k in keys:
            if isinstance(val, dict):
                val = val.get(k)
            else:
                return default
            if val is None:
                return default
        return str(val) if val else default

    title_en   = safe_get(article, "title",   "en", default="No Title")
    title_zh   = safe_get(article, "title",   "zh", default=title_en)
    content_en = safe_get(article, "content", "en", default="No content available.")
    content_zh = safe_get(article, "content", "zh", default=content_en)
    source     = article.get("source",    "Unknown")
    hot_score  = article.get("hot_score", "N/A")
    link       = article.get("link",      "#")
    today      = get_today()

    # å¼‚å¸¸å…œåº•ï¼šzh ä»ä¸ºç©ºæ—¶ç”¨ en
    if not content_zh.strip() or content_zh in ("æ— å†…å®¹", "æš‚æ— å†…å®¹", "ç¿»è¯‘å¤±è´¥ï¼Œæ˜¾ç¤ºåŸæ–‡", "ç¿»è¯‘å¼‚å¸¸ï¼Œæ˜¾ç¤ºåŸæ–‡"):
        content_zh = content_en
    if not title_zh.strip():
        title_zh = title_en

    logging.info(f"[HTML] #{index} æ ‡é¢˜EN={title_en[:30]} æ ‡é¢˜ZH={title_zh[:30]}")
    logging.info(f"[HTML] #{index} å†…å®¹EN={len(content_en)}å­— å†…å®¹ZH={len(content_zh)}å­—")

    html = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AIèµ„è®¯æ—¥æŠ¥ {today} Â· ç¬¬{index}æ¡</title>
<style>
*{{margin:0;padding:0;box-sizing:border-box}}
body{{
  font-family:-apple-system,BlinkMacSystemFont,"Segoe UI","PingFang SC","Microsoft YaHei",
              "Helvetica Neue",Arial,sans-serif;
  background:#f0f2f5;color:#1a1a1a;line-height:1.8;min-height:100vh;
  display:flex;flex-direction:column;
}}
.header{{
  background:linear-gradient(135deg,#0052cc 0%,#1a75ff 100%);
  color:#fff;padding:22px 32px;flex-shrink:0;
}}
.header-inner{{max-width:1100px;margin:0 auto;}}
.header h1{{font-size:20px;font-weight:700;margin-bottom:8px;letter-spacing:.02em;}}
.badges{{display:flex;gap:10px;flex-wrap:wrap;margin-top:6px;}}
.badge{{
  background:rgba(255,255,255,.20);border-radius:20px;
  padding:3px 12px;font-size:12px;white-space:nowrap;
}}
.main{{
  flex:1;max-width:1100px;width:100%;
  margin:24px auto;padding:0 16px 16px;
}}
.bilingual-wrapper{{
  display:grid;grid-template-columns:1fr 1fr;
  background:#fff;border-radius:12px;
  box-shadow:0 2px 20px rgba(0,0,0,.10);
  overflow:hidden;min-height:260px;
}}
.col{{padding:28px 28px;}}
.col.en{{background:#f7f9fc;border-right:1px solid #e5eaf0;}}
.col.zh{{background:#fff;}}
.lang-tag{{
  display:inline-flex;align-items:center;gap:5px;
  font-size:11px;font-weight:700;letter-spacing:.14em;text-transform:uppercase;
  color:#0052cc;background:#e6eeff;border-radius:4px;
  padding:3px 10px;margin-bottom:14px;
}}
.col.zh .lang-tag{{color:#c0392b;background:#fdecea;}}
.col-title{{
  font-size:17px;font-weight:700;line-height:1.5;
  color:#111;margin-bottom:14px;
}}
.col-content{{font-size:14px;line-height:1.95;color:#444;}}
.footer{{
  max-width:1100px;width:100%;margin:0 auto;
  padding:14px 16px 32px;
  display:flex;align-items:center;justify-content:space-between;
  flex-wrap:wrap;gap:10px;
}}
.btn{{
  display:inline-block;padding:9px 20px;border-radius:8px;
  font-size:13px;font-weight:600;text-decoration:none;
  cursor:pointer;transition:all .15s ease;border:none;
}}
.btn-primary{{background:#0052cc;color:#fff;}}
.btn-primary:hover{{background:#003d99;}}
.btn-ghost{{
  background:#fff;color:#333;
  border:1px solid #d0d5dd;cursor:pointer;
}}
.btn-ghost:hover{{background:#f4f5f7;}}
.footer-note{{font-size:12px;color:#aaa;}}
@media(max-width:640px){{
  .bilingual-wrapper{{grid-template-columns:1fr;}}
  .col.en{{border-right:none;border-bottom:1px solid #e5eaf0;}}
  .header{{padding:16px;}}
  .col{{padding:18px 16px;}}
  .header h1{{font-size:17px;}}
}}
</style>
</head>
<body>
<div class="header">
  <div class="header-inner">
    <h1>ğŸ¤– AIèµ„è®¯æ—¥æŠ¥ Â· ä¸­è‹±åŒè¯­å¯¹ç…§</h1>
    <div class="badges">
      <span class="badge">ğŸ“… {today}</span>
      <span class="badge">ç¬¬ {index} æ¡</span>
      <span class="badge">ğŸ“¡ {source}</span>
      <span class="badge">ğŸ”¥ çƒ­åº¦ {hot_score}</span>
    </div>
  </div>
</div>

<div class="main">
  <div class="bilingual-wrapper">
    <div class="col en">
      <div class="lang-tag">ğŸ“ English Original</div>
      <div class="col-title">{title_en}</div>
      <div class="col-content">{content_en}</div>
    </div>
    <div class="col zh">
      <div class="lang-tag">ğŸ“ ä¸­æ–‡ç¿»è¯‘</div>
      <div class="col-title">{title_zh}</div>
      <div class="col-content">{content_zh}</div>
    </div>
  </div>
</div>

<div class="footer">
  <div style="display:flex;gap:10px;flex-wrap:wrap;">
    <a class="btn btn-primary" href="{link}" target="_blank">ğŸ”— æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
    <button class="btn btn-ghost"
      onclick="try{{if(window.history.length>1){{window.history.back();}}else{{window.close();}}}}catch(e){{window.close();}}">
      â† å…³é—­
    </button>
  </div>
  <span class="footer-note">æ¥æºï¼š{source} Â· AIèµ„è®¯æ—¥æŠ¥è‡ªåŠ¨æ¨é€</span>
</div>
</body>
</html>"""
    return html


# ===================== Gist ä¸Šä¼  =====================
@retry
def upload_to_gist(html, index):
    """ä¸Šä¼  HTML åˆ° Gistï¼Œè¿”å› htmlpreview æ¸²æŸ“é“¾æ¥ï¼ˆWi-Fi ç¯å¢ƒä¸‹å¯è®¿é—®ï¼‰"""
    if not (GIST_TOKEN and len(GIST_TOKEN) > 10):
        logging.error("âŒ GIST_TOKEN æœªé…ç½®æˆ–è¿‡çŸ­")
        return None

    file_name = f"ai_news_{index}_{get_today()}.html"
    resp = requests.post(
        "https://api.github.com/gists",
        headers={
            "Authorization": f"token {GIST_TOKEN}",
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "AI-News-Daily/6.0"
        },
        json={
            "files": {file_name: {"content": html}},
            "public": True,
            "description": f"AIèµ„è®¯æ—¥æŠ¥ç¬¬{index}æ¡ - {get_today()}"
        },
        timeout=25
    )
    if resp.status_code == 201:
        res      = resp.json()
        gist_id  = res["id"]
        username = res["owner"]["login"]
        raw_url  = f"https://gist.githubusercontent.com/{username}/{gist_id}/raw/{file_name}"
        rendered = f"https://htmlpreview.github.io/?{raw_url}"
        logging.info(f"âœ… Gistä¸Šä¼ æˆåŠŸ")
        return rendered
    logging.error(f"âŒ Gistä¸Šä¼ å¤±è´¥ {resp.status_code}: {resp.text[:100]}")
    return None



    """
    å°† HTML å†™å…¥ä»“åº“ docs/ ç›®å½•ï¼Œé€šè¿‡ GitHub Pages è®¿é—®ã€‚
    URL æ ¼å¼ï¼šhttps://diaozhan234-png.github.io/ai-news-daily/æ–‡ä»¶å.html
    GitHub Pages åœ¨å›½å†…è®¿é—®ç¨³å®šï¼Œè§£å†³ htmlpreview SSL é—®é¢˜ã€‚
    """
    if not (GIST_TOKEN and len(GIST_TOKEN) > 10):
        return None

    file_name = f"news_{index}_{get_today()}.html"
    api_url   = f"https://api.github.com/repos/diaozhan234-png/ai-news-daily/contents/docs/{file_name}"
    headers   = {
        "Authorization": f"token {GIST_TOKEN}",
        "Accept": "application/vnd.github.v3+json",
        "User-Agent": "AI-News-Daily/6.0"
    }

    import base64
    content_b64 = base64.b64encode(html.encode("utf-8")).decode("ascii")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ›´æ–°éœ€è¦ shaï¼‰
    sha = None
    check = requests.get(api_url, headers=headers, timeout=15)
    if check.status_code == 200:
        sha = check.json().get("sha")

    body = {
        "message": f"Add news {index} for {get_today()}",
        "content": content_b64,
    }
    if sha:
        body["sha"] = sha

    resp = requests.put(api_url, headers=headers, json=body, timeout=25)
    if resp.status_code in (200, 201):
        pages_url = f"https://diaozhan234-png.github.io/ai-news-daily/{file_name}"
        logging.info(f"âœ… GitHub Pages ä¸Šä¼ æˆåŠŸ: {pages_url}")
        return pages_url

    logging.error(f"âŒ GitHub Pages ä¸Šä¼ å¤±è´¥ {resp.status_code}: {resp.text[:100]}")
    return None
    """
    ä¸Šä¼  HTML åˆ° Gistï¼Œè¿”å›å›½å†…å¯è®¿é—®çš„é“¾æ¥ã€‚
    
    æ–¹æ¡ˆï¼šä½¿ç”¨ cdn.jsdelivr.net ä½œä¸º CDN ä»£ç†è®¿é—® Gist raw å†…å®¹ã€‚
    jsdelivr åœ¨å›½å†…æœ‰ CDN èŠ‚ç‚¹ï¼Œè®¿é—®é€Ÿåº¦å¿«ä¸”ç¨³å®šï¼Œæ”¯æŒç›´æ¥æ¸²æŸ“ HTMLã€‚
    é“¾æ¥æ ¼å¼ï¼šhttps://cdn.jsdelivr.net/gh/ç”¨æˆ·å/ä»“åº“@åˆ†æ”¯/æ–‡ä»¶è·¯å¾„
    
    ç”±äº Gist æ— æ³•ç›´æ¥ç”¨ jsdelivrï¼Œæ”¹ç”¨å¦ä¸€æ–¹æ¡ˆï¼š
    å°† HTML è½¬ä¸º base64 data URIï¼Œé£ä¹¦å†…åµŒæµè§ˆå™¨å¯ç›´æ¥æ¸²æŸ“ï¼Œæ— éœ€å¤–éƒ¨æœåŠ¡ã€‚
    """
    if not (GIST_TOKEN and len(GIST_TOKEN) > 10):
        logging.error("âŒ GIST_TOKEN æœªé…ç½®æˆ–è¿‡çŸ­")
        return None   # è¿”å› None è¡¨ç¤ºä¸ç”Ÿæˆå¤–é“¾ï¼Œæ”¹ç”¨å†…åµŒæ–¹å¼

    file_name = f"ai_news_{index}_{get_today()}.html"
    resp = requests.post(
        "https://api.github.com/gists",
        headers={
            "Authorization": f"token {GIST_TOKEN}",
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "AI-News-Daily/5.0"
        },
        json={
            "files": {file_name: {"content": html}},
            "public": True,
            "description": f"AIèµ„è®¯æ—¥æŠ¥ç¬¬{index}æ¡ - {get_today()}"
        },
        timeout=25
    )
    if resp.status_code == 201:
        res      = resp.json()
        gist_id  = res["id"]
        username = res["owner"]["login"]
        # å°è¯•å¤šä¸ªé•œåƒï¼Œè¿”å›ç¬¬ä¸€ä¸ªï¼ˆé£ä¹¦ä¼šæŒ‰é¡ºåºå°è¯•ï¼‰
        # raw.githubusercontent.com æœ‰æ—¶å›½å†…å¯è®¿é—®
        raw_url = f"https://gist.githubusercontent.com/{username}/{gist_id}/raw/{file_name}"
        logging.info(f"âœ… Gistä¸Šä¼ æˆåŠŸ: {raw_url}")
        return raw_url
    logging.error(f"âŒ Gistä¸Šä¼ å¤±è´¥ {resp.status_code}: {resp.text[:120]}")
    return None


def make_data_uri(html):
    """å°† HTML è½¬ä¸º base64 data URIï¼Œå¯åœ¨ä»»ä½•æµè§ˆå™¨ç›´æ¥æ‰“å¼€ï¼Œæ— éœ€å¤–éƒ¨æœåŠ¡"""
    encoded = base64.b64encode(html.encode("utf-8")).decode("ascii")
    return f"data:text/html;base64,{encoded}"


# ===================== çˆ¬è™«ï¼šèšç„¦ AI æŠ€æœ¯/åº”ç”¨/æŠ•èèµ„ =====================
#
# æ¥æºé€‰æ‹©åŸåˆ™ï¼š
#   â‘  æŠ€æœ¯å‰æ²¿ï¼šarXiv cs.AI / cs.LG / cs.CLï¼ˆæ¨¡å‹ã€ç®—æ³•ï¼‰
#   â‘¡ äº§å“åŠ¨æ€ï¼šOpenAI Blogã€Anthropic Newsã€Google DeepMind Blog
#   â‘¢ è¡Œä¸šèµ„è®¯ï¼šTechCrunch AIã€VentureBeat AIã€MIT Tech Review AI
#   â‘£ æŠ•èèµ„ï¼šThe Information AIï¼ˆéœ€è®¢é˜…å¯æ¢ï¼‰ã€Forbes AI
#   â‘¤ å·¥å…·èšåˆï¼šOpenTools AIã€AI Newsï¼ˆainews.ioï¼‰
#   â‘¥ ç¤¾åŒºçƒ­ç‚¹ï¼šHackerNewsï¼ˆAI/LLMç›¸å…³ï¼‰
#
# AIç›¸å…³æ€§å…³é”®è¯ï¼ˆç”¨äºè¿‡æ»¤æ‰€æœ‰æ¥æºçš„éAIæ–‡ç« ï¼‰
AI_FILTER_KEYWORDS = [
    "artificial intelligence", " ai ", "machine learning", "deep learning",
    "large language model", "llm", "chatgpt", "gpt-", "claude", "gemini",
    "neural network", "generative ai", "openai", "anthropic", "deepmind",
    "nvidia", "foundation model", "transformer", "diffusion model",
    "autonomous", "robotics", "computer vision", "natural language",
    "reinforcement learning", "fine-tun", "inference", "multimodal",
    "rag", "agent", "copilot", "hugging face", "mistral", "llama",
    "funding", "investment", "startup", "raises", "valued",  # æŠ•èèµ„å…³é”®è¯
]

# ===================== é‡ç‚¹ç›‘æ§å…¬å¸é…ç½® =====================
# ç”¨æˆ·æŒ‡å®šçš„é‡ç‚¹å…³æ³¨å…¬å¸ - è¿™äº›å…¬å¸çš„åŠ¨æ€ä¼˜å…ˆæ¨é€
TARGET_COMPANIES = {
    # è‹±æ–‡å/å…³é”®è¯
    "openai":       ["openai", "chatgpt", "gpt-4", "gpt-5", "sora", "o1", "o3", "sam altman"],
    "google":       ["google ai", "google deepmind", "deepmind", "gemini", "google cloud ai",
                     "vertex ai", "google bard", "sundar pichai"],
    "anthropic":    ["anthropic", "claude ", "dario amodei", "amanda askell"],
    "microsoft":    ["microsoft ai", "copilot", "azure ai", "bing ai"],
    "meta":         ["meta ai", "llama", "meta llm"],
    "deepseek":     ["deepseek", "deep seek"],
    "manus":        ["manus ai", "manus agent", "monica ai"],
    # ä¸­æ–‡å…¬å¸ï¼ˆç”¨æ‹¼éŸ³/è‹±æ–‡åæ£€ç´¢ï¼‰
    "tencent":      ["tencent ai", "tencent", "è…¾è®¯", "æ··å…ƒ", "hunyuan"],
    "bytedance":    ["bytedance", "byte dance", "doubao", "å­—èŠ‚", "è±†åŒ…", "coze"],
    "alibaba":      ["alibaba ai", "alibaba", "qwen", "tongyi", "é€šä¹‰", "é˜¿é‡Œ"],
    "kimi":         ["kimi", "moonshot ai", "æœˆä¹‹æš—é¢"],
    "zhipu":        ["zhipu", "chatglm", "glm-", "æ™ºè°±"],
    "minmax":       ["minmax", "min-max", "abab", "minimax"],
}

# æ‰å¹³åŒ–æˆä¸€ä¸ªå…³é”®è¯é›†åˆï¼Œç”¨äºå¿«é€ŸåŒ¹é…
TARGET_KEYWORDS = []
for kws in TARGET_COMPANIES.values():
    TARGET_KEYWORDS.extend(kws)

# å…³æ³¨ç»´åº¦ï¼šæŠ€æœ¯/äº§å“/å•†ä¸š/äººæ‰
FOCUS_DIMENSIONS = [
    # æŠ€æœ¯çªç ´
    "new model", "new release", "launches", "released", "announced",
    "breakthrough", "benchmark", "outperforms", "beats", "surpasses",
    "open source", "open-source", "research paper", "technical report",
    # äº§å“ä¸åº”ç”¨
    "product", "feature", "update", "version", "api", "platform",
    "app", "tool", "integration", "plugin", "enterprise",
    # å•†ä¸šåŒ–
    "funding", "investment", "raises", "valued", "valuation",
    "revenue", "customers", "partnership", "deal", "acquisition",
    "ipo", "series", "million", "billion",
    # äººæ‰
    "hires", "hired", "joins", "leaves", "fired", "departs",
    "ceo", "cto", "vp", "chief", "president", "executive",
    "talent", "team",
]


def is_target_company_news(title, summary=""):
    """åˆ¤æ–­æ˜¯å¦æ¶‰åŠé‡ç‚¹ç›‘æ§å…¬å¸ï¼Œè¿”å› (bool, company_name)"""
    text = (title + " " + summary[:200]).lower()
    for company, keywords in TARGET_COMPANIES.items():
        if any(kw in text for kw in keywords):
            return True, company
    return False, None


def is_ai_related(title, summary=""):
    """
    åˆ¤æ–­æ–‡ç« æ˜¯å¦ä¸AIç§‘æŠ€ç›¸å…³ã€‚
    ä¼˜å…ˆçº§ï¼šé‡ç‚¹å…¬å¸ > AIæ ¸å¿ƒè¯ > å®½æ³›AIè¯
    è´Ÿå‘è¯è¿‡æ»¤ï¼šåŒ»å­¦/æ”¿æ²»è¯é¢˜æ’é™¤
    """
    text        = (title + " " + summary).lower()
    title_lower = title.lower()

    # æ”¿æ²»/å®‰å…¨/å†›äº‹ç±»è´Ÿå‘è¯ â€” å³ä½¿æ¶‰åŠé‡ç‚¹å…¬å¸ä¹Ÿæ’é™¤
    # ç”¨æˆ·åªå…³æ³¨æŠ€æœ¯ã€äº§å“ã€å•†ä¸šåŒ–ã€äººæ‰ï¼Œä¸éœ€è¦æ”¿æ²»æ–°é—»
    HARD_EXCLUDE = [
        # æ”¿æ²»å¯¹æŠ—
        "cyberwar", "cyber war", "espionage", "surveillance state",
        "disinformation", "propaganda", "influence operation",
        "election interference", "congressional", "senate hearing",
        "geopolitical", "sanctions", "export ban", "trade war",
        "national security", "warfare", "bioweapon", "nuclear weapon",
        "chinese government", "chinese official", "beijing government",
        "cia", "nsa", "fbi", "doj ", "white house",
        "lawmaker", "legislat",
        # çº¯æ”¿æ²»
        "election", "congress", "senate", "trump", "biden",
        "immigration", "deportat",
        # åŒ»å­¦
        "cancer", "tumor", "gene therapy", "vaccine", "drug trial",
        "surgery", "clinical trial", "patient", "hospital",
        "obesity", "diabetes", "cardiovascular",
        # è‡ªç„¶ç¾å®³/ç¯å¢ƒ
        "earthquake", "hurricane", "flood", "wildfire", "climate change",
    ]

    has_hard_exclude = any(kw in title_lower for kw in HARD_EXCLUDE)

    # ç¡¬æ’é™¤ï¼šæ— è®ºæ¶‰åŠå“ªå®¶å…¬å¸ï¼Œæ”¿æ²»/å†›äº‹/åŒ»å­¦å†…å®¹ä¸€å¾‹è¿‡æ»¤
    if has_hard_exclude:
        logging.debug(f"  [ç¡¬æ’é™¤] {title[:50]}")
        return False

    # é‡ç‚¹ç›‘æ§å…¬å¸ï¼ˆæ’é™¤æ”¿æ²»å†…å®¹åï¼‰
    is_target, _ = is_target_company_news(title, summary)
    if is_target:
        return True

    # æŠ€æœ¯ç±»æ ¸å¿ƒAIè¯
    CORE_AI_WORDS = [
        "large language model", "llm", "foundation model",
        "generative ai", "ai model", "ai system", "ai tool",
        "machine learning model", "deep learning model",
        "neural network", "transformer model", "diffusion model",
        "ai chip", "nvidia gpu", "ai infrastructure",
        "ai startup", "ai funding", "ai investment",
        "ai agent", "ai assistant", "ai platform",
    ]
    has_core_ai = any(kw in text for kw in CORE_AI_WORDS)
    if has_core_ai:
        return True

    BROAD_AI_WORDS = [
        " ai ", "artificial intelligence", "machine learning",
        "neural", "autonomous", "automation",
        "inference", "fine-tun", "embedding", "multimodal",
        "rag ", "hugging face", "raises $", "million round",
    ]
    return any(kw in text for kw in BROAD_AI_WORDS)


PUSHED_TITLES_FILE = "/tmp/ai_news_pushed_titles.txt"  # GitHub Actions æ¯æ¬¡è¿è¡Œæ˜¯å…¨æ–°ç¯å¢ƒï¼Œæ”¹ç”¨ Gist æŒä¹…åŒ–


def load_pushed_titles():
    """ä» Gist åŠ è½½å†å²æ¨é€æ ‡é¢˜ï¼ˆè·¨å¤©å»é‡ï¼‰"""
    if not (GIST_TOKEN and len(GIST_TOKEN) > 10):
        return set()
    try:
        resp = requests.get(
            "https://api.github.com/gists",
            headers={
                "Authorization": f"token {GIST_TOKEN}",
                "Accept": "application/vnd.github.v3+json",
            },
            params={"per_page": 30},
            timeout=15
        )
        for gist in resp.json():
            if gist.get("description") == "AI_NEWS_DEDUP_CACHE":
                gist_id = gist["id"]
                detail  = requests.get(
                    f"https://api.github.com/gists/{gist_id}",
                    headers={"Authorization": f"token {GIST_TOKEN}"},
                    timeout=15
                ).json()
                content = list(detail["files"].values())[0]["content"]
                titles  = set(line.strip() for line in content.splitlines() if line.strip())
                logging.info(f"ğŸ“š åŠ è½½å†å²æ¨é€è®°å½•: {len(titles)} æ¡")
                return titles
    except Exception as e:
        logging.warning(f"âš ï¸ åŠ è½½å»é‡ç¼“å­˜å¤±è´¥: {e}")
    return set()


def save_pushed_titles(titles):
    """å°†æ¨é€æ ‡é¢˜ä¿å­˜åˆ° Gistï¼ˆæœ€è¿‘7å¤©ï¼Œçº¦35æ¡ï¼‰"""
    if not (GIST_TOKEN and len(GIST_TOKEN) > 10):
        return
    # åªä¿ç•™æœ€è¿‘35æ¡ï¼Œé˜²æ­¢æ— é™å¢é•¿
    titles_list = list(titles)[-35:]
    content     = "\n".join(titles_list)
    try:
        # å…ˆæŸ¥æ‰¾å·²æœ‰çš„ç¼“å­˜ Gist
        resp = requests.get(
            "https://api.github.com/gists",
            headers={"Authorization": f"token {GIST_TOKEN}"},
            params={"per_page": 30},
            timeout=15
        )
        existing_id = None
        for gist in resp.json():
            if gist.get("description") == "AI_NEWS_DEDUP_CACHE":
                existing_id = gist["id"]
                break

        headers = {
            "Authorization": f"token {GIST_TOKEN}",
            "Accept": "application/vnd.github.v3+json",
        }
        body = {
            "description": "AI_NEWS_DEDUP_CACHE",
            "files": {"dedup_cache.txt": {"content": content}},
            "public": False
        }

        if existing_id:
            requests.patch(
                f"https://api.github.com/gists/{existing_id}",
                headers=headers, json=body, timeout=15
            )
        else:
            requests.post(
                "https://api.github.com/gists",
                headers=headers, json=body, timeout=15
            )
        logging.info(f"ğŸ’¾ å»é‡ç¼“å­˜å·²ä¿å­˜: {len(titles_list)} æ¡")
    except Exception as e:
        logging.warning(f"âš ï¸ ä¿å­˜å»é‡ç¼“å­˜å¤±è´¥: {e}")


CHINESE_DOMAINS = [
    "sina.com.cn", "sina.cn", "sohu.com", "163.com", "qq.com",
    "weibo.com", "zhihu.com", "36kr.com", "ifeng.com", "xinhua",
    "people.com.cn", "cnbeta", "sspai.com", "jiemian.com",
    "jiqizhixin.com", "leiphone.com", "infoq.cn", "oschina.net",
    "baidu.com", "toutiao.com", "csdn.net", "juejin.cn",
]


def is_chinese_url(url):
    """åˆ¤æ–­ URL æ˜¯å¦æŒ‡å‘ä¸­æ–‡ç«™ç‚¹"""
    return any(d in url for d in CHINESE_DOMAINS)


def _make_article(entry, source, hot_range):
    """é€šç”¨æ–‡ç« æ„å»ºï¼štitleç¿»è¯‘ + æ­£æ–‡è·å–ç¿»è¯‘ã€‚ä¸­æ–‡ç«™ç‚¹è¿”å› Noneã€‚"""
    link = getattr(entry, "link", "") or ""
    if is_chinese_url(link):
        logging.warning(f"  ğŸš« ä¸­æ–‡ç«™ç‚¹è·³è¿‡: {link[:60]}")
        return None
    title       = safe_translate(clean_title(entry.title))
    raw_content = get_rich_content(entry, entry.link)   # å®Œæ•´æ­£æ–‡ï¼Œä¸æˆªæ–­
    content     = safe_translate(raw_content)           # åˆ†æ®µç¿»è¯‘å…¨æ–‡
    return {
        "title":     title,
        "content":   content,
        "link":      entry.link,
        "source":    source,
        "hot_score": round(random.uniform(*hot_range), 1)
    }


def _make_article_with_company(entry, source, hot_range, company_tag=None):
    """æ„å»ºæ–‡ç« ï¼Œé™„å¸¦å…¬å¸æ ‡ç­¾ï¼ˆç”¨äºé£ä¹¦å¡ç‰‡æ˜¾ç¤ºï¼‰"""
    article = _make_article(entry, source, hot_range)
    if company_tag:
        article["company_tag"] = company_tag
    return article


def crawl_target_company_news():
    """
    é‡ç‚¹å…¬å¸åŠ¨æ€ä¸“é¡¹çˆ¬è™«ã€‚
    æ¥æºï¼šGoogle News RSSï¼ˆæ”¯æŒä¸­æ–‡å…¬å¸æœç´¢ï¼‰+ å®˜æ–¹åšå®¢
    æ¯ä¸ªå…¬å¸æœç´¢æœ€æ–°åŠ¨æ€ï¼Œé‡ç‚¹å…³æ³¨ï¼šæŠ€æœ¯/äº§å“/å•†ä¸šåŒ–/äººæ‰
    """
    results = []

    # Google News RSS æœç´¢å„å…¬å¸ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
    COMPANY_QUERIES = [
        ("OpenAI",                      "OpenAI",    (88, 95)),
        ("Anthropic Claude AI",         "Anthropic", (87, 94)),
        ("Google Gemini AI",            "Google",    (86, 93)),
        ("DeepSeek AI model",           "DeepSeek",  (87, 94)),
        ("ByteDance AI Doubao",         "å­—èŠ‚è·³åŠ¨",  (85, 92)),
        ("Tencent AI Hunyuan",          "è…¾è®¯",      (84, 91)),
        ("Alibaba Qwen AI model",       "é˜¿é‡Œå·´å·´",  (84, 91)),
        ("Kimi Moonshot AI",            "Kimi",      (83, 90)),
        ("Zhipu AI ChatGLM",            "æ™ºè°±AI",    (83, 90)),
        ("MiniMax AI model",            "MiniMax",   (82, 89)),
        ("Manus AI agent",              "Manus",     (83, 90)),
    ]

    tried = 0
    for query, company, hot_range in COMPANY_QUERIES:
        if len(results) >= 5:   # æœ€å¤šè´¡çŒ®5æ¡
            break
        try:
            # å¼ºåˆ¶ä½¿ç”¨è‹±æ–‡ç‰ˆ Google Newsï¼Œé¿å…è¿”å›ä¸­æ–‡è½åœ°é¡µ
            rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=en-US&gl=US&ceid=US:en"
            feed = feedparser.parse(rss_url)
            tried += 1

            if not feed.entries:
                continue

            for entry in feed.entries[:8]:  # æ¯å®¶å…¬å¸æœ€å¤šæ£€æŸ¥8æ¡
                title   = getattr(entry, "title", "")
                summary = getattr(entry, "summary", "")

                # ç¡®è®¤æ¶‰åŠè¯¥å…¬å¸ä¸”æœ‰å®è´¨å†…å®¹
                is_target, _ = is_target_company_news(title, summary)
                if not is_target:
                    continue
                if len(title) < 10:
                    continue
                # æ”¿æ²»/å†›äº‹ç±»å†…å®¹è¿‡æ»¤ï¼ˆå³ä½¿æ¶‰åŠé‡ç‚¹å…¬å¸ï¼‰
                if not is_ai_related(title, summary):
                    logging.warning(f"  ğŸš« å…¬å¸çˆ¬è™«æ”¿æ²»å†…å®¹è¿‡æ»¤: {title[:50]}")
                    continue

                logging.info(f"ğŸ¯ é‡ç‚¹å…¬å¸ [{company}]: {title[:60]}")
                # è§£æçœŸå® URLï¼ŒNone è¡¨ç¤ºä¸­æ–‡è½åœ°é¡µï¼Œè·³è¿‡
                real_link = resolve_google_news_url(entry.link)
                if real_link is None:
                    logging.warning(f"  âš ï¸ ä¸­æ–‡è½åœ°é¡µï¼Œè·³è¿‡: {title[:40]}")
                    continue
                article = _make_article(entry, f"Google News Â· {company}", hot_range)
                if article is None:
                    logging.warning(f"  âš ï¸ ä¸­æ–‡ç«™ç‚¹ï¼Œè·³è¿‡: {title[:40]}")
                    continue
                article["link"]        = real_link

                # å†…å®¹è´¨é‡æ£€æŸ¥ï¼šæ­£æ–‡å¤ªçŸ­è¯´æ˜æŠ“å–å¤±è´¥ï¼Œæ¢ä¸‹ä¸€æ¡
                content_en = (article.get("content") or {}).get("en", "")
                if len(content_en.strip()) < 80:
                    logging.warning(f"  âš ï¸ æ­£æ–‡è¿‡çŸ­({len(content_en)}å­—)ï¼Œå°è¯•ä¸‹ä¸€æ¡: {title[:40]}")
                    continue

                article["company_tag"] = company
                results.append(article)
                break   # æ¯å®¶å…¬å¸åªå–æœ€æ–°1æ¡

        except Exception as e:
            logging.warning(f"âš ï¸ å…¬å¸çˆ¬è™« [{company}]: {e}")

    logging.info(f"é‡ç‚¹å…¬å¸çˆ¬è™«å®Œæˆ: å°è¯•{tried}å®¶ï¼Œè·å–{len(results)}æ¡")
    return results


def crawl_arxiv():
    """arXiv â€” åªæŠ“AI/ML/NLPæ ¸å¿ƒç ”ç©¶è®ºæ–‡"""
    ARXIV_MUST_HAVE = [
        "language model", "llm", "large language", "neural network",
        "deep learning", "transformer", "diffusion model", "generative model",
        "reinforcement learning", "fine-tuning", "pre-train", "foundation model",
        "prompt", "chatgpt", "gpt", "bert", "attention mechanism",
        "multimodal", "text generation", "image generation", "reasoning",
        "alignment", "rlhf", "in-context learning", "chain-of-thought",
        "ai agent", "llm agent", "retrieval augmented", "embedding model",
    ]
    ARXIV_EXCLUDE_IF_NO_CORE = [
        "obesity", "overweight", "health", "medical", "clinical", "patient",
        "cancer", "disease", "diagnosis", "hospital", "drug", "genomic",
        "covid", "pandemic", "social media", "education", "finance",
        "traffic", "weather", "earthquake", "flood", "agriculture",
        "children", "adolescent", "elderly", "population",
    ]
    try:
        for category in ["cs.AI", "cs.CL", "cs.LG"]:
            feed = feedparser.parse(f"http://export.arxiv.org/rss/{category}")
            if not feed.entries:
                continue
            for entry in feed.entries[:10]:
                title    = entry.title.lower()
                summary  = strip_html(getattr(entry, "summary", "")).lower()
                combined = title + " " + summary[:300]

                # ä¼˜å…ˆï¼šè®ºæ–‡æ¶‰åŠé‡ç‚¹å…¬å¸ï¼ˆå¦‚ DeepSeek/OpenAI å‘è¡¨çš„è®ºæ–‡ï¼‰
                is_target, company = is_target_company_news(entry.title, summary)
                has_core    = any(kw in combined for kw in ARXIV_MUST_HAVE)
                has_exclude = any(kw in title for kw in ARXIV_EXCLUDE_IF_NO_CORE)

                if has_exclude and not is_target:
                    logging.warning(f"arXiv ğŸš«è·¨å­¦ç§‘: {entry.title[:50]}")
                    continue
                if has_core or is_target:
                    logging.info(f"arXiv [{category}] âœ…: {entry.title[:60]}")
                    return [_make_article(entry, "arXiv å­¦æœ¯è®ºæ–‡", (88, 93))]

        logging.warning("âš ï¸ arXiv: æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
        return []
    except Exception as e:
        logging.error(f"âŒ arXiv: {e}")
        return []


def crawl_openai():
    """OpenAI å®˜æ–¹åšå®¢ â€” äº§å“/æ¨¡å‹åŠ¨æ€"""
    try:
        feed = feedparser.parse("https://openai.com/blog/rss/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        logging.info(f"OpenAI: {entry.title[:50]}")
        return [_make_article(entry, "OpenAI å®˜æ–¹åšå®¢", (86, 92))]
    except Exception as e:
        logging.error(f"âŒ OpenAI: {e}")
        return []


def crawl_anthropic():
    """Anthropic å®˜æ–¹æ–°é—» â€” Claude/å®‰å…¨ç ”ç©¶"""
    try:
        # Anthropic æš‚æ— æ ‡å‡† RSSï¼Œä½¿ç”¨å…¶ news é¡µé¢çš„ Atom
        feed = feedparser.parse("https://www.anthropic.com/news/rss")
        if not feed.entries:
            feed = feedparser.parse("https://www.anthropic.com/feed")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        logging.info(f"Anthropic: {entry.title[:50]}")
        return [_make_article(entry, "Anthropic å®˜æ–¹", (85, 91))]
    except Exception as e:
        logging.error(f"âŒ Anthropic: {e}")
        return []


def crawl_google_deepmind():
    """Google DeepMind Blog â€” å‰æ²¿æ¨¡å‹/ç ”ç©¶"""
    try:
        for rss in [
            "https://deepmind.google/blog/rss.xml",
            "https://blog.google/technology/ai/rss/",
            "https://developers.googleblog.com/feeds/posts/default?alt=rss",
        ]:
            feed = feedparser.parse(rss)
            if feed.entries:
                entry = feed.entries[0]
                logging.info(f"Google/DeepMind: {entry.title[:50]}")
                return [_make_article(entry, "Google DeepMind", (84, 90))]
        return []
    except Exception as e:
        logging.error(f"âŒ Google DeepMind: {e}")
        return []


def crawl_mit_tech_review():
    """MIT Technology Review â€” ä¸¥æ ¼è¿‡æ»¤ï¼Œåªæ¨AI/ç§‘æŠ€ç›¸å…³æ–‡ç« """
    try:
        feed = feedparser.parse("https://www.technologyreview.com/feed/")
        if not feed.entries:
            return []

        for entry in feed.entries[:20]:
            title   = entry.title
            summary = getattr(entry, "summary", "")
            if is_ai_related(title, summary):
                logging.info(f"MIT Tech Review (AIåŒ¹é…): {title[:50]}")
                return [_make_article(entry, "MIT Technology Review", (85, 90))]

        logging.warning("âš ï¸ MIT Tech Review: å‰20æ¡å†…æ— AIç›¸å…³æ–‡ç« ï¼Œè·³è¿‡")
        return []
    except Exception as e:
        logging.error(f"âŒ MIT Tech Review: {e}")
        return []


def crawl_venturebeat():
    """VentureBeat AI â€” äº§å“å‘å¸ƒ/è¡Œä¸šåŠ¨æ€"""
    try:
        for rss in [
            "https://venturebeat.com/category/ai/feed/",
            "https://venturebeat.com/category/artificial-intelligence/feed/",
        ]:
            feed = feedparser.parse(rss)
            if feed.entries:
                entry = feed.entries[0]
                logging.info(f"VentureBeat: {entry.title[:50]}")
                return [_make_article(entry, "VentureBeat", (83, 89))]
        return []
    except Exception as e:
        logging.error(f"âŒ VentureBeat: {e}")
        return []


def crawl_techcrunch():
    """TechCrunch AI â€” æŠ•èèµ„/åˆ›ä¸š/äº§å“"""
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        logging.info(f"TechCrunch: {entry.title[:50]}")
        return [_make_article(entry, "TechCrunch", (82, 88))]
    except Exception as e:
        logging.error(f"âŒ TechCrunch: {e}")
        return []


def crawl_forbes():
    """Forbes AI â€” æŠ•èèµ„/å•†ä¸šåº”ç”¨"""
    try:
        for rss in [
            "https://www.forbes.com/innovation/artificial-intelligence/feed/",
            "https://www.forbes.com/technology/artificial-intelligence/feed/",
        ]:
            feed = feedparser.parse(rss)
            if feed.entries:
                entry = feed.entries[0]
                logging.info(f"Forbes: {entry.title[:50]}")
                return [_make_article(entry, "Forbes", (83, 89))]
        return []
    except Exception as e:
        logging.error(f"âŒ Forbes: {e}")
        return []


def crawl_opentools_ai():
    """OpenTools AI â€” æ–°å·¥å…·å‘å¸ƒ/åº”ç”¨åŠ¨æ€"""
    try:
        for rss in ["https://opentools.ai/rss", "https://opentools.ai/feed"]:
            feed = feedparser.parse(rss)
            if feed.entries:
                entry = feed.entries[0]
                logging.info(f"OpenTools AI: {entry.title[:50]}")
                return [_make_article(entry, "OpenTools AI", (81, 87))]
        return []
    except Exception as e:
        logging.error(f"âŒ OpenTools AI: {e}")
        return []


def crawl_hackernews():
    """
    HackerNews â€” åªæŠ“æœ‰å®è´¨å†…å®¹çš„AIç›¸å…³å¤–é“¾æ–‡ç« ã€‚
    è¿‡æ»¤è§„åˆ™ï¼š
      1. æ ‡é¢˜å¿…é¡» >= 20 å­—ç¬¦ï¼ˆæ’é™¤ "LLM=True" è¿™ç±»æ— æ„ä¹‰æ ‡é¢˜ï¼‰
      2. å¿…é¡»æœ‰å¤–éƒ¨é“¾æ¥ URLï¼ˆæ’é™¤çº¯ HN è®¨è®ºå¸–ï¼‰
      3. æ ‡é¢˜å¿…é¡»åŒ…å« AI æ ¸å¿ƒå…³é”®è¯
      4. æ’é™¤çº¯æŠ€æœ¯ä»£ç /åº“å‘å¸ƒï¼ˆè¿™ç±»å†…å®¹æ„ä¹‰ä¸å¤§ï¼‰
    """
    AI_KEYWORDS = [
        "llm", "large language model", "gpt", "claude", "gemini", "mistral",
        "llama", "machine learning", "neural", "transformer", "openai",
        "anthropic", "deepmind", "diffusion", "generative ai", "ai model",
        "artificial intelligence", "chatbot", "foundation model",
        "multimodal", "ai agent", "rag", "fine-tuning", "inference",
        "ai startup", "ai funding", "raises", "ai tool", "copilot",
    ]
    # æ’é™¤çº¯ä»£ç åº“/æ¡†æ¶å‘å¸ƒï¼ˆæ ‡é¢˜ç‰¹å¾ï¼‰
    EXCLUDE_PATTERNS = [
        "show hn", "ask hn", "tell hn",   # HN å†…éƒ¨å¸–
        "=true", "=false", "= true", "= false",  # ä»£ç ç‰‡æ®µ
        "[pdf]", "[video]",
    ]
    try:
        resp = requests.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json",
            timeout=GLOBAL_TIMEOUT
        )
        ids = resp.json()[:30]  # æ£€æŸ¥å‰30æ¡æé«˜å‘½ä¸­ç‡

        for story_id in ids:
            item = requests.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json",
                timeout=GLOBAL_TIMEOUT
            ).json()

            title = item.get("title", "")
            url   = item.get("url", "")

            # è´¨é‡é—¨æ§›
            if len(title) < 20:
                continue  # æ ‡é¢˜å¤ªçŸ­ï¼Œæ— å®è´¨å†…å®¹
            if not url:
                continue  # æ²¡æœ‰å¤–éƒ¨é“¾æ¥ï¼Œæ˜¯çº¯è®¨è®ºå¸–
            if any(p in title.lower() for p in EXCLUDE_PATTERNS):
                continue  # æ’é™¤ç‰¹å®šç±»å‹

            title_lower = title.lower()
            if any(kw in title_lower for kw in AI_KEYWORDS):
                # æŠ“å–å¤–é“¾æ­£æ–‡
                body = fetch_article_content(url) or strip_html(item.get("text", "")) or title
                if len(body) < 50:
                    body = title  # æ­£æ–‡å¤ªçŸ­ä¹Ÿä¸ç¿»è¯‘ç©ºå†…å®¹
                content = safe_translate(body)
                logging.info(f"HackerNews âœ…: {title[:60]}")
                return [{
                    "title":     safe_translate(title),
                    "content":   content,
                    "link":      url,
                    "source":    "HackerNews",
                    "hot_score": round(random.uniform(80, 86), 1)
                }]

        logging.warning("âš ï¸ HackerNews: å‰30æ¡å†…æ— ç¬¦åˆæ¡ä»¶çš„AIæ–‡ç« ")
        return []
    except Exception as e:
        logging.error(f"âŒ HackerNews: {e}")
        return []


# ===================== é£ä¹¦æ¨é€ =====================
def send_to_feishu(articles):
    """
    é£ä¹¦å¡ç‰‡æ¨é€ v6ï¼šåŒè¯­å…¨æ–‡ç›´æ¥å†™å…¥å¡ç‰‡ï¼Œå½»åº•ä¸ä¾èµ–å¤–éƒ¨é“¾æ¥ã€‚
    è§£å†³ htmlpreview.github.io å›½å†… SSL æŠ¥é”™é—®é¢˜ã€‚
    """
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½® FEISHU_WEBHOOK")
        return False

    IDX_EMOJI = {1: "1ï¸âƒ£", 2: "2ï¸âƒ£", 3: "3ï¸âƒ£", 4: "4ï¸âƒ£", 5: "5ï¸âƒ£"}
    SOURCE_ICON = {
        "arXiv å­¦æœ¯è®ºæ–‡":        "ğŸ“",
        "OpenAI å®˜æ–¹åšå®¢":       "ğŸ¤–",
        "Anthropic å®˜æ–¹":        "ğŸ§ ",
        "Google DeepMind":       "ğŸ”¬",
        "MIT Technology Review": "ğŸ“",
        "VentureBeat":           "ğŸ“Š",
        "TechCrunch":            "ğŸ’¡",
        "Forbes":                "ğŸ’°",
        "OpenTools AI":          "ğŸ› ï¸",
        "HackerNews":            "ğŸ”¥",
    }

    # é‡ç‚¹å…¬å¸æ ‡ç­¾æ ·å¼
    COMPANY_BADGE = {
        "OpenAI": "ğŸŸ¢", "Anthropic": "ğŸŸ ", "Google": "ğŸ”µ",
        "DeepSeek": "ğŸ”´", "å­—èŠ‚è·³åŠ¨": "âš«", "è…¾è®¯": "ğŸŸ£",
        "é˜¿é‡Œå·´å·´": "ğŸŸ¡", "Kimi": "ğŸŒ™", "æ™ºè°±AI": "ğŸ’",
        "MiniMax": "ğŸŒŠ", "Manus": "âš¡", "Microsoft": "ğŸ”·",
    }

    elements = []
    for idx, article in enumerate(articles, 1):
        title_zh    = (article.get("title")   or {}).get("zh") or (article.get("title") or {}).get("en") or "æ— æ ‡é¢˜"
        title_en    = (article.get("title")   or {}).get("en") or ""
        content_zh  = (article.get("content") or {}).get("zh") or (article.get("content") or {}).get("en") or "æš‚æ— æ‘˜è¦"
        content_en  = (article.get("content") or {}).get("en") or ""
        source      = article.get("source",     "æœªçŸ¥æ¥æº")
        hot_score   = article.get("hot_score",  "N/A")
        orig_link   = article.get("link", "#")
        company_tag = article.get("company_tag", "")

        num_emoji  = IDX_EMOJI.get(idx, f"{idx}.")
        src_icon   = SOURCE_ICON.get(source, "ğŸ“°")
        summary_zh = content_zh[:200] + "..." if len(content_zh) > 200 else content_zh

        company_line = ""
        if company_tag:
            badge = COMPANY_BADGE.get(company_tag, "ğŸ¢")
            company_line = f"{badge} **{company_tag}**ã€€"

        # æ‰€æœ‰æ–‡ç« ç»Ÿä¸€ä¸Šä¼  Gist ç”Ÿæˆä¸­è‹±å¯¹ç…§é“¾æ¥
        bilingual_url = upload_to_gist(generate_bilingual_html(article, idx), idx)

        # æ ‡é¢˜è¡Œ
        title_line = f"**è‹±æ–‡æ ‡é¢˜**ï¼š{title_en[:100]}\n\n" if title_en else ""

        # æŒ‰é’®
        action_buttons = []
        if bilingual_url:
            action_buttons.append({
                "tag": "button",
                "text": {"tag": "plain_text", "content": "ğŸ“„ æŸ¥çœ‹ä¸­è‹±å¯¹ç…§"},
                "type": "primary",
                "url": bilingual_url
            })
        if orig_link and orig_link != "#":
            action_buttons.append({
                "tag": "button",
                "text": {"tag": "plain_text", "content": "ğŸ”— æŸ¥çœ‹åŸæ–‡"},
                "type": "default",
                "url": orig_link
            })

        card_elements = [
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": (
                        f"**{num_emoji} {title_zh}**\n"
                        f"{company_line}{src_icon} {source}ã€€ğŸ”¥ çƒ­åº¦ {hot_score}\n\n"
                        f"{title_line}"
                        f"**ä¸­æ–‡æ‘˜è¦**ï¼š{summary_zh}"
                    )
                }
            },
        ]
        if action_buttons:
            card_elements.append({"tag": "action", "actions": action_buttons})
        card_elements.append({"tag": "hr"})
        elements.extend(card_elements)

    while elements and elements[-1].get("tag") == "hr":
        elements.pop()

    card = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title":    {"tag": "plain_text", "content": f"ğŸ¤– å…¨çƒAIèµ„è®¯æ—¥æŠ¥ | {get_today()}"},
            "template": "blue"
        },
        "elements": elements
    }

    try:
        resp   = requests.post(FEISHU_WEBHOOK, json={"msg_type": "interactive", "card": card},
                               timeout=GLOBAL_TIMEOUT)
        result = resp.json()
        if resp.status_code == 200 and (result.get("StatusCode") == 0 or result.get("code") == 0):
            logging.info("âœ… é£ä¹¦æ¨é€æˆåŠŸ")
            return True
        logging.error(f"âŒ é£ä¹¦æ¨é€å¤±è´¥: {resp.text}")
        return False
    except Exception as e:
        logging.error(f"âŒ é£ä¹¦æ¨é€å¼‚å¸¸: {e}")
        return False


# ===================== ä¸»å‡½æ•° =====================
def main():
    """
    ============================================================
    é—®é¢˜2 è§£ç­”ï¼šåŒ—äº¬æ—¶é—´ 09:30 è‡ªåŠ¨è§¦å‘
    ------------------------------------------------------------
    åœ¨ GitHub Actions çš„ workflow YAML ä¸­è®¾ç½®å¦‚ä¸‹ cronï¼š

      on:
        schedule:
          - cron: '30 1 * * *'   # UTC 01:30 = åŒ—äº¬æ—¶é—´ 09:30

    æ³¨æ„ï¼šGitHub Actions ä½¿ç”¨ UTC æ—¶é—´ï¼ŒåŒ—äº¬æ—¶é—´ï¼ˆCSTï¼‰= UTC + 8
    å› æ­¤ åŒ—äº¬ 09:30 â†’ UTC 01:30
    ============================================================
    """
    logging.info("ğŸš€ AIèµ„è®¯æ—¥æŠ¥ v6 å¯åŠ¨")
    logging.info(f"ğŸ“… ä»Šæ—¥æ—¥æœŸï¼š{get_today()}")

    # çˆ¬è™«åˆ—è¡¨ï¼šé‡ç‚¹å…¬å¸çˆ¬è™«ä¼˜å…ˆï¼Œå…¶ä»–çˆ¬è™«è¡¥å……
    crawlers = [
        crawl_target_company_news, # ğŸ¯ é‡ç‚¹å…¬å¸ä¸“é¡¹ç›‘æ§ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        crawl_openai,              # OpenAI å®˜æ–¹åšå®¢
        crawl_anthropic,           # Anthropic å®˜æ–¹
        crawl_google_deepmind,     # Google AI / DeepMind
        crawl_arxiv,               # å­¦æœ¯å‰æ²¿
        crawl_mit_tech_review,     # MIT æ·±åº¦åˆ†æ
        crawl_venturebeat,         # è¡Œä¸šèµ„è®¯
        crawl_techcrunch,          # æŠ•èèµ„/äº§å“
        crawl_forbes,              # å•†ä¸š/æŠ•èèµ„
        crawl_hackernews,          # ç¤¾åŒºçƒ­ç‚¹
    ]

    all_articles = []
    for crawler in crawlers:
        try:
            results = crawler() or []
            if results:
                all_articles.extend(results)
                logging.info(f"âœ… {crawler.__name__} â†’ {len(results)} æ¡")
            else:
                logging.warning(f"âš ï¸ {crawler.__name__} â†’ 0 æ¡")
        except Exception as e:
            logging.error(f"âŒ {crawler.__name__} å´©æºƒ: {e}")

    # è¿‡æ»¤1ï¼šå¿…é¡»æœ‰æ ‡é¢˜
    # è¿‡æ»¤2ï¼šå…¨å±€AIç›¸å…³æ€§æ£€æŸ¥
    # è¿‡æ»¤3ï¼šå†…å®¹è´¨é‡æ£€æŸ¥
    # è¿‡æ»¤4ï¼šæ ‡é¢˜å»é‡ï¼ˆåŒä¸€ç¯‡æ–‡ç« ä¸é‡å¤æ¨é€ï¼‰
    QUALITY_BLACKLIST = [
        "æœåŠ¡é”™è¯¯", "æœåŠ¡ç›®å‰ä¸å¯ç”¨", "é‚£æ˜¯ä¸ªé”™è¯¯", "é”™è¯¯-27",
        "error_code", "unauthorized", "rate limit",
        "that's an error", "service error -27", "not available at this time",
        "503 service", "access denied", "enable javascript",
        "our systems have detected", "cloudflare",
        # HackerNews PDFé“¾æ¥è¢«å½“æ­£æ–‡
        "pdf:", "https://arxiv.org/pdf", "https://arxiv.org/abs",
    ]

    seen_titles = load_pushed_titles()   # åŠ è½½å†å²æ¨é€è®°å½•ï¼ˆè·¨å¤©å»é‡ï¼‰
    logging.info(f"ğŸ“š å†å²å»é‡è®°å½•: {len(seen_titles)} æ¡")
    valid = []
    for a in all_articles:
        if not (a and isinstance(a.get("title"), dict) and a["title"].get("en")):
            continue
        title_en   = a["title"].get("en", "").strip()
        content_en = (a.get("content") or {}).get("en", "")
        content_zh = (a.get("content") or {}).get("zh", "")

        # ä¸­æ–‡ç«™ç‚¹è¿‡æ»¤
        article_link = a.get("link", "")
        if is_chinese_url(article_link):
            logging.warning(f"ğŸš« ä¸­æ–‡ç«™ç‚¹è¿‡æ»¤: {article_link[:60]}")
            continue

        # æ ‡é¢˜å»é‡
        title_key = title_en.lower()[:60]
        if title_key in seen_titles:
            logging.warning(f"ğŸš« é‡å¤æ ‡é¢˜ï¼Œè·³è¿‡: {title_en[:50]}")
            continue
        seen_titles.add(title_key)

        # AIç›¸å…³æ€§æ£€æŸ¥ï¼ˆå¯¹ "AI-designed proteins" è¿™ç±»
        # æ ‡é¢˜å«AIå­—æ¯ä½†å®é™…æ˜¯åŒ»å­¦æ–‡ç« ï¼Œcontentæ£€æŸ¥æ›´å¯é ï¼‰
        if not is_ai_related(title_en, content_en[:500]):
            logging.warning(f"ğŸš« å…¨å±€è¿‡æ»¤éAIå†…å®¹: {title_en[:50]}")
            continue

        # å†…å®¹è´¨é‡æ£€æŸ¥
        check_text = content_zh + content_en
        if any(p in check_text.lower() for p in [q.lower() for q in QUALITY_BLACKLIST]):
            logging.warning(f"ğŸš« å†…å®¹å«é”™è¯¯æ–‡æœ¬ï¼Œä¸¢å¼ƒ: {title_en[:50]}")
            continue

        # å†…å®¹é•¿åº¦æ£€æŸ¥ï¼šæ‘˜è¦å¤ªçŸ­ï¼ˆå°‘äº50å­—ï¼‰è¯´æ˜æ­£æ–‡æ²¡æŠ“åˆ°ï¼Œä¸¢å¼ƒ
        content_len = len(content_zh.strip())
        if content_len < 50:
            logging.warning(f"ğŸš« å†…å®¹è¿‡çŸ­({content_len}å­—)ï¼Œä¸¢å¼ƒ: {title_en[:50]}")
            continue

        # æ ‡é¢˜é•¿åº¦æ£€æŸ¥
        if len(title_en) < 10:
            logging.warning(f"ğŸš« æ ‡é¢˜è¿‡çŸ­ï¼Œä¸¢å¼ƒ: {title_en}")
            continue

        # é‡ç‚¹å…¬å¸æ–‡ç« åŠ åˆ†ï¼ˆç¡®ä¿æ’åœ¨å‰5æ¡ï¼‰
        is_target, company = is_target_company_news(title_en, content_en[:200])
        if is_target and not a.get("company_tag"):
            a["company_tag"] = company
        if is_target:
            a["hot_score"] = round(float(a.get("hot_score", 85) or 85) + 10, 1)

        valid.append(a)

    # æŒ‰çƒ­åº¦é™åº
    valid = sorted(valid, key=lambda x: float(x.get("hot_score", 0) or 0), reverse=True)

    # â”€â”€ ä¸è¶³5æ¡æ—¶ï¼Œé™ä½é—¨æ§›ä» all_articles é‡Œè¡¥è¶³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if len(valid) < 5:
        logging.warning(f"âš ï¸ æœ‰æ•ˆæ–‡ç« ä»… {len(valid)} æ¡ï¼Œå°è¯•é™ä½é—¨æ§›è¡¥è¶³è‡³5æ¡")
        used_keys = {a["title"].get("en","").lower()[:60] for a in valid}

        for a in all_articles:
            if len(valid) >= 5:
                break
            if not (a and isinstance(a.get("title"), dict) and a["title"].get("en")):
                continue

            title_en  = a["title"].get("en", "").strip()
            title_key = title_en.lower()[:60]

            # å»é‡
            if title_key in used_keys or title_key in seen_titles:
                continue
            # ä¸­æ–‡ç«™ç‚¹ä»ç„¶ä¸è¦
            if is_chinese_url(a.get("link", "")):
                continue
            # æ ‡é¢˜å¤ªçŸ­ä¸è¦
            if len(title_en) < 10:
                continue
            # å†…å®¹å®Œå…¨ä¸ºç©ºä¸è¦
            content_en = (a.get("content") or {}).get("en", "")
            content_zh = (a.get("content") or {}).get("zh", "")
            if not (content_en or content_zh).strip():
                continue

            used_keys.add(title_key)
            logging.info(f"  â• é™çº§è¡¥å……: {title_en[:60]}")
            valid.append(a)

        valid = sorted(valid, key=lambda x: float(x.get("hot_score", 0) or 0), reverse=True)

    # è¿˜ä¸å¤Ÿ5æ¡å°±è®°å½•è­¦å‘Šï¼ˆä¸å†ç”¨å ä½å¡«å……ï¼‰
    logging.info(f"ğŸ“‹ æœ€ç»ˆæ¨é€ {len(valid)} æ¡èµ„è®¯")
    if len(valid) < 5:
        logging.warning(f"âš ï¸ æœ€ç»ˆåªæœ‰ {len(valid)} æ¡ï¼Œæ¥æºä¸è¶³")

    valid = valid[:5]

    send_to_feishu(valid)

    # æ¨é€æˆåŠŸåä¿å­˜æ ‡é¢˜åˆ°æŒä¹…åŒ–ç¼“å­˜ï¼Œä¾›æ˜å¤©å»é‡
    for a in valid:
        title_en  = (a.get("title") or {}).get("en", "").strip()
        title_key = title_en.lower()[:60]
        if title_key:
            seen_titles.add(title_key)
    save_pushed_titles(seen_titles)

    logging.info("ğŸ ä»»åŠ¡å®Œæˆ")


if __name__ == "__main__":
    main()
