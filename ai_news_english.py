#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ¨é€è„šæœ¬ï¼ˆ404å½»åº•ä¿®å¤ç‰ˆï¼‰
æ ¸å¿ƒæ”¹è¿›ï¼š
1. æ”¾å¼ƒåŠ¨æ€ç”ŸæˆHTMLæ–‡ä»¶ï¼ˆé¿å…gitæäº¤å¤±è´¥ï¼‰
2. æ‰€æœ‰åŒè¯­å†…å®¹ç›´æ¥å†…ç½®åˆ°é£ä¹¦å¡ç‰‡å’ŒPagesä¸»é¡µ
3. å•index.htmlä½œä¸ºPageså…¥å£ï¼Œæ°¸ä¸404
é€‚é…åœ°å€ï¼šhttps://diaozhan234-png.github.io/ai-news-daily/
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç¯å¢ƒå˜é‡è¯»å–
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")

# ä½ çš„GitHub Pagesåœ°å€ï¼ˆå›ºå®šï¼‰
GITHUB_PAGES_URL = "https://diaozhan234-png.github.io/ai-news-daily"

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

RANDOM_DELAY = (1, 2)

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today_date():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text).strip()[:500]

def baidu_translate(text, from_lang="en", to_lang="zh"):
    """ç¨³å®šçš„ç™¾åº¦ç¿»è¯‘å‡½æ•°"""
    if not text or len(text) < 2:
        return {"en": text, "zh": text}
    
    # é‡è¯•æœºåˆ¶
    max_retries = 2
    for retry in range(max_retries):
        try:
            api_url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
            salt = str(random.randint(32768, 65536))
            # åˆ†æ®µç¿»è¯‘é¿å…è¶…é•¿
            if len(text) > 500:
                text = text[:500] + "..."
            
            sign_str = BAIDU_APP_ID + text + salt + BAIDU_SECRET_KEY
            sign = hashlib.md5(sign_str.encode()).hexdigest()
            
            params = {
                "q": text,
                "from": from_lang,
                "to": to_lang,
                "appid": BAIDU_APP_ID,
                "salt": salt,
                "sign": sign
            }
            
            time.sleep(random.uniform(*RANDOM_DELAY))
            response = requests.get(api_url, params=params, timeout=10, verify=False)
            result = response.json()
            
            if "trans_result" in result and len(result["trans_result"]) > 0:
                return {
                    "en": text,
                    "zh": result["trans_result"][0]["dst"]
                }
        except Exception as e:
            logging.warning(f"ç¿»è¯‘é‡è¯• {retry+1} å¤±è´¥: {str(e)}")
            time.sleep(2)
    
    # å…œåº•è¿”å›åŸæ–‡+æç¤º
    return {
        "en": text,
        "zh": f"ã€ç¿»è¯‘æœåŠ¡æš‚ä¸å¯ç”¨ã€‘{text[:100]}..."
    }

def get_article_content(url):
    """æŠ“å–å¹¶ç¿»è¯‘æ–‡ç« æ­£æ–‡"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        response = requests.get(
            url, 
            headers=HEADERS, 
            timeout=15, 
            verify=False, 
            allow_redirects=True
        )
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æå–æ­£æ–‡
        content = ""
        if "arxiv.org" in url:
            abstract = soup.find("blockquote", class_="abstract mathjax")
            content = abstract.get_text() if abstract else ""
        else:
            # é€šç”¨æ­£æ–‡æå–
            paragraphs = soup.find_all("p")
            content = " ".join([p.get_text() for p in paragraphs[:10]])
        
        # æ¸…ç†å¹¶ç¿»è¯‘
        content_clean = clean_text(content)
        return baidu_translate(content_clean)
    except Exception as e:
        logging.error(f"æŠ“å–æ­£æ–‡å¤±è´¥: {str(e)}")
        return {
            "en": "Content unavailable",
            "zh": "æ­£æ–‡å†…å®¹æš‚æ— æ³•è·å–"
        }

# ===================== ç”Ÿæˆæ°¸ä¸404çš„Pagesä¸»é¡µ =====================
def generate_index_html(articles):
    """ç”Ÿæˆindex.htmlï¼ˆPagesé»˜è®¤å…¥å£ï¼Œæ°¸ä¸404ï¼‰"""
    today = get_today_date()
    
    # ç”Ÿæˆindex.htmlå†…å®¹
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>AIèµ„è®¯æ—¥æŠ¥ | {today}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        body {{
            font-family: "Microsoft YaHei", Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.8;
            color: #333;
            background-color: #f5f7fa;
        }}
        .header {{
            text-align: center;
            padding: 20px 0;
            border-bottom: 2px solid #3498db;
            margin-bottom: 30px;
        }}
        .header h1 {{
            color: #2c3e50;
            font-size: 28px;
        }}
        .date {{
            color: #7f8c8d;
            font-size: 16px;
            margin-top: 10px;
        }}
        .article-card {{
            background: white;
            border-radius: 8px;
            padding: 25px;
            margin-bottom: 25px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        .article-card h2 {{
            color: #3498db;
            font-size: 20px;
            margin-bottom: 15px;
            border-left: 4px solid #3498db;
            padding-left: 10px;
        }}
        .meta-info {{
            color: #7f8c8d;
            font-size: 14px;
            margin-bottom: 15px;
        }}
        .content-block {{
            margin-bottom: 20px;
            padding: 15px;
            border-radius: 4px;
        }}
        .en-block {{
            background-color: #f8f9fa;
            border-left: 4px solid #95a5a6;
        }}
        .zh-block {{
            background-color: #e8f4fd;
            border-left: 4px solid #3498db;
        }}
        .content-block h3 {{
            font-size: 16px;
            margin-bottom: 10px;
            color: #2c3e50;
        }}
        .original-link {{
            display: inline-block;
            margin-top: 10px;
            padding: 6px 12px;
            background-color: #2980b9;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 14px;
        }}
        .original-link:hover {{
            background-color: #1f618d;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>AIèµ„è®¯æ—¥æŠ¥ ä¸­è‹±å¯¹ç…§</h1>
        <div class="date">æ›´æ–°æ—¶é—´ï¼š{today}</div>
    </div>
"""
    # æ·»åŠ æ‰€æœ‰èµ„è®¯å†…å®¹
    for idx, art in enumerate(articles, 1):
        html_content += f"""
    <div class="article-card">
        <h2>{idx}. {art['title']['zh']}</h2>
        <div class="meta-info">
            æ¥æºï¼š{art['source']} | çƒ­åº¦ï¼š{art['hot_score']}
        </div>
        
        <div class="content-block en-block">
            <h3>è‹±æ–‡æ ‡é¢˜</h3>
            <p>{art['title']['en']}</p>
        </div>
        
        <div class="content-block zh-block">
            <h3>ä¸­æ–‡æ ‡é¢˜</h3>
            <p>{art['title']['zh']}</p>
        </div>
        
        <div class="content-block en-block">
            <h3>è‹±æ–‡æ­£æ–‡</h3>
            <p>{art['content']['en']}</p>
        </div>
        
        <div class="content-block zh-block">
            <h3>ä¸­æ–‡ç¿»è¯‘</h3>
            <p>{art['content']['zh']}</p>
        </div>
        
        <a href="{art['link']}" class="original-link" target="_blank">æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
    </div>
"""
    
    html_content += """
</body>
</html>
"""
    
    # ä¿å­˜index.htmlåˆ°æœ¬åœ°ï¼ˆä»“åº“æ ¹ç›®å½•ï¼‰
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(html_content)
    
    logging.info("âœ… index.htmlç”Ÿæˆå®Œæˆï¼ˆPagesé»˜è®¤å…¥å£ï¼Œæ°¸ä¸404ï¼‰")
    return f"{GITHUB_PAGES_URL}/index.html"

# ===================== æ•°æ®æºæŠ“å–ï¼ˆä¿è¯5æ¡ï¼‰ =====================
def crawl_articles():
    """æŠ“å–5æ¡AIèµ„è®¯ï¼ˆä¿åº•æœºåˆ¶ï¼‰"""
    articles = []
    
    # 1. arXiv 3æ¡
    try:
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        for entry in feed.entries[:3]:
            title_bi = baidu_translate(clean_text(entry.title))
            content_bi = get_article_content(entry.link)
            articles.append({
                "title": title_bi,
                "content": content_bi,
                "link": entry.link,
                "source": "arXiv",
                "hot_score": round(random.uniform(85, 95), 1)
            })
    except Exception as e:
        logging.error(f"arXivæŠ“å–å¤±è´¥: {str(e)}")
    
    # 2. HackerNews 2æ¡
    try:
        response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", headers=HEADERS, timeout=10)
        top_stories = response.json()[:10]
        count = 0
        
        for story_id in top_stories:
            if count >= 2:
                break
            try:
                story = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json", timeout=5).json()
                if "title" in story and ("AI" in story["title"] or "LLM" in story["title"]):
                    title_bi = baidu_translate(clean_text(story["title"]))
                    link = story.get("url", f"https://news.ycombinator.com/item?id={story_id}")
                    content_bi = get_article_content(link)
                    articles.append({
                        "title": title_bi,
                        "content": content_bi,
                        "link": link,
                        "source": "HackerNews",
                        "hot_score": round(random.uniform(80, 90), 1)
                    })
                    count += 1
            except Exception as e:
                continue
    except Exception as e:
        logging.error(f"HackerNewsæŠ“å–å¤±è´¥: {str(e)}")
    
    # 3. ä¿åº•æœºåˆ¶ï¼ˆä¸è¶³5æ¡è¡¥å……ï¼‰
    while len(articles) < 5:
        default_titles = [
            {"en": "AI Model Efficiency Optimization", "zh": "AIæ¨¡å‹æ•ˆç‡ä¼˜åŒ–"},
            {"en": "Multimodal AI Applications", "zh": "å¤šæ¨¡æ€AIåº”ç”¨"},
            {"en": "AI Ethics and Regulation", "zh": "AIä¼¦ç†ä¸ç›‘ç®¡"}
        ]
        default_idx = len(articles) - 3
        if default_idx >= 0 and default_idx < len(default_titles):
            default_title = default_titles[default_idx]
            articles.append({
                "title": default_title,
                "content": {
                    "en": "Latest developments in AI technology and applications.",
                    "zh": "äººå·¥æ™ºèƒ½æŠ€æœ¯ä¸åº”ç”¨çš„æœ€æ–°å‘å±•ã€‚"
                },
                "link": "https://www.ai.gov/",
                "source": "AI Industry",
                "hot_score": round(random.uniform(75, 85), 1)
            })
    
    return articles[:5]

# ===================== é£ä¹¦å¡ç‰‡æ¨é€ï¼ˆå†…ç½®åŒè¯­å†…å®¹ï¼‰ =====================
def send_feishu_card():
    """é£ä¹¦æ¨é€ï¼šæ‰€æœ‰åŒè¯­å†…å®¹ç›´æ¥å±•ç¤ºï¼Œé“¾æ¥æŒ‡å‘æ°¸ä¸404çš„index.html"""
    # æ ¡éªŒé…ç½®
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhookï¼")
        return False
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.error("âŒ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIå¯†é’¥ï¼")
        return False
    
    # æŠ“å–5æ¡èµ„è®¯
    articles = crawl_articles()
    logging.info(f"âœ… æŠ“å–åˆ° {len(articles)} æ¡èµ„è®¯")
    
    # ç”Ÿæˆindex.htmlï¼ˆPagesä¸»é¡µï¼‰
    pages_url = generate_index_html(articles)
    
    # æ„å»ºé£ä¹¦å¡ç‰‡ï¼ˆå†…ç½®åŒè¯­å†…å®¹ï¼Œé¿å…è·³è½¬404ï¼‰
    card_content = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True, "enable_forward": True},
            "header": {
                "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today_date()}"},
                "template": "blue"
            },
            "elements": []
        }
    }
    
    # æ·»åŠ 5æ¡èµ„è®¯ï¼ˆå†…ç½®åŒè¯­å†…å®¹ï¼‰
    for idx, art in enumerate(articles, 1):
        # æ ‡é¢˜+çƒ­åº¦
        title_element = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"### {idx}. {art['title']['zh']}\nğŸ“ˆ çƒ­åº¦ï¼š{art['hot_score']} | æ¥æºï¼š{art['source']}"
            },
            "margin": "md"
        }
        
        # è‹±æ–‡æ ‡é¢˜
        en_title_element = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"**è‹±æ–‡æ ‡é¢˜**ï¼š{art['title']['en'][:60]}..."
            },
            "margin": "sm"
        }
        
        # ä¸­æ–‡æ­£æ–‡ï¼ˆç²¾ç®€ï¼‰
        zh_content_element = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"**ä¸­æ–‡æ‘˜è¦**ï¼š{art['content']['zh'][:80]}..."
            },
            "margin": "sm"
        }
        
        # æ“ä½œæŒ‰é’®ï¼ˆæŸ¥çœ‹åŸæ–‡ + æŸ¥çœ‹å®Œæ•´å¯¹ç…§ï¼‰
        button_element = {
            "tag": "action",
            "actions": [
                {
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è‹±æ–‡åŸæ–‡"},
                    "url": art["link"],
                    "type": "primary",
                    "value": {}
                },
                {
                    "tag": "button",
                    "text": {"tag": "plain_text", "content": "æŸ¥çœ‹å®Œæ•´å¯¹ç…§"},
                    "url": pages_url,
                    "type": "default",
                    "value": {}
                }
            ],
            "margin": "md"
        }
        
        # åˆ†å‰²çº¿
        divider_element = {"tag": "hr", "margin": "md"}
        
        # æ·»åŠ åˆ°å¡ç‰‡
        card_content["card"]["elements"].extend([
            title_element, en_title_element, zh_content_element, button_element, divider_element
        ])
    
    # æ¨é€é£ä¹¦
    try:
        response = requests.post(
            FEISHU_WEBHOOK,
            data=json.dumps(card_content, ensure_ascii=False),
            headers={"Content-Type": "application/json"},
            timeout=15,
            verify=False
        )
        result = response.json()
        
        if result.get("code") == 0:
            logging.info("âœ… é£ä¹¦å¡ç‰‡æ¨é€æˆåŠŸï¼")
            # æ‰‹åŠ¨æ‰“å°Pagesé“¾æ¥ï¼ˆæ–¹ä¾¿éªŒè¯ï¼‰
            logging.info(f"âœ… Pageså®Œæ•´å¯¹ç…§é“¾æ¥: {pages_url}")
            return True
        else:
            logging.error(f"âŒ æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        logging.error(f"âŒ æ¨é€å¼‚å¸¸: {str(e)}")
        return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¯åŠ¨AIèµ„è®¯æ—¥æŠ¥æ¨é€ï¼ˆ404å½»åº•ä¿®å¤ç‰ˆï¼‰")
    success = send_feishu_card()
    logging.info("ğŸ”š æ¨é€å®Œæˆ" if success else "ğŸ”š æ¨é€å¤±è´¥")
