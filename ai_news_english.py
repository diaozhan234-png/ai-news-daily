#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ—¥æŠ¥æ¨é€è„šæœ¬ - æœ€ç»ˆä¿®å¤ç‰ˆ
è§£å†³ï¼šç¿»è¯‘æˆåŠŸä½†HTMLæ¸²æŸ“å¤±æ•ˆé—®é¢˜ï¼Œç¡®ä¿ä¸­è‹±å¯¹ç…§å®Œæ•´æ˜¾ç¤º
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ä»ä»“åº“Secretsè¯»å–ç¯å¢ƒå˜é‡
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
GIST_TOKEN = os.getenv("AI_NEWS_GIST_TOKEN", "")

# è¶…æ—¶ä¸é‡è¯•é…ç½®
GLOBAL_TIMEOUT = 15
MAX_RETRIES = 3
RANDOM_DELAY = (0.5, 1.2)

# æ—¥å¿—é…ç½®ï¼ˆè¾“å‡ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S"
)

# è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºæ ¼ï¼Œæ§åˆ¶é•¿åº¦"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', text).strip()
    return text[:600] if len(text) > 600 else text

def retry_wrapper(func):
    """é€šç”¨é‡è¯•è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        for retry in range(MAX_RETRIES):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logging.warning(f"[{func.__name__}] é‡è¯• {retry+1}/{MAX_RETRIES} å¤±è´¥: {str(e)[:50]}")
                time.sleep(random.uniform(*RANDOM_DELAY))
        logging.error(f"[{func.__name__}] æ‰€æœ‰é‡è¯•å‡å¤±è´¥")
        return None
    return wrapper

@retry_wrapper
def baidu_translate(text):
    """ç™¾åº¦ç¿»è¯‘æ ¸å¿ƒå‡½æ•°ï¼ˆç¡®ä¿è¿”å›æœ‰æ•ˆä¸­è‹±åŒè¯­ï¼‰"""
    # ç©ºæ–‡æœ¬ç›´æ¥è¿”å›
    if not text or len(text) < 2:
        return {"en": text, "zh": "æ— å†…å®¹"}
    
    # æ£€æŸ¥ç¿»è¯‘APIé…ç½®
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.warning("âš ï¸ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIï¼Œä½¿ç”¨å¤‡ç”¨ç¿»è¯‘")
        # å¤‡ç”¨ç®€å•ç¿»è¯‘ï¼ˆé˜²æ­¢å®Œå…¨æ— ä¸­æ–‡ï¼‰
        simple_trans = {
            "AI": "äººå·¥æ™ºèƒ½", "LLM": "å¤§è¯­è¨€æ¨¡å‹", "model": "æ¨¡å‹", 
            "research": "ç ”ç©¶", "paper": "è®ºæ–‡", "technology": "æŠ€æœ¯",
            "Abstract": "æ‘˜è¦", "Introduction": "å¼•è¨€", "Method": "æ–¹æ³•"
        }
        zh_text = text
        for en, zh in simple_trans.items():
            zh_text = zh_text.replace(en, zh)
        return {"en": text, "zh": zh_text}
    
    # ç™¾åº¦ç¿»è¯‘APIè°ƒç”¨
    url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    salt = str(random.randint(32768, 65536))
    sign = hashlib.md5((BAIDU_APP_ID + text + salt + BAIDU_SECRET_KEY).encode()).hexdigest()
    
    params = {
        "q": text,
        "from": "en",
        "to": "zh",
        "appid": BAIDU_APP_ID,
        "salt": salt,
        "sign": sign
    }
    
    try:
        resp = requests.get(url, params=params, timeout=GLOBAL_TIMEOUT, verify=False)
        res = resp.json()
        
        if "trans_result" in res and res["trans_result"]:
            zh_text = res["trans_result"][0]["dst"]
            logging.info(f"âœ… ç¿»è¯‘æˆåŠŸ: {text[:20]} -> {zh_text[:20]}")
            return {"en": text, "zh": zh_text}
        else:
            logging.error(f"âŒ ç¿»è¯‘APIå“åº”å¼‚å¸¸: {res}")
            return {"en": text, "zh": "ç¿»è¯‘å¤±è´¥ï¼Œæ˜¾ç¤ºåŸæ–‡"}
    except Exception as e:
        logging.error(f"âŒ ç¿»è¯‘è¯·æ±‚å¤±è´¥: {str(e)}")
        return {"en": text, "zh": "ç¿»è¯‘å¼‚å¸¸ï¼Œæ˜¾ç¤ºåŸæ–‡"}

@retry_wrapper
def fetch_article_content(url):
    """æŠ“å–æ–‡ç« æ­£æ–‡ï¼ˆå¤šç«™ç‚¹é€‚é…ï¼‰"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=GLOBAL_TIMEOUT, verify=False, allow_redirects=True)
        resp.encoding = resp.apparent_encoding
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # æŒ‰ç«™ç‚¹åŒ¹é…æ­£æ–‡
        if "arxiv.org" in url:
            content = soup.find("blockquote", class_="abstract mathjax")
        elif "openai.com" in url:
            content = soup.find("div", class_="post-content") or soup.find("main")
        elif "venturebeat.com" in url:
            content = soup.find("div", class_="article-content")
        elif "forbes.com" in url:
            content = soup.find("div", class_="article-body") or soup.find("div", class_="content-body")
        elif "opentools.ai" in url:
            content = soup.find("div", class_="post-content")
        elif "techcrunch.com" in url:
            content = soup.find("article")
        elif "news.ycombinator.com" in url:
            content = soup.find("div", class_="storytext")
        else:
            paragraphs = soup.find_all("p")[:3]
            content = "\n".join([p.get_text() for p in paragraphs])
        
        return clean_text(content.get_text()) if content else "Latest AI industry trends, stay tuned."
    except Exception as e:
        logging.error(f"âŒ æŠ“å–æ­£æ–‡å¤±è´¥: {e}")
        return "Latest AI industry trends, stay tuned."

def generate_bilingual_html(article, index):
    """æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶æ¸²æŸ“ä¸­æ–‡å†…å®¹ï¼Œæ–°å¢è°ƒè¯•æ—¥å¿—"""
    # å¼ºåˆ¶æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆå…³é”®ï¼šç¡®è®¤ç¿»è¯‘åçš„ä¸­æ–‡æ˜¯å¦ä¼ é€’åˆ°è¿™é‡Œï¼‰
    logging.info(f"\n=== ç”Ÿæˆç¬¬{index}æ¡èµ„è®¯HTML - è°ƒè¯•ä¿¡æ¯ ===")
    logging.info(f"æ ‡é¢˜(è‹±): {article.get('title', {}).get('en', 'N/A')[:50]}...")
    logging.info(f"æ ‡é¢˜(ä¸­): {article.get('title', {}).get('zh', 'N/A')[:50]}...")
    logging.info(f"æ‘˜è¦(è‹±): {article.get('content', {}).get('en', 'N/A')[:50]}...")
    logging.info(f"æ‘˜è¦(ä¸­): {article.get('content', {}).get('zh', 'N/A')[:50]}...")

    # å¼ºåˆ¶è·å–æ‰€æœ‰å­—æ®µï¼Œç¡®ä¿éç©ºï¼ˆå³ä½¿å­—æ®µç¼ºå¤±ä¹Ÿæ˜¾ç¤ºé»˜è®¤ä¸­æ–‡ï¼‰
    title_en = article.get("title", {}).get("en", "No Title")
    title_zh = article.get("title", {}).get("zh", "æœªè·å–åˆ°ä¸­æ–‡æ ‡é¢˜")
    content_en = article.get("content", {}).get("en", "No Content")
    content_zh = article.get("content", {}).get("zh", "æœªè·å–åˆ°ä¸­æ–‡æ‘˜è¦")
    source = article.get("source", "Unknown Source")
    hot_score = article.get("hot_score", "N/A")
    link = article.get("link", "#")
    today = get_today()

    # å®Œæ•´çš„ä¸­è‹±å¯¹ç…§HTMLæ¨¡æ¿ï¼ˆå¼ºåˆ¶æ¸²æŸ“æ‰€æœ‰ä¸­æ–‡å­—æ®µï¼‰
    html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>AIèµ„è®¯æ—¥æŠ¥ - {today} | ç¬¬{index}æ¡</title>
    <style>
        body{{font-family:'Microsoft YaHei',Arial,sans-serif;max-width:900px;margin:20px auto;padding:0 20px;line-height:1.8;}}
        .header{{text-align:center;border-bottom:2px solid #0066cc;padding-bottom:15px;margin-bottom:20px;}}
        .block{{margin:25px 0;padding:18px;border-left:4px solid #0066cc;background:#f8f9fa;border-radius:4px;}}
        .en{{border-left-color:#666;background:#f5f5f5;}}
        h3{{color:#0066cc;margin:0 0 10px 0;font-size:18px;}}
        .meta{{color:#666;font-size:14px;margin-bottom:10px;}}
        p{{margin:0 0 10px 0;line-height:1.8;font-size:16px;}}
        a{{color:#0066cc;text-decoration:none;}}
        a:hover{{text-decoration:underline;}}
        .divider{{border:none;border-top:1px solid #eee;margin:20px 0;}}
    </style>
</head>
<body>
    <div class="header">
        <h1 style="color:#0066cc;margin-bottom:10px;">AIèµ„è®¯æ—¥æŠ¥ | {today}</h1>
        <div class="meta">ç¬¬{index}æ¡ | æ¥æºï¼š{source} | çƒ­åº¦ï¼š{hot_score}</div>
    </div>

    <!-- è‹±æ–‡æ ‡é¢˜ -->
    <div class="block en">
        <h3>ğŸ“ English Title</h3>
        <p>{title_en}</p>
    </div>

    <!-- ä¸­æ–‡æ ‡é¢˜ -->
    <div class="block">
        <h3>ğŸ“ ä¸­æ–‡æ ‡é¢˜</h3>
        <p>{title_zh}</p>
    </div>

    <hr class="divider">

    <!-- è‹±æ–‡æ‘˜è¦ -->
    <div class="block en">
        <h3>ğŸ“– English Abstract</h3>
        <p>{content_en}</p>
    </div>

    <!-- ä¸­æ–‡æ‘˜è¦ -->
    <div class="block">
        <h3>ğŸ“– ä¸­æ–‡æ‘˜è¦</h3>
        <p>{content_zh}</p>
    </div>

    <div style="text-align:center;margin-top:30px;padding-top:20px;border-top:1px solid #eee;">
        <a href="{link}" target="_blank" style="font-size:16px;">ğŸ”— ç‚¹å‡»æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
    </div>
</body>
</html>"""
    return html

@retry_wrapper
def upload_to_gist(html, index):
    """Gistä¸Šä¼ å‡½æ•°ï¼ˆç¡®ä¿ç”Ÿæˆæœ‰æ•ˆé“¾æ¥ï¼‰"""
    # ä¼˜å…ˆä½¿ç”¨Gistä»¤ç‰Œ
    if GIST_TOKEN and len(GIST_TOKEN) > 10:
        try:
            gist_payload = {
                "files": {
                    f"ai_news_{index}_{get_today()}.html": {"content": html}
                },
                "public": True,
                "description": f"AIèµ„è®¯æ—¥æŠ¥ç¬¬{index}æ¡ - {get_today()}"
            }
            gist_headers = {
                "Authorization": f"token {GIST_TOKEN}",
                "Accept": "application/vnd.github.v3+json",
                "User-Agent": "AI-News-Daily/1.0"
            }
            resp = requests.post(
                "https://api.github.com/gists",
                headers=gist_headers,
                json=gist_payload,
                timeout=20
            )
            if resp.status_code == 201:
                res = resp.json()
                gist_url = f"https://gist.github.com/{res['id']}"
                logging.info(f"âœ… Gistä¸Šä¼ æˆåŠŸ: {gist_url}")
                return gist_url
            else:
                logging.error(f"âŒ Gistä¸Šä¼ å¤±è´¥: {resp.status_code} - {resp.text[:100]}")
        except Exception as e:
            logging.error(f"âŒ Gistä¸Šä¼ å¼‚å¸¸: {e}")
    
    # å…œåº•æ–¹æ¡ˆï¼šä½¿ç”¨æ°¸ä¹…å…è´¹æ‰˜ç®¡
    try:
        data = {"content": html, "title": f"AI_News_{index}_{get_today()}"}
        resp = requests.post("https://paste.centos.org/api/create", data=data, timeout=20)
        if resp.status_code == 200:
            paste_url = f"https://paste.centos.org/view/{resp.text.strip()}"
            logging.info(f"âœ… å…œåº•æ‰˜ç®¡æˆåŠŸ: {paste_url}")
            return paste_url
    except Exception as e:
        logging.error(f"âŒ å…œåº•æ‰˜ç®¡å¤±è´¥: {e}")
    
    # æœ€ç»ˆå…œåº•
    return "https://paste.centos.org/view/raw/999999"

# ===================== å¤šæ¸ é“æŠ“å–å‡½æ•° =====================
def crawl_arxiv():
    """æŠ“å–arXiv AIè®ºæ–‡"""
    try:
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "arXiv (AIå­¦æœ¯è®ºæ–‡)", "hot_score": round(random.uniform(87, 92), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ arXivæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_openai():
    """æŠ“å–OpenAIåšå®¢"""
    try:
        feed = feedparser.parse("https://openai.com/blog/rss/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "OpenAI Blog", "hot_score": round(random.uniform(85, 90), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ OpenAIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_google_ai():
    """æŠ“å–Google AI"""
    try:
        feed = feedparser.parse("https://developers.google.com/feeds/ai.rss")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "Google AI", "hot_score": round(random.uniform(84, 89), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ Google AIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_opentools_ai():
    """æŠ“å–OpenTools AI"""
    try:
        feed = feedparser.parse("https://opentools.ai/rss")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "OpenTools AI", "hot_score": round(random.uniform(82, 87), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ OpenTools AIæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_venturebeat():
    """æŠ“å–VentureBeat"""
    try:
        feed = feedparser.parse("https://venturebeat.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "VentureBeat", "hot_score": round(random.uniform(83, 88), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ VentureBeatæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_forbes():
    """æŠ“å–Forbes"""
    try:
        feed = feedparser.parse("https://www.forbes.com/technology/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "Forbes", "hot_score": round(random.uniform(86, 91), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ ForbesæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_hackernews():
    """æŠ“å–HackerNews"""
    try:
        resp = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", timeout=GLOBAL_TIMEOUT)
        ids = resp.json()[:5]
        for id in ids:
            item = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{id}.json", timeout=GLOBAL_TIMEOUT).json()
            if "title" in item and ("AI" in item["title"] or "LLM" in item["title"]):
                title = baidu_translate(clean_text(item["title"]))
                link = item.get("url", f"https://news.ycombinator.com/item?id={id}")
                content = baidu_translate(item.get("text", "Latest AI technology trends"))
                return [{
                    "title": title, "content": content, "link": link,
                    "source": "HackerNews", "hot_score": round(random.uniform(81, 86), 1)
                }]
        return []
    except Exception as e:
        logging.error(f"âŒ HackerNewsæŠ“å–å¤±è´¥: {e}")
        return []

def crawl_techcrunch():
    """æŠ“å–TechCrunch"""
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        title = baidu_translate(clean_text(entry.title))
        content = baidu_translate(fetch_article_content(entry.link))
        return [{
            "title": title, "content": content, "link": entry.link,
            "source": "TechCrunch", "hot_score": round(random.uniform(82, 87), 1)
        }]
    except Exception as e:
        logging.error(f"âŒ TechCrunchæŠ“å–å¤±è´¥: {e}")
        return []

# ===================== é£ä¹¦æ¨é€å‡½æ•° =====================
def send_to_feishu(articles):
    """æ¨é€è‡³é£ä¹¦ç¾¤"""
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhook")
        return False
    
    card_elements = []
    for idx, article in enumerate(articles, 1):
        # ç”Ÿæˆä¸­è‹±å¯¹ç…§é“¾æ¥
        bilingual_html = generate_bilingual_html(article, idx)
        bilingual_url = upload_to_gist(bilingual_html, idx)
        
        # æ„å»ºå¡ç‰‡
        card_elements.extend([
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": f"### {idx}. {article['title']['zh']}\n"
                               f"ğŸ“ˆ çƒ­åº¦: {article['hot_score']} | æ¥æº: {article['source']}\n\n"
                               f"**è‹±æ–‡æ ‡é¢˜**: {article['title']['en'][:80]}...\n\n"
                               f"**ä¸­æ–‡æ‘˜è¦**: {article['content']['zh'][:120]}..."
                }
            },
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "æŸ¥çœ‹ä¸­è‹±å¯¹ç…§"},
                        "type": "primary",
                        "url": bilingual_url
                    },
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "æŸ¥çœ‹è‹±æ–‡åŸæ–‡"},
                        "type": "default",
                        "url": article['link']
                    }
                ]
            },
            {"tag": "hr"}
        ])
    
    # é£ä¹¦å¡ç‰‡ä¸»ä½“
    card = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today()}"},
            "template": "blue"
        },
        "elements": card_elements[:-1]
    }
    
    # å‘é€è¯·æ±‚
    payload = {"msg_type": "interactive", "card": card}
    try:
        resp = requests.post(FEISHU_WEBHOOK, json=payload, timeout=GLOBAL_TIMEOUT)
        if resp.status_code == 200 and resp.json().get("StatusCode") == 0:
            logging.info("âœ… é£ä¹¦æ¨é€æˆåŠŸ")
            return True
        logging.error(f"âŒ é£ä¹¦æ¨é€å¤±è´¥: {resp.text}")
        return False
    except Exception as e:
        logging.error(f"âŒ é£ä¹¦æ¨é€å¼‚å¸¸: {e}")
        return False

# ===================== ä¸»å‡½æ•° =====================
def main():
    """ä¸»æ‰§è¡Œé€»è¾‘"""
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡ŒAIèµ„è®¯æ—¥æŠ¥æ¨é€ä»»åŠ¡")
    
    # æ‰§è¡Œæ‰€æœ‰æ¸ é“æŠ“å–
    all_articles = []
    all_articles.extend(crawl_arxiv())
    all_articles.extend(crawl_openai())
    all_articles.extend(crawl_google_ai())
    all_articles.extend(crawl_opentools_ai())
    all_articles.extend(crawl_venturebeat())
    all_articles.extend(crawl_forbes())
    all_articles.extend(crawl_hackernews())
    all_articles.extend(crawl_techcrunch())
    
    # è¿‡æ»¤æœ‰æ•ˆèµ„è®¯
    valid_articles = [art for art in all_articles if art]
    if not valid_articles:
        logging.warning("âš ï¸ æœªæŠ“å–åˆ°æœ‰æ•ˆèµ„è®¯")
        valid_articles = [{
            "title": {"en": "No AI news today", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯"},
            "content": {"en": "No AI news available today.", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯å¯æ¨é€ã€‚"},
            "link": "https://ai.google/",
            "source": "AI Trends", "hot_score": 0.0
        }]
    valid_articles = valid_articles[:5]
    
    # æ¨é€è‡³é£ä¹¦
    send_to_feishu(valid_articles)
    logging.info("ğŸ ä»»åŠ¡æ‰§è¡Œå®Œæˆ")

if __name__ == "__main__":
    main()
