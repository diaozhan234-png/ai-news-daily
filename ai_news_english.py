#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ—¥æŠ¥æ¨é€è„šæœ¬ v4 - å½»åº•ä¿®å¤ç‰ˆ
==============================================================
ä¿®å¤æ¸…å•ï¼š
  Fix-1  baidu_translate è¿”å› None æ—¶é˜²æŠ¤ â†’ safe_translate()
  Fix-2  ç™¾åº¦ç¿»è¯‘ 2000å­—é™åˆ¶ â†’ è¶…é•¿æ–‡æœ¬è‡ªåŠ¨åˆ†æ®µç¿»è¯‘
  Fix-3  fetch_article_content ç²¾å‡†æ®µè½æå–ï¼Œè¿‡æ»¤å¹¿å‘Š/å¯¼èˆªå™ªå£°
  Fix-4  get_rich_content å¢åŠ  HTML æ¸…æ´— + æœ€ç»ˆéç©ºæ ¡éªŒ
  Fix-5  generate_bilingual_html å­—æ®µç©ºå€¼å…¨é¢å…œåº•

æ–°å¢ï¼š
  + å®šæ—¶è¿è¡Œè¯´æ˜ï¼ˆåŒ—äº¬æ—¶é—´ 09:30ï¼ŒGitHub Actions cronï¼‰
  + æ¶ˆæ¯æ¥æºå…¨é¢ä¼˜åŒ–ï¼Œèšç„¦ AI æŠ€æœ¯/åº”ç”¨/æŠ•èèµ„
  + æ–°å¢ The Information AI / MIT Tech Review / AI News
"""

import requests
import os
import datetime
import time
import random
import hashlib
import re
import logging
import urllib3
import feedparser
from bs4 import BeautifulSoup

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

FEISHU_WEBHOOK  = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID    = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")
GIST_TOKEN      = os.getenv("AI_NEWS_GIST_TOKEN", "")

GLOBAL_TIMEOUT  = 20
MAX_RETRIES     = 3
RANDOM_DELAY    = (0.8, 1.5)
TRANSLATE_MAX   = 1800   # ç™¾åº¦ç¿»è¯‘å•æ¬¡æœ€å¤§å­—ç¬¦æ•°ï¼ˆå®˜æ–¹ä¸Šé™2000ï¼Œç•™ä½™é‡ï¼‰
CONTENT_MIN_LEN = 80     # å†…å®¹ä½äºæ­¤é•¿åº¦åˆ™ç»§ç»­å°è¯•ä¸‹ä¸€çº§

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S"
)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9",
    "Cache-Control": "no-cache",
}

# ===================== å·¥å…·å‡½æ•° =====================
def get_today():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€æ§åˆ¶é•¿åº¦"""
    if not text:
        return ""
    text = re.sub(r'\s+', ' ', str(text)).strip()
    return text[:TRANSLATE_MAX] if len(text) > TRANSLATE_MAX else text

def strip_html(raw_html):
    """å°† HTML å­—ç¬¦ä¸²è½¬ä¸ºçº¯æ–‡æœ¬"""
    if not raw_html:
        return ""
    return clean_text(BeautifulSoup(str(raw_html), "html.parser").get_text())

def retry(func):
    """é‡è¯•è£…é¥°å™¨ï¼Œå¤±è´¥è¿”å› None"""
    def wrapper(*args, **kwargs):
        for i in range(MAX_RETRIES):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logging.warning(f"[{func.__name__}] ç¬¬{i+1}æ¬¡å¤±è´¥: {str(e)[:60]}")
                time.sleep(random.uniform(*RANDOM_DELAY))
        logging.error(f"[{func.__name__}] å…¨éƒ¨é‡è¯•å¤±è´¥")
        return None
    return wrapper


# ===================== Fix-1 + Fix-2ï¼šç¿»è¯‘å‡½æ•° =====================
def _call_baidu_api(text):
    """
    å•æ¬¡è°ƒç”¨ç™¾åº¦ç¿»è¯‘ APIï¼Œè¿”å›ä¸­æ–‡å­—ç¬¦ä¸²æˆ– Noneã€‚
    text é•¿åº¦è°ƒç”¨æ–¹ä¿è¯ <= TRANSLATE_MAXã€‚
    """
    url  = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    salt = str(random.randint(32768, 65536))
    sign = hashlib.md5((BAIDU_APP_ID + text + salt + BAIDU_SECRET_KEY).encode()).hexdigest()
    params = {"q": text, "from": "en", "to": "zh",
              "appid": BAIDU_APP_ID, "salt": salt, "sign": sign}
    resp = requests.get(url, params=params, timeout=GLOBAL_TIMEOUT, verify=False)
    res  = resp.json()
    if "trans_result" in res and res["trans_result"]:
        return res["trans_result"][0]["dst"]
    logging.error(f"ç™¾åº¦ç¿»è¯‘å¼‚å¸¸å“åº”: {res}")
    return None


def translate_long_text(text):
    """
    Fix-2ï¼šè¶…é•¿æ–‡æœ¬æŒ‰å¥å­åˆ†æ®µç¿»è¯‘ï¼ˆä¸è¶…è¿‡ TRANSLATE_MAXï¼‰ï¼Œç»“æœæ‹¼æ¥è¿”å›ã€‚
    """
    if not text or not text.strip():
        return ""

    # æŒ‰å¥å·/é—®å·/æ„Ÿå¹å·åˆ†å¥ï¼Œå°½é‡ä¿æŒè¯­ä¹‰å®Œæ•´
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    chunks, cur = [], ""
    for sent in sentences:
        if len(cur) + len(sent) + 1 <= TRANSLATE_MAX:
            cur = (cur + " " + sent).strip()
        else:
            if cur:
                chunks.append(cur)
            # å•å¥è¶…é•¿åˆ™å¼ºåˆ¶æˆªæ–­
            cur = sent[:TRANSLATE_MAX]
    if cur:
        chunks.append(cur)

    zh_parts = []
    for chunk in chunks:
        zh = _call_baidu_api(chunk)
        if zh:
            zh_parts.append(zh)
        else:
            zh_parts.append(chunk)   # ç¿»è¯‘å¤±è´¥ä¿ç•™åŸæ–‡æ®µ
        time.sleep(random.uniform(0.3, 0.6))   # é¿å… API é¢‘ç‡é™åˆ¶

    return "".join(zh_parts)


def safe_translate(text):
    """
    Fix-1 + Fix-2ï¼šå®‰å…¨ç¿»è¯‘å‡½æ•°ï¼Œå§‹ç»ˆè¿”å› {"en": ..., "zh": ...}ï¼Œç»ä¸è¿”å› Noneã€‚
    - æœªé…ç½® API â†’ è¿”å›åŸæ–‡ä½œä¸º zhï¼ˆä¿ç•™è‹±æ–‡å¯è¯»ï¼‰
    - API è°ƒç”¨å¤±è´¥ â†’ è¿”å›åŸæ–‡ä½œä¸º zh
    - è¶…é•¿æ–‡æœ¬ â†’ åˆ†æ®µç¿»è¯‘åæ‹¼æ¥
    """
    en_text = clean_text(text) if text else ""

    if not en_text or len(en_text) < 3:
        return {"en": en_text, "zh": en_text or "æš‚æ— å†…å®¹"}

    # æœªé…ç½®ç¿»è¯‘ API
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.warning("âš ï¸ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIï¼Œä¸­æ–‡æ æ˜¾ç¤ºè‹±æ–‡åŸæ–‡")
        return {"en": en_text, "zh": en_text}

    try:
        zh_text = translate_long_text(en_text)
        if zh_text and zh_text.strip():
            logging.info(f"âœ… ç¿»è¯‘å®Œæˆ: {en_text[:25]}... â†’ {zh_text[:25]}...")
            return {"en": en_text, "zh": zh_text}
        else:
            logging.warning("âš ï¸ ç¿»è¯‘ç»“æœä¸ºç©ºï¼Œä½¿ç”¨åŸæ–‡")
            return {"en": en_text, "zh": en_text}
    except Exception as e:
        logging.error(f"âŒ ç¿»è¯‘å¼‚å¸¸: {e}")
        return {"en": en_text, "zh": en_text}


# ===================== Fix-3ï¼šç²¾å‡†æ­£æ–‡æŠ“å– =====================
@retry
def fetch_article_content(url):
    """
    Fix-3ï¼šæŒ‰ç«™ç‚¹ä½¿ç”¨ç²¾å‡† CSS é€‰æ‹©å™¨ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆä¸è¿”å›å ä½ç¬¦ï¼‰ã€‚
    é€šç”¨å…œåº•ï¼šå–æ­£æ–‡æ®µè½ï¼ˆè¿‡æ»¤ < 40å­—çš„å™ªå£°æ®µï¼‰ã€‚
    """
    try:
        resp = requests.get(
            url, headers=HEADERS, timeout=GLOBAL_TIMEOUT,
            verify=False, allow_redirects=True
        )
        resp.encoding = resp.apparent_encoding
        soup = BeautifulSoup(resp.text, "html.parser")

        # ç§»é™¤å¹²æ‰°å…ƒç´ ï¼ˆå¹¿å‘Šã€å¯¼èˆªã€ä¾§è¾¹æ ã€è„šæ³¨ï¼‰
        for tag in soup.find_all(["script", "style", "nav", "header",
                                   "footer", "aside", "figure", "figcaption",
                                   "noscript", "iframe"]):
            tag.decompose()
        for tag in soup.find_all(class_=re.compile(
            r"(ad|ads|advert|sponsor|promo|related|recommend|sidebar|"
            r"newsletter|subscribe|comment|social|share|cookie|banner)",
            re.I
        )):
            tag.decompose()

        # æŒ‰ç«™ç‚¹ç²¾å‡†é€‰æ‹©æ­£æ–‡å®¹å™¨
        content_el = None
        if "arxiv.org" in url:
            content_el = soup.find("blockquote", class_="abstract mathjax")
        elif "openai.com" in url:
            content_el = (soup.find("div", class_=re.compile(r"post.?content", re.I))
                          or soup.find("main"))
        elif "venturebeat.com" in url:
            content_el = (soup.find("div", class_=re.compile(r"article.?content|entry.?content", re.I))
                          or soup.find("article"))
        elif "forbes.com" in url:
            content_el = (soup.find("div", class_=re.compile(r"article.?body|body.?text", re.I))
                          or soup.find("article"))
        elif "opentools.ai" in url:
            content_el = (soup.find("div", class_=re.compile(r"post.?content|entry.?content", re.I))
                          or soup.find("article"))
        elif "techcrunch.com" in url:
            # TechCrunch: å– <article> å†…çš„ <p> æ®µè½ï¼Œè·³è¿‡å›¾ç‰‡è¯´æ˜ç­‰
            article = soup.find("article")
            if article:
                paras = [p.get_text(" ", strip=True) for p in article.find_all("p")
                         if len(p.get_text(strip=True)) > 40]
                return clean_text(" ".join(paras[:8]))   # å–å‰8æ®µ
        elif "technologyreview.com" in url:
            content_el = (soup.find("div", class_=re.compile(r"article.?body|content.?body", re.I))
                          or soup.find("article"))
        elif "news.ycombinator.com" in url:
            content_el = soup.find("div", class_="storytext")
        elif "reuters.com" in url or "bloomberg.com" in url:
            content_el = soup.find("div", attrs={"data-testid": re.compile(r"body|article", re.I)})

        # æœ‰ç²¾å‡†å®¹å™¨ â†’ å–æ®µè½
        if content_el:
            paras = [p.get_text(" ", strip=True) for p in content_el.find_all("p")
                     if len(p.get_text(strip=True)) > 30]
            text  = " ".join(paras[:8]) if paras else content_el.get_text(" ", strip=True)
            return clean_text(text)

        # é€šç”¨å…œåº•ï¼šå…¨æ–‡æœç´¢ <p>ï¼Œè¿‡æ»¤çŸ­æ®µ
        paras = [p.get_text(" ", strip=True) for p in soup.find_all("p")
                 if len(p.get_text(strip=True)) > 40][:6]
        return clean_text(" ".join(paras))

    except Exception as e:
        logging.error(f"âŒ æŠ“å–æ­£æ–‡å¤±è´¥ [{url[:50]}]: {e}")
        return ""


# ===================== Fix-4ï¼šå¤šçº§å†…å®¹è·å– =====================
def get_rich_content(entry, url):
    """
    Fix-4ï¼šå¤šçº§å…œåº•ï¼Œç¡®ä¿ç¿»è¯‘è¾“å…¥æœ‰å®è´¨å†…å®¹ã€‚
    çº§åˆ«ï¼šRSS full content â†’ RSS summaryï¼ˆHTMLå‰¥ç¦»ï¼‰â†’ æŠ“å–æ­£æ–‡ â†’ æ ‡é¢˜å…œåº•
    """
    # 1ï¸âƒ£ RSS content:encodedï¼ˆéƒ¨åˆ†ç«™ç‚¹æä¾›å…¨æ–‡ï¼‰
    if hasattr(entry, "content") and entry.content:
        raw = entry.content[0].get("value", "")
        text = strip_html(raw)
        if len(text) >= CONTENT_MIN_LEN:
            logging.info(f"  [å†…å®¹] RSS full content ({len(text)}å­—)")
            return text

    # 2ï¸âƒ£ RSS summary / descriptionï¼ˆHTMLå‰¥ç¦»ï¼‰
    raw_summary = getattr(entry, "summary", "") or getattr(entry, "description", "")
    summary = strip_html(raw_summary)
    if len(summary) >= CONTENT_MIN_LEN:
        logging.info(f"  [å†…å®¹] RSS summary ({len(summary)}å­—)")
        return summary

    # 3ï¸âƒ£ æŠ“å–åŸæ–‡æ­£æ–‡
    logging.info(f"  [å†…å®¹] RSSä¸è¶³({len(summary)}å­—)ï¼ŒæŠ“å–åŸæ–‡...")
    fetched = fetch_article_content(url) or ""
    if len(fetched) >= CONTENT_MIN_LEN:
        logging.info(f"  [å†…å®¹] æŠ“å–æ­£æ–‡ ({len(fetched)}å­—)")
        return fetched

    # 4ï¸âƒ£ æ‹¼æ¥å·²æœ‰å†…å®¹
    combined = (summary or fetched).strip()
    if combined:
        logging.warning(f"  [å†…å®¹] æ‹¼æ¥å…œåº• ({len(combined)}å­—)")
        return combined

    # 5ï¸âƒ£ æ ‡é¢˜æ‰©å±•ï¼ˆç»å¯¹å…œåº•ï¼Œä¿è¯ä¸ç¿»è¯‘ç©ºå­—ç¬¦ä¸²ï¼‰
    title = clean_text(getattr(entry, "title", ""))
    fallback = f"{title}. For more details, please visit the original article." if title else "AI industry latest update."
    logging.warning(f"  [å†…å®¹] æ ‡é¢˜å…œåº•")
    return fallback


# ===================== HTML ç”Ÿæˆ =====================
def generate_bilingual_html(article, index):
    """
    Fix-5ï¼šæ‰€æœ‰å­—æ®µå¢åŠ ç©ºå€¼å…œåº•ï¼Œç¡®ä¿ä»»ä½•æƒ…å†µä¸‹é¡µé¢éƒ½èƒ½æ­£å¸¸æ¸²æŸ“ã€‚
    """
    # å®‰å…¨å–å€¼ï¼ˆé˜²æ­¢ content ä¸º None å¯¼è‡´ .get å´©æºƒï¼‰
    def safe_get(obj, *keys, default=""):
        val = obj
        for k in keys:
            if isinstance(val, dict):
                val = val.get(k)
            else:
                return default
            if val is None:
                return default
        return str(val) if val else default

    title_en   = safe_get(article, "title",   "en", default="No Title")
    title_zh   = safe_get(article, "title",   "zh", default=title_en)
    content_en = safe_get(article, "content", "en", default="No content available.")
    content_zh = safe_get(article, "content", "zh", default=content_en)
    source     = article.get("source",    "Unknown")
    hot_score  = article.get("hot_score", "N/A")
    link       = article.get("link",      "#")
    today      = get_today()

    # å¼‚å¸¸å…œåº•ï¼šzh ä»ä¸ºç©ºæ—¶ç”¨ en
    if not content_zh.strip() or content_zh in ("æ— å†…å®¹", "æš‚æ— å†…å®¹", "ç¿»è¯‘å¤±è´¥ï¼Œæ˜¾ç¤ºåŸæ–‡", "ç¿»è¯‘å¼‚å¸¸ï¼Œæ˜¾ç¤ºåŸæ–‡"):
        content_zh = content_en
    if not title_zh.strip():
        title_zh = title_en

    logging.info(f"[HTML] #{index} æ ‡é¢˜EN={title_en[:30]} æ ‡é¢˜ZH={title_zh[:30]}")
    logging.info(f"[HTML] #{index} å†…å®¹EN={len(content_en)}å­— å†…å®¹ZH={len(content_zh)}å­—")

    html = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AIèµ„è®¯æ—¥æŠ¥ {today} Â· ç¬¬{index}æ¡</title>
<style>
*{{margin:0;padding:0;box-sizing:border-box}}
body{{
  font-family:-apple-system,BlinkMacSystemFont,"Segoe UI","PingFang SC","Microsoft YaHei",
              "Helvetica Neue",Arial,sans-serif;
  background:#f0f2f5;color:#1a1a1a;line-height:1.8;min-height:100vh;
  display:flex;flex-direction:column;
}}
.header{{
  background:linear-gradient(135deg,#0052cc 0%,#1a75ff 100%);
  color:#fff;padding:22px 32px;flex-shrink:0;
}}
.header-inner{{max-width:1100px;margin:0 auto;}}
.header h1{{font-size:20px;font-weight:700;margin-bottom:8px;letter-spacing:.02em;}}
.badges{{display:flex;gap:10px;flex-wrap:wrap;margin-top:6px;}}
.badge{{
  background:rgba(255,255,255,.20);border-radius:20px;
  padding:3px 12px;font-size:12px;white-space:nowrap;
}}
.main{{
  flex:1;max-width:1100px;width:100%;
  margin:24px auto;padding:0 16px 16px;
}}
.bilingual-wrapper{{
  display:grid;grid-template-columns:1fr 1fr;
  background:#fff;border-radius:12px;
  box-shadow:0 2px 20px rgba(0,0,0,.10);
  overflow:hidden;min-height:260px;
}}
.col{{padding:28px 28px;}}
.col.en{{background:#f7f9fc;border-right:1px solid #e5eaf0;}}
.col.zh{{background:#fff;}}
.lang-tag{{
  display:inline-flex;align-items:center;gap:5px;
  font-size:11px;font-weight:700;letter-spacing:.14em;text-transform:uppercase;
  color:#0052cc;background:#e6eeff;border-radius:4px;
  padding:3px 10px;margin-bottom:14px;
}}
.col.zh .lang-tag{{color:#c0392b;background:#fdecea;}}
.col-title{{
  font-size:17px;font-weight:700;line-height:1.5;
  color:#111;margin-bottom:14px;
}}
.col-content{{font-size:14px;line-height:1.95;color:#444;}}
.footer{{
  max-width:1100px;width:100%;margin:0 auto;
  padding:14px 16px 32px;
  display:flex;align-items:center;justify-content:space-between;
  flex-wrap:wrap;gap:10px;
}}
.btn{{
  display:inline-block;padding:9px 20px;border-radius:8px;
  font-size:13px;font-weight:600;text-decoration:none;
  cursor:pointer;transition:all .15s ease;border:none;
}}
.btn-primary{{background:#0052cc;color:#fff;}}
.btn-primary:hover{{background:#003d99;}}
.btn-ghost{{
  background:#fff;color:#333;
  border:1px solid #d0d5dd;cursor:pointer;
}}
.btn-ghost:hover{{background:#f4f5f7;}}
.footer-note{{font-size:12px;color:#aaa;}}
@media(max-width:640px){{
  .bilingual-wrapper{{grid-template-columns:1fr;}}
  .col.en{{border-right:none;border-bottom:1px solid #e5eaf0;}}
  .header{{padding:16px;}}
  .col{{padding:18px 16px;}}
  .header h1{{font-size:17px;}}
}}
</style>
</head>
<body>
<div class="header">
  <div class="header-inner">
    <h1>ğŸ¤– AIèµ„è®¯æ—¥æŠ¥ Â· ä¸­è‹±åŒè¯­å¯¹ç…§</h1>
    <div class="badges">
      <span class="badge">ğŸ“… {today}</span>
      <span class="badge">ç¬¬ {index} æ¡</span>
      <span class="badge">ğŸ“¡ {source}</span>
      <span class="badge">ğŸ”¥ çƒ­åº¦ {hot_score}</span>
    </div>
  </div>
</div>

<div class="main">
  <div class="bilingual-wrapper">
    <div class="col en">
      <div class="lang-tag">ğŸ“ English Original</div>
      <div class="col-title">{title_en}</div>
      <div class="col-content">{content_en}</div>
    </div>
    <div class="col zh">
      <div class="lang-tag">ğŸ“ ä¸­æ–‡ç¿»è¯‘</div>
      <div class="col-title">{title_zh}</div>
      <div class="col-content">{content_zh}</div>
    </div>
  </div>
</div>

<div class="footer">
  <div style="display:flex;gap:10px;flex-wrap:wrap;">
    <a class="btn btn-primary" href="{link}" target="_blank">ğŸ”— æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
    <button class="btn btn-ghost"
      onclick="try{{if(window.history.length>1){{window.history.back();}}else{{window.close();}}}}catch(e){{window.close();}}">
      â† å…³é—­
    </button>
  </div>
  <span class="footer-note">æ¥æºï¼š{source} Â· AIèµ„è®¯æ—¥æŠ¥è‡ªåŠ¨æ¨é€</span>
</div>
</body>
</html>"""
    return html


# ===================== Gist ä¸Šä¼  =====================
@retry
def upload_to_gist(html, index):
    """ä¸Šä¼  HTML åˆ° Gist å¹¶è¿”å› htmlpreview æ¸²æŸ“é“¾æ¥"""
    if not (GIST_TOKEN and len(GIST_TOKEN) > 10):
        logging.error("âŒ GIST_TOKEN æœªé…ç½®æˆ–è¿‡çŸ­")
        return "#"

    file_name = f"ai_news_{index}_{get_today()}.html"
    resp = requests.post(
        "https://api.github.com/gists",
        headers={
            "Authorization": f"token {GIST_TOKEN}",
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "AI-News-Daily/4.0"
        },
        json={
            "files": {file_name: {"content": html}},
            "public": True,
            "description": f"AIèµ„è®¯æ—¥æŠ¥ç¬¬{index}æ¡ - {get_today()}"
        },
        timeout=25
    )
    if resp.status_code == 201:
        res      = resp.json()
        gist_id  = res["id"]
        username = res["owner"]["login"]
        raw_url  = f"https://gist.githubusercontent.com/{username}/{gist_id}/raw/{file_name}"
        rendered = f"https://htmlpreview.github.io/?{raw_url}"
        logging.info(f"âœ… Gistä¸Šä¼ æˆåŠŸ: {rendered}")
        return rendered
    logging.error(f"âŒ Gistä¸Šä¼ å¤±è´¥ {resp.status_code}: {resp.text[:120]}")
    return "#"


# ===================== çˆ¬è™«ï¼šèšç„¦ AI æŠ€æœ¯/åº”ç”¨/æŠ•èèµ„ =====================
#
# æ¥æºé€‰æ‹©åŸåˆ™ï¼š
#   â‘  æŠ€æœ¯å‰æ²¿ï¼šarXiv cs.AI / cs.LG / cs.CLï¼ˆæ¨¡å‹ã€ç®—æ³•ï¼‰
#   â‘¡ äº§å“åŠ¨æ€ï¼šOpenAI Blogã€Anthropic Newsã€Google DeepMind Blog
#   â‘¢ è¡Œä¸šèµ„è®¯ï¼šTechCrunch AIã€VentureBeat AIã€MIT Tech Review AI
#   â‘£ æŠ•èèµ„ï¼šThe Information AIï¼ˆéœ€è®¢é˜…å¯æ¢ï¼‰ã€Forbes AI
#   â‘¤ å·¥å…·èšåˆï¼šOpenTools AIã€AI Newsï¼ˆainews.ioï¼‰
#   â‘¥ ç¤¾åŒºçƒ­ç‚¹ï¼šHackerNewsï¼ˆAI/LLMç›¸å…³ï¼‰
#
def _make_article(entry, source, hot_range):
    """é€šç”¨æ–‡ç« æ„å»ºï¼štitleç¿»è¯‘ + æ­£æ–‡è·å–ç¿»è¯‘"""
    title   = safe_translate(clean_text(entry.title))
    raw_content = get_rich_content(entry, entry.link)
    content = safe_translate(raw_content)
    return {
        "title":     title,
        "content":   content,
        "link":      entry.link,
        "source":    source,
        "hot_score": round(random.uniform(*hot_range), 1)
    }


def crawl_arxiv():
    """arXiv AI/ML è®ºæ–‡ â€” æŠ€æœ¯å‰æ²¿"""
    try:
        # cs.AI + cs.LGï¼ˆæœºå™¨å­¦ä¹ ï¼‰+ cs.CLï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰
        for category in ["cs.AI", "cs.LG", "cs.CL"]:
            feed = feedparser.parse(f"http://export.arxiv.org/rss/{category}")
            if feed.entries:
                entry = feed.entries[0]
                logging.info(f"arXiv [{category}]: {entry.title[:50]}")
                return [_make_article(entry, "arXiv å­¦æœ¯è®ºæ–‡", (88, 93))]
        return []
    except Exception as e:
        logging.error(f"âŒ arXiv: {e}")
        return []


def crawl_openai():
    """OpenAI å®˜æ–¹åšå®¢ â€” äº§å“/æ¨¡å‹åŠ¨æ€"""
    try:
        feed = feedparser.parse("https://openai.com/blog/rss/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        logging.info(f"OpenAI: {entry.title[:50]}")
        return [_make_article(entry, "OpenAI å®˜æ–¹åšå®¢", (86, 92))]
    except Exception as e:
        logging.error(f"âŒ OpenAI: {e}")
        return []


def crawl_anthropic():
    """Anthropic å®˜æ–¹æ–°é—» â€” Claude/å®‰å…¨ç ”ç©¶"""
    try:
        # Anthropic æš‚æ— æ ‡å‡† RSSï¼Œä½¿ç”¨å…¶ news é¡µé¢çš„ Atom
        feed = feedparser.parse("https://www.anthropic.com/news/rss")
        if not feed.entries:
            feed = feedparser.parse("https://www.anthropic.com/feed")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        logging.info(f"Anthropic: {entry.title[:50]}")
        return [_make_article(entry, "Anthropic å®˜æ–¹", (85, 91))]
    except Exception as e:
        logging.error(f"âŒ Anthropic: {e}")
        return []


def crawl_google_deepmind():
    """Google DeepMind Blog â€” å‰æ²¿æ¨¡å‹/ç ”ç©¶"""
    try:
        for rss in [
            "https://deepmind.google/blog/rss.xml",
            "https://blog.google/technology/ai/rss/",
            "https://developers.googleblog.com/feeds/posts/default?alt=rss",
        ]:
            feed = feedparser.parse(rss)
            if feed.entries:
                entry = feed.entries[0]
                logging.info(f"Google/DeepMind: {entry.title[:50]}")
                return [_make_article(entry, "Google DeepMind", (84, 90))]
        return []
    except Exception as e:
        logging.error(f"âŒ Google DeepMind: {e}")
        return []


def crawl_mit_tech_review():
    """MIT Technology Review AI â€” æ·±åº¦æŠ€æœ¯åˆ†æ"""
    try:
        feed = feedparser.parse("https://www.technologyreview.com/feed/")
        # è¿‡æ»¤ AI ç›¸å…³æ–‡ç« 
        ai_entries = [e for e in feed.entries
                      if any(kw in (e.title + getattr(e, "summary", "")).lower()
                             for kw in ["ai", "artificial intelligence", "machine learning",
                                        "llm", "model", "neural", "robot", "generative"])]
        if not ai_entries:
            ai_entries = feed.entries[:1]
        if not ai_entries:
            return []
        entry = ai_entries[0]
        logging.info(f"MIT Tech Review: {entry.title[:50]}")
        return [_make_article(entry, "MIT Technology Review", (85, 90))]
    except Exception as e:
        logging.error(f"âŒ MIT Tech Review: {e}")
        return []


def crawl_venturebeat():
    """VentureBeat AI â€” äº§å“å‘å¸ƒ/è¡Œä¸šåŠ¨æ€"""
    try:
        for rss in [
            "https://venturebeat.com/category/ai/feed/",
            "https://venturebeat.com/category/artificial-intelligence/feed/",
        ]:
            feed = feedparser.parse(rss)
            if feed.entries:
                entry = feed.entries[0]
                logging.info(f"VentureBeat: {entry.title[:50]}")
                return [_make_article(entry, "VentureBeat", (83, 89))]
        return []
    except Exception as e:
        logging.error(f"âŒ VentureBeat: {e}")
        return []


def crawl_techcrunch():
    """TechCrunch AI â€” æŠ•èèµ„/åˆ›ä¸š/äº§å“"""
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        if not feed.entries:
            return []
        entry = feed.entries[0]
        logging.info(f"TechCrunch: {entry.title[:50]}")
        return [_make_article(entry, "TechCrunch", (82, 88))]
    except Exception as e:
        logging.error(f"âŒ TechCrunch: {e}")
        return []


def crawl_forbes():
    """Forbes AI â€” æŠ•èèµ„/å•†ä¸šåº”ç”¨"""
    try:
        for rss in [
            "https://www.forbes.com/innovation/artificial-intelligence/feed/",
            "https://www.forbes.com/technology/artificial-intelligence/feed/",
        ]:
            feed = feedparser.parse(rss)
            if feed.entries:
                entry = feed.entries[0]
                logging.info(f"Forbes: {entry.title[:50]}")
                return [_make_article(entry, "Forbes", (83, 89))]
        return []
    except Exception as e:
        logging.error(f"âŒ Forbes: {e}")
        return []


def crawl_opentools_ai():
    """OpenTools AI â€” æ–°å·¥å…·å‘å¸ƒ/åº”ç”¨åŠ¨æ€"""
    try:
        for rss in ["https://opentools.ai/rss", "https://opentools.ai/feed"]:
            feed = feedparser.parse(rss)
            if feed.entries:
                entry = feed.entries[0]
                logging.info(f"OpenTools AI: {entry.title[:50]}")
                return [_make_article(entry, "OpenTools AI", (81, 87))]
        return []
    except Exception as e:
        logging.error(f"âŒ OpenTools AI: {e}")
        return []


def crawl_hackernews():
    """HackerNews â€” ç¤¾åŒºçƒ­ç‚¹ï¼ˆAI/LLM/æ¨¡å‹ç›¸å…³ï¼‰"""
    AI_KEYWORDS = {
        "ai", "llm", "gpt", "claude", "gemini", "mistral", "llama",
        "machine learning", "neural", "transformer", "model", "openai",
        "anthropic", "deepmind", "diffusion", "generative", "rag",
        "inference", "fine.tun", "embedding", "agent", "multimodal"
    }
    try:
        resp = requests.get(
            "https://hacker-news.firebaseio.com/v0/topstories.json",
            timeout=GLOBAL_TIMEOUT
        )
        ids = resp.json()[:20]  # æœç´¢å‰20æ¡ç¡®ä¿å‘½ä¸­
        for story_id in ids:
            item = requests.get(
                f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json",
                timeout=GLOBAL_TIMEOUT
            ).json()
            title = item.get("title", "")
            if any(kw in title.lower() for kw in AI_KEYWORDS):
                link    = item.get("url") or f"https://news.ycombinator.com/item?id={story_id}"
                body    = strip_html(item.get("text", "")) or title
                content = safe_translate(body)
                logging.info(f"HackerNews: {title[:50]}")
                return [{
                    "title":     safe_translate(title),
                    "content":   content,
                    "link":      link,
                    "source":    "HackerNews",
                    "hot_score": round(random.uniform(80, 86), 1)
                }]
        return []
    except Exception as e:
        logging.error(f"âŒ HackerNews: {e}")
        return []


# ===================== é£ä¹¦æ¨é€ =====================
def send_to_feishu(articles):
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½® FEISHU_WEBHOOK")
        return False

    elements = []
    for idx, article in enumerate(articles, 1):
        rendered_url = upload_to_gist(generate_bilingual_html(article, idx), idx)

        # å®‰å…¨å–å€¼
        title_zh   = (article.get("title")   or {}).get("zh") or (article.get("title")   or {}).get("en") or "æ— æ ‡é¢˜"
        title_en   = (article.get("title")   or {}).get("en") or ""
        content_zh = (article.get("content") or {}).get("zh") or (article.get("content") or {}).get("en") or "æš‚æ— æ‘˜è¦"
        source     = article.get("source",    "æœªçŸ¥æ¥æº")
        hot_score  = article.get("hot_score", "N/A")
        orig_link  = article.get("link", "#")

        elements.extend([
            {
                "tag": "div",
                "text": {
                    "tag": "lark_md",
                    "content": (
                        f"### {idx}. {title_zh}\n"
                        f"ğŸ”¥ çƒ­åº¦: {hot_score} | ğŸ“¡ æ¥æº: {source}\n\n"
                        f"**è‹±æ–‡æ ‡é¢˜**ï¼š{title_en[:90]}{'...' if len(title_en) > 90 else ''}\n\n"
                        f"**ä¸­æ–‡æ‘˜è¦**ï¼š{content_zh[:150]}{'...' if len(content_zh) > 150 else ''}"
                    )
                }
            },
            {
                "tag": "action",
                "actions": [
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "ğŸ“„ æŸ¥çœ‹ä¸­è‹±å¯¹ç…§"},
                        "type": "primary",
                        "url": rendered_url
                    },
                    {
                        "tag": "button",
                        "text": {"tag": "plain_text", "content": "ğŸ”— æŸ¥çœ‹è‹±æ–‡åŸæ–‡"},
                        "type": "default",
                        "url": orig_link
                    }
                ]
            },
            {"tag": "hr"}
        ])

    # ç§»é™¤æœ«å°¾å¤šä½™åˆ†å‰²çº¿
    while elements and elements[-1].get("tag") == "hr":
        elements.pop()

    card = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title":    {"tag": "plain_text", "content": f"ğŸ¤– å…¨çƒAIèµ„è®¯æ—¥æŠ¥ | {get_today()}"},
            "template": "blue"
        },
        "elements": elements
    }

    try:
        resp   = requests.post(FEISHU_WEBHOOK, json={"msg_type": "interactive", "card": card},
                               timeout=GLOBAL_TIMEOUT)
        result = resp.json()
        if resp.status_code == 200 and (result.get("StatusCode") == 0 or result.get("code") == 0):
            logging.info("âœ… é£ä¹¦æ¨é€æˆåŠŸ")
            return True
        logging.error(f"âŒ é£ä¹¦æ¨é€å¤±è´¥: {resp.text}")
        return False
    except Exception as e:
        logging.error(f"âŒ é£ä¹¦æ¨é€å¼‚å¸¸: {e}")
        return False


# ===================== ä¸»å‡½æ•° =====================
def main():
    """
    ============================================================
    é—®é¢˜2 è§£ç­”ï¼šåŒ—äº¬æ—¶é—´ 09:30 è‡ªåŠ¨è§¦å‘
    ------------------------------------------------------------
    åœ¨ GitHub Actions çš„ workflow YAML ä¸­è®¾ç½®å¦‚ä¸‹ cronï¼š

      on:
        schedule:
          - cron: '30 1 * * *'   # UTC 01:30 = åŒ—äº¬æ—¶é—´ 09:30

    æ³¨æ„ï¼šGitHub Actions ä½¿ç”¨ UTC æ—¶é—´ï¼ŒåŒ—äº¬æ—¶é—´ï¼ˆCSTï¼‰= UTC + 8
    å› æ­¤ åŒ—äº¬ 09:30 â†’ UTC 01:30
    ============================================================
    """
    logging.info("ğŸš€ AIèµ„è®¯æ—¥æŠ¥ v4 å¯åŠ¨")
    logging.info(f"ğŸ“… ä»Šæ—¥æ—¥æœŸï¼š{get_today()}")

    # çˆ¬è™«åˆ—è¡¨ï¼ˆä¼˜å…ˆçº§é¡ºåºï¼‰
    crawlers = [
        crawl_arxiv,           # å­¦æœ¯å‰æ²¿
        crawl_openai,          # OpenAI åŠ¨æ€
        crawl_anthropic,       # Anthropic / Claude
        crawl_google_deepmind, # Google AI / DeepMind
        crawl_mit_tech_review, # MIT æ·±åº¦åˆ†æ
        crawl_venturebeat,     # è¡Œä¸šèµ„è®¯
        crawl_techcrunch,      # æŠ•èèµ„/äº§å“
        crawl_forbes,          # å•†ä¸š/æŠ•èèµ„
        crawl_opentools_ai,    # å·¥å…·èšåˆ
        crawl_hackernews,      # ç¤¾åŒºçƒ­ç‚¹
    ]

    all_articles = []
    for crawler in crawlers:
        try:
            results = crawler() or []
            if results:
                all_articles.extend(results)
                logging.info(f"âœ… {crawler.__name__} â†’ {len(results)} æ¡")
            else:
                logging.warning(f"âš ï¸ {crawler.__name__} â†’ 0 æ¡")
        except Exception as e:
            logging.error(f"âŒ {crawler.__name__} å´©æºƒ: {e}")

    # è¿‡æ»¤ï¼šå¿…é¡»æœ‰æ ‡é¢˜
    valid = [a for a in all_articles
             if a and isinstance(a.get("title"), dict) and a["title"].get("en")]

    if not valid:
        logging.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆèµ„è®¯ï¼Œä½¿ç”¨å…œåº•å ä½")
        valid = [{
            "title":   {"en": "No AI news today", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯"},
            "content": {"en": "No AI news available today.", "zh": "ä»Šæ—¥æš‚æ— AIèµ„è®¯å¯æ¨é€ï¼Œè¯·æ˜æ—¥å†æŸ¥çœ‹ã€‚"},
            "link":    "https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FtVnVHZ0pWVXlnQVAB",
            "source":  "å ä½",
            "hot_score": 0.0
        }]

    # æŒ‰çƒ­åº¦é™åºï¼Œå–å‰5æ¡
    valid = sorted(valid, key=lambda x: float(x.get("hot_score", 0) or 0), reverse=True)[:5]
    logging.info(f"ğŸ“‹ æœ€ç»ˆæ¨é€ {len(valid)} æ¡èµ„è®¯")

    send_to_feishu(valid)
    logging.info("ğŸ ä»»åŠ¡å®Œæˆ")


if __name__ == "__main__":
    main()
