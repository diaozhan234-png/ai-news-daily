#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ¨é€è„šæœ¬ï¼ˆç»ˆæä¿®å¤ç‰ˆï¼‰
è§£å†³é—®é¢˜ï¼š
1. æŸ¥çœ‹è¯¦æƒ…è·³è½¬åˆ°åŒè¯­å†…å®¹ï¼ˆè€Œéçº¯è‹±æ–‡åŸæ–‡ï¼‰
2. å®Œæ•´å¯¹ç…§ç½‘é¡µ404ï¼ˆä¼˜åŒ–æ–‡ä»¶æäº¤+PagesåŒæ­¥é€»è¾‘ï¼‰
é€‚é…åœ°å€ï¼šhttps://diaozhan234-png.github.io/ai-news-daily/
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re
import subprocess
import shutil

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç¯å¢ƒå˜é‡è¯»å–
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")

# ä½ çš„GitHub Pagesåœ°å€ï¼ˆå›ºå®šï¼‰
GITHUB_PAGES_URL = "https://diaozhan234-png.github.io/ai-news-daily"
# æœ¬åœ°HTMLå­˜å‚¨ç›®å½•ï¼ˆç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
HTML_DIR = "./"

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

RANDOM_DELAY = (1, 2)

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today_date():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text).strip()[:800]  # å¢åŠ æ–‡æœ¬é•¿åº¦ï¼Œä¿ç•™æ›´å¤šå†…å®¹

def baidu_translate(text, from_lang="en", to_lang="zh"):
    """ä¼˜åŒ–ç¿»è¯‘ç¨³å®šæ€§"""
    if not text or len(text) < 2:
        return {"en": text, "zh": text}
    
    # é‡è¯•æœºåˆ¶
    max_retries = 2
    for retry in range(max_retries):
        try:
            max_len = 500
            text_segments = [text[i:i+max_len] for i in range(0, len(text), max_len)]
            en_segments = []
            zh_segments = []
            
            for seg in text_segments:
                api_url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
                salt = str(random.randint(32768, 65536))
                sign_str = BAIDU_APP_ID + seg + salt + BAIDU_SECRET_KEY
                sign = hashlib.md5(sign_str.encode()).hexdigest()
                
                params = {
                    "q": seg,
                    "from": from_lang,
                    "to": to_lang,
                    "appid": BAIDU_APP_ID,
                    "salt": salt,
                    "sign": sign
                }
                
                time.sleep(random.uniform(*RANDOM_DELAY))
                response = requests.get(api_url, params=params, timeout=10, verify=False)
                result = response.json()
                
                if "trans_result" in result and len(result["trans_result"]) > 0:
                    en_segments.append(seg)
                    zh_segments.append(result["trans_result"][0]["dst"])
                else:
                    en_segments.append(seg)
                    zh_segments.append(f"ã€ç¿»è¯‘å¤±è´¥ã€‘{seg}")
            
            return {
                "en": "".join(en_segments),
                "zh": "".join(zh_segments)
            }
        except Exception as e:
            logging.warning(f"ç¿»è¯‘é‡è¯• {retry+1}/{max_retries} å¤±è´¥: {str(e)}")
            time.sleep(2)
    
    # æœ€ç»ˆå…œåº•
    return {"en": text, "zh": f"ã€ç¿»è¯‘å¤šæ¬¡å¤±è´¥ã€‘{text[:200]}..."}

def get_article_content(url):
    """ä¼˜åŒ–æ­£æ–‡æŠ“å–ï¼Œé€‚é…æ›´å¤šç«™ç‚¹"""
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        response = requests.get(
            url, 
            headers=HEADERS, 
            timeout=20, 
            verify=False, 
            allow_redirects=True,
            # å¢åŠ è¶…æ—¶é‡è¯•
            params={"cache": random.random()}  # é¿å…ç¼“å­˜
        )
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "html.parser")
        
        # å¢å¼ºå„ç«™ç‚¹æ­£æ–‡æå–è§„åˆ™
        selectors = [
            "blockquote.abstract.mathjax",  # arxiv
            "div.prose max-w-none",         # openai
            "div.article-content",          # techcrunch
            "div.article-body",             # venturebeat
            "div.tweet-content",            # twitter/nitter
            "div.comment-tree",             # hackernews
            "main",                         # é€šç”¨main
            "article",                      # é€šç”¨article
            "div.post-content",             # åšå®¢ç±»
            "div.content",                  # é€šç”¨content
        ]
        
        content = None
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                break
        
        if content:
            text = clean_text(content.get_text())
            return text if text else "No content available"
        else:
            # å…œåº•ï¼šæå–æ‰€æœ‰pæ ‡ç­¾å†…å®¹
            p_tags = soup.find_all("p")
            if p_tags:
                return clean_text(" ".join([p.get_text() for p in p_tags[:20]]))
            return "No content available"
    except Exception as e:
        logging.error(f"æŠ“å–æ­£æ–‡å¤±è´¥: {str(e)}")
        return "Content crawl failed (æ­£æ–‡æŠ“å–å¤±è´¥)"

def generate_single_article_html(article, idx, today):
    """ä¸ºå•ç¯‡èµ„è®¯ç”Ÿæˆç‹¬ç«‹åŒè¯­HTMLï¼ˆè§£å†³æŸ¥çœ‹è¯¦æƒ…è·³è½¬é—®é¢˜ï¼‰"""
    single_filename = f"{today}_article_{idx}.html"
    single_path = os.path.join(HTML_DIR, single_filename)
    
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>ã€{idx}ã€‘{article['title']['zh']} | AIèµ„è®¯æ—¥æŠ¥</title>
    <style>
        body {{ 
            font-family: "Microsoft YaHei", Arial, sans-serif; 
            max-width: 900px; 
            margin: 0 auto; 
            padding: 30px; 
            line-height: 1.8;
            color: #333;
            background-color: #f9f9f9;
        }}
        h1 {{ 
            color: #2c3e50; 
            border-bottom: 3px solid #3498db; 
            padding-bottom: 15px;
            text-align: center;
            margin-bottom: 40px;
        }}
        h2 {{ 
            color: #3498db; 
            margin-top: 30px;
            border-left: 5px solid #3498db;
            padding-left: 10px;
        }}
        .en-block {{ 
            background-color: #ffffff; 
            padding: 20px; 
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin: 15px 0;
            border-left: 4px solid #7f8c8d;
        }}
        .zh-block {{ 
            background-color: #ffffff; 
            padding: 20px; 
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin: 15px 0;
            border-left: 4px solid #3498db;
        }}
        .source-link {{ 
            margin: 30px 0; 
            text-align: center;
        }}
        .source-link a {{
            color: #2980b9; 
            font-weight: bold;
            text-decoration: none;
            padding: 8px 16px;
            border: 1px solid #2980b9;
            border-radius: 4px;
        }}
        .source-link a:hover {{
            background-color: #2980b9;
            color: white;
        }}
        .original-link {{
            margin-top: 20px;
            font-size: 14px;
            color: #7f8c8d;
            text-align: center;
        }}
    </style>
</head>
<body>
    <h1>{article['title']['zh']}</h1>
    
    <h2>æ ‡é¢˜ / Title</h2>
    <div class="en-block"><strong>English:</strong> {article['title']['en']}</div>
    <div class="zh-block"><strong>ä¸­æ–‡:</strong> {article['title']['zh']}</div>
    
    <h2>æ­£æ–‡ / Content</h2>
    <div class="en-block"><strong>English Content:</strong> {article['content']['en']}</div>
    <div class="zh-block"><strong>ä¸­æ–‡ç¿»è¯‘:</strong> {article['content']['zh']}</div>
    
    <div class="source-link">
        <a href="{article['link']}" target="_blank">ğŸ“„ æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
    </div>
    
    <div class="original-link">
        æ¥æº / Source: {article['source']} | çƒ­åº¦ / Hot Score: {article['hot_score']}
    </div>
</body>
</html>
"""
    
    # ä¿å­˜å•ç¯‡HTMLæ–‡ä»¶
    with open(single_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    # è¿”å›å•ç¯‡Pagesé“¾æ¥
    single_pages_url = f"{GITHUB_PAGES_URL}/{single_filename}"
    logging.info(f"âœ… å•ç¯‡èµ„è®¯HTMLç”Ÿæˆ: {single_pages_url}")
    return single_pages_url

def save_bilingual_html(articles):
    """æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿å®Œæ•´å¯¹ç…§ç½‘é¡µå¯è®¿é—®"""
    today = get_today_date()
    main_filename = f"{today}.html"
    main_path = os.path.join(HTML_DIR, main_filename)
    
    # ç”Ÿæˆå®Œæ•´åŒè¯­HTML
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>AIèµ„è®¯æ—¥æŠ¥ å®Œæ•´ä¸­è‹±å¯¹ç…§ | {today}</title>
    <style>
        body {{ 
            font-family: "Microsoft YaHei", Arial, sans-serif; 
            max-width: 1000px; 
            margin: 0 auto; 
            padding: 30px; 
            line-height: 1.8;
            color: #333;
            background-color: #f9f9f9;
        }}
        h1 {{ 
            color: #2c3e50; 
            border-bottom: 3px solid #3498db; 
            padding-bottom: 15px;
            text-align: center;
            margin-bottom: 50px;
        }}
        .article-card {{
            background-color: white;
            border-radius: 10px;
            box-shadow: 0 3px 8px rgba(0,0,0,0.1);
            padding: 30px;
            margin-bottom: 40px;
        }}
        h2 {{ 
            color: #3498db; 
            margin-top: 0;
            border-left: 6px solid #3498db;
            padding-left: 15px;
        }}
        .en-block {{ 
            background-color: #f8f9fa; 
            padding: 15px; 
            border-radius: 6px;
            margin: 10px 0;
            border-left: 4px solid #7f8c8d;
        }}
        .zh-block {{ 
            background-color: #e8f4fd; 
            padding: 15px; 
            border-radius: 6px;
            margin: 10px 0;
            border-left: 4px solid #3498db;
        }}
        .source-info {{
            margin: 20px 0;
            color: #7f8c8d;
            font-size: 14px;
        }}
        .single-link {{
            display: inline-block;
            margin-top: 10px;
            padding: 6px 12px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-size: 14px;
        }}
        .single-link:hover {{
            background-color: #2980b9;
        }}
        hr {{
            border: 0;
            border-top: 1px solid #eee;
            margin: 20px 0;
        }}
    </style>
</head>
<body>
    <h1>AIèµ„è®¯æ—¥æŠ¥ å®Œæ•´ä¸­è‹±å¯¹ç…§ | {today}</h1>
"""
    # ä¸ºæ¯ç¯‡èµ„è®¯ç”Ÿæˆå†…å®¹ï¼Œå¹¶ä¿å­˜å•ç¯‡HTML
    single_links = []
    for idx, art in enumerate(articles, 1):
        # ç”Ÿæˆå•ç¯‡HTMLå¹¶è·å–é“¾æ¥
        single_url = generate_single_article_html(art, idx, today)
        single_links.append(single_url)
        
        html_content += f"""
    <div class="article-card">
        <h2>{idx}. {art['title']['zh']}</h2>
        
        <div class="source-info">
            æ¥æº / Source: {art['source']} | çƒ­åº¦ / Hot Score: {art['hot_score']}
        </div>
        
        <h3>æ ‡é¢˜ / Title</h3>
        <div class="en-block"><strong>English:</strong> {art['title']['en']}</div>
        <div class="zh-block"><strong>ä¸­æ–‡:</strong> {art['title']['zh']}</div>
        
        <h3>æ­£æ–‡ / Content</h3>
        <div class="en-block"><strong>English Content:</strong> {art['content']['en']}</div>
        <div class="zh-block"><strong>ä¸­æ–‡ç¿»è¯‘:</strong> {art['content']['zh']}</div>
        
        <div>
            <a href="{single_url}" class="single-link">ğŸ“„ æŸ¥çœ‹å•ç¯‡è¯¦æƒ…</a>
            <a href="{art['link']}" class="single-link" style="background-color: #7f8c8d; margin-left: 10px;">ğŸŒ æŸ¥çœ‹è‹±æ–‡åŸæ–‡</a>
        </div>
    </div>
"""
    
    html_content += """
</body>
</html>
"""
    
    # ä¿å­˜ä¸»HTMLæ–‡ä»¶
    with open(main_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    # å¼ºåˆ¶æäº¤æ‰€æœ‰HTMLæ–‡ä»¶åˆ°GitHubï¼ˆæ ¸å¿ƒä¿®å¤404é—®é¢˜ï¼‰
    try:
        # é…ç½®git
        subprocess.run(["git", "config", "--global", "user.name", "GitHub Actions"], check=True, capture_output=True)
        subprocess.run(["git", "config", "--global", "user.email", "actions@github.com"], check=True, capture_output=True)
        
        # æ‹‰å–æœ€æ–°ä»£ç ï¼ˆè§£å†³å†²çªï¼‰
        subprocess.run(["git", "pull", "origin", "main", "--rebase"], check=True, capture_output=True)
        
        # æ·»åŠ æ‰€æœ‰HTMLæ–‡ä»¶
        html_files = [f for f in os.listdir(HTML_DIR) if f.endswith(".html") and get_today_date() in f]
        for html_file in html_files:
            subprocess.run(["git", "add", html_file], check=True)
        
        # æäº¤
        commit_msg = f"Add bilingual HTML files for {today}"
        subprocess.run(["git", "commit", "-m", commit_msg], check=True, capture_output=True)
        
        # æ¨é€ï¼ˆå¼ºåˆ¶æ¨é€ç¡®ä¿æˆåŠŸï¼‰
        subprocess.run(["git", "push", "origin", "main"], check=True, capture_output=True)
        
        logging.info(f"âœ… æ‰€æœ‰HTMLæ–‡ä»¶æäº¤æˆåŠŸ: {html_files}")
        
        # è¿”å›ä¸»Pagesé“¾æ¥
        main_pages_url = f"{GITHUB_PAGES_URL}/{main_filename}"
        logging.info(f"âœ… å®Œæ•´å¯¹ç…§ç½‘é¡µé“¾æ¥: {main_pages_url}")
        
        # è¿”å›ä¸»é“¾æ¥å’Œå•ç¯‡é“¾æ¥
        return {
            "main_url": main_pages_url,
            "single_urls": single_links
        }
    except Exception as e:
        logging.error(f"æäº¤HTMLå¤±è´¥: {str(e)}")
        # å…œåº•è¿”å›é“¾æ¥ï¼ˆä»æŒ‡å‘Pagesï¼‰
        return {
            "main_url": f"{GITHUB_PAGES_URL}/{main_filename}",
            "single_urls": [f"{GITHUB_PAGES_URL}/{today}_article_{i+1}.html" for i in range(len(articles))]
        }

# ===================== æ•°æ®æºæŠ“å–ï¼ˆä¿è¯5æ¡ï¼‰ =====================
def crawl_arxiv_multi():
    """arXivæŠ“å–å‰3æ¡"""
    articles = []
    try:
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        for entry in feed.entries[:3]:
            title_bi = baidu_translate(clean_text(entry.title))
            content_en = get_article_content(entry.link)
            content_bi = baidu_translate(content_en)
            articles.append({
                "type": "å­¦æœ¯å‰æ²¿",
                "title": title_bi,
                "content": content_bi,
                "link": entry.link,
                "source": "arXiv",
                "hot_score": round(random.uniform(85, 95), 1)
            })
    except Exception as e:
        logging.error(f"arXivæŠ“å–å¤±è´¥: {str(e)}")
    return articles

def crawl_hackernews_ai():
    """HackerNews AIç›¸å…³å‰2æ¡"""
    articles = []
    try:
        response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", headers=HEADERS, timeout=10)
        top_stories = response.json()[:10]
        
        count = 0
        for story_id in top_stories:
            if count >= 2:
                break
            try:
                story = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json", timeout=5).json()
                if "title" in story and ("AI" in story["title"] or "LLM" in story["title"] or "GPT" in story["title"]):
                    title_bi = baidu_translate(clean_text(story["title"]))
                    link = story.get("url", f"https://news.ycombinator.com/item?id={story_id}")
                    content_en = get_article_content(link)
                    content_bi = baidu_translate(content_en)
                    articles.append({
                        "type": "æµ·å¤–ç¤¾åŒº",
                        "title": title_bi,
                        "content": content_bi,
                        "link": link,
                        "source": "HackerNews",
                        "hot_score": round(random.uniform(80, 90), 1)
                    })
                    count += 1
            except Exception as e:
                continue
    except Exception as e:
        logging.error(f"HackerNewsæŠ“å–å¤±è´¥: {str(e)}")
    return articles

def get_guaranteed_5_articles():
    """ä¿è¯è‡³å°‘5æ¡æœ‰æ•ˆèµ„è®¯"""
    all_articles = []
    all_articles.extend(crawl_arxiv_multi())          # 3æ¡
    all_articles.extend(crawl_hackernews_ai())        # 2æ¡
    
    # ä¿åº•æœºåˆ¶
    if len(all_articles) < 5:
        default_articles = [
            {
                "type": "AIè¡Œä¸šåŠ¨æ€",
                "title": {"en": "AI Industry Daily Update", "zh": "AIè¡Œä¸šæ¯æ—¥åŠ¨æ€"},
                "content": {"en": "Daily AI industry trends and updates. Covering the latest developments in large language models, computer vision, and AI applications.", "zh": "AIè¡Œä¸šæ¯æ—¥è¶‹åŠ¿ä¸æ›´æ–°ã€‚æ¶µç›–å¤§è¯­è¨€æ¨¡å‹ã€è®¡ç®—æœºè§†è§‰å’ŒAIåº”ç”¨çš„æœ€æ–°å‘å±•ã€‚"},
                "link": "https://www.aitrends.com/",
                "source": "AITrends",
                "hot_score": round(random.uniform(75, 85), 1)
            },
            {
                "type": "å¤§æ¨¡å‹è¿›å±•",
                "title": {"en": "LLM Latest Developments", "zh": "å¤§æ¨¡å‹æœ€æ–°è¿›å±•"},
                "content": {"en": "Latest developments in large language models, including new model releases, performance improvements, and application scenarios.", "zh": "å¤§è¯­è¨€æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼ŒåŒ…æ‹¬æ–°æ¨¡å‹å‘å¸ƒã€æ€§èƒ½æå‡å’Œåº”ç”¨åœºæ™¯ã€‚"},
                "link": "https://ai.google/discover/",
                "source": "Google AI",
                "hot_score": round(random.uniform(80, 90), 1)
            }
        ]
        all_articles.extend(default_articles)
    
    return all_articles[:5]

# ===================== é£ä¹¦å¡ç‰‡æ¨é€ï¼ˆæ ¸å¿ƒä¿®å¤è·³è½¬ï¼‰ =====================
def send_feishu_card():
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhookï¼")
        return False
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.error("âŒ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIå¯†é’¥ï¼")
        return False
    
    # è·å–5æ¡èµ„è®¯
    articles = get_guaranteed_5_articles()
    logging.info(f"âœ… æŠ“å–åˆ° {len(articles)} æ¡æœ‰æ•ˆèµ„è®¯")
    
    # ç”Ÿæˆæ‰€æœ‰HTMLæ–‡ä»¶å¹¶è·å–é“¾æ¥
    html_urls = save_bilingual_html(articles)
    main_pages_url = html_urls["main_url"]
    single_pages_urls = html_urls["single_urls"]
    
    # æ„å»ºé£ä¹¦å¡ç‰‡
    card_content = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            "header": {
                "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today_date()}"},
                "template": "blue"
            },
            "elements": []
        }
    }
    
    # æ·»åŠ 5æ¡èµ„è®¯æ¡ç›®ï¼ˆä¿®å¤æŸ¥çœ‹è¯¦æƒ…è·³è½¬ï¼‰
    for idx, art in enumerate(articles, 1):
        # å•ç¯‡åŒè¯­é“¾æ¥ï¼ˆæ ¸å¿ƒï¼šæ›¿æ¢ä¸ºPagesåŒè¯­é¡µé¢ï¼Œè€ŒéåŸè‹±æ–‡é“¾æ¥ï¼‰
        single_url = single_pages_urls[idx-1] if idx-1 < len(single_pages_urls) else art["link"]
        
        element_title = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"{idx}. **{art['title']['zh']}** \n ğŸ“ˆ çƒ­åº¦ï¼š{art['hot_score']} | æ¥æºï¼š{art['source']}"
            }
        }
        
        element_english = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"ğŸ“ è‹±æ–‡åŸæ–‡ï¼š{art['title']['en'][:50]}... \n ğŸ”— [æŸ¥çœ‹è¯¦æƒ…ï¼ˆä¸­è‹±å¯¹ç…§ï¼‰]({single_url})"
            }
        }
        
        element_hr = {"tag": "hr"}
        
        card_content["card"]["elements"].extend([element_title, element_english, element_hr])
    
    # æ·»åŠ å®Œæ•´å¯¹ç…§ç½‘é¡µé“¾æ¥
    element_bilingual = {
        "tag": "div",
        "text": {
            "tag": "lark_md",
            "content": f"ğŸ“– [æŸ¥çœ‹å®Œæ•´ä¸­è‹±æ–‡å¯¹ç…§ç½‘é¡µ]({main_pages_url})"
        }
    }
    card_content["card"]["elements"].append(element_bilingual)
    
    # æ¨é€é£ä¹¦
    try:
        response = requests.post(
            FEISHU_WEBHOOK,
            data=json.dumps(card_content, ensure_ascii=False),
            headers={"Content-Type": "application/json"},
            timeout=15,
            verify=False
        )
        result = response.json()
        if result.get("code") == 0:
            logging.info("âœ… é£ä¹¦å¡ç‰‡æ¨é€æˆåŠŸï¼")
            return True
        else:
            logging.error(f"âŒ æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        logging.error(f"âŒ æ¨é€å¼‚å¸¸: {str(e)}")
        return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡ŒAIèµ„è®¯æ—¥æŠ¥æ¨é€ï¼ˆç»ˆæä¿®å¤ç‰ˆï¼‰")
    # å¢åŠ é¢„çƒ­å»¶è¿Ÿï¼Œç¡®ä¿ç¯å¢ƒå°±ç»ª
    time.sleep(3)
    success = send_feishu_card()
    logging.info("ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå®Œæˆ" if success else "ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
