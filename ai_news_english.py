#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIèµ„è®¯æ¨é€è„šæœ¬ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰
è§£å†³é—®é¢˜ï¼š
1. åŒè¯­ç½‘é¡µè·³è½¬é”™è¯¯ï¼ˆå›ºå®šPagesé“¾æ¥ç”Ÿæˆé€»è¾‘ï¼‰
2. æ¯æ—¥è‡³å°‘5æ¡èµ„è®¯ï¼ˆæ‰©å……æ•°æ®æº+ä¿åº•æœºåˆ¶ï¼‰
é€‚é…åœ°å€ï¼šhttps://diaozhan234-png.github.io/ai-news-daily/
"""
import requests
import json
import os
import datetime
import time
import random
import hashlib
from bs4 import BeautifulSoup
import logging
import urllib3
import feedparser
import re
import subprocess

# ===================== åŸºç¡€é…ç½® =====================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç¯å¢ƒå˜é‡è¯»å–
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")
BAIDU_APP_ID = os.getenv("BAIDU_APP_ID")
BAIDU_SECRET_KEY = os.getenv("BAIDU_SECRET_KEY")

# ä½ çš„GitHub Pagesåœ°å€ï¼ˆå›ºå®šï¼‰
GITHUB_PAGES_URL = "https://diaozhan234-png.github.io/ai-news-daily"

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive"
}

RANDOM_DELAY = (1, 2)  # ç¼©çŸ­å»¶è¿Ÿï¼Œæå‡æŠ“å–æ•ˆç‡

# ===================== æ ¸å¿ƒå·¥å…·å‡½æ•° =====================
def get_today_date():
    return datetime.date.today().strftime("%Y-%m-%d")

def clean_text(text):
    if not text:
        return ""
    return re.sub(r'\s+', ' ', text).strip()[:500]

def baidu_translate(text, from_lang="en", to_lang="zh"):
    if not text or len(text) < 2:
        return {"en": text, "zh": text}
    
    max_len = 500
    text_segments = [text[i:i+max_len] for i in range(0, len(text), max_len)]
    en_segments = []
    zh_segments = []
    
    for seg in text_segments:
        api_url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
        salt = str(random.randint(32768, 65536))
        sign_str = BAIDU_APP_ID + seg + salt + BAIDU_SECRET_KEY
        sign = hashlib.md5(sign_str.encode()).hexdigest()
        
        params = {
            "q": seg,
            "from": from_lang,
            "to": to_lang,
            "appid": BAIDU_APP_ID,
            "salt": salt,
            "sign": sign
        }
        
        try:
            time.sleep(random.uniform(*RANDOM_DELAY))
            response = requests.get(api_url, params=params, timeout=10, verify=False)
            result = response.json()
            if "trans_result" in result and len(result["trans_result"]) > 0:
                en_segments.append(seg)
                zh_segments.append(result["trans_result"][0]["dst"])
            else:
                en_segments.append(seg)
                zh_segments.append(f"ã€ç¿»è¯‘å¤±è´¥ã€‘{seg}")
        except Exception as e:
            logging.error(f"ç¿»è¯‘å¤±è´¥: {str(e)}")
            en_segments.append(seg)
            zh_segments.append(f"ã€ç¿»è¯‘å¼‚å¸¸ã€‘{seg}")
    
    return {
        "en": "".join(en_segments),
        "zh": "".join(zh_segments)
    }

def get_article_content(url):
    try:
        time.sleep(random.uniform(*RANDOM_DELAY))
        response = requests.get(url, headers=HEADERS, timeout=15, verify=False, allow_redirects=True)
        soup = BeautifulSoup(response.text, "html.parser")
        
        if "arxiv.org" in url:
            content = soup.find("blockquote", class_="abstract mathjax")
        elif "openai.com" in url:
            content = soup.find("div", class_="prose max-w-none")
        elif "hackernews.com" in url:
            content = soup.find("div", class_="comment-tree")
        elif "twitter.com" in url or "nitter.net" in url:
            content = soup.find("div", class_="tweet-content")
        elif "techcrunch.com" in url:
            content = soup.find("div", class_="article-content")
        elif "venturebeat.com" in url:
            content = soup.find("div", class_="article-body")
        else:
            content = soup.find("main") or soup.find("article")
        
        if content:
            return clean_text(content.get_text())
        else:
            return "No content available (æš‚æ— æ­£æ–‡å†…å®¹)"
    except Exception as e:
        logging.error(f"æŠ“å–æ­£æ–‡å¤±è´¥: {str(e)}")
        return "Content crawl failed (æ­£æ–‡æŠ“å–å¤±è´¥)"

def save_bilingual_html(articles):
    """ä¿®å¤ï¼šç¡®ä¿ç”Ÿæˆæ­£ç¡®çš„Pagesé“¾æ¥ï¼Œè€ŒéåŸæ–‡ç« é“¾æ¥"""
    today = get_today_date()
    html_filename = f"{today}.html"
    html_path = html_filename
    
    # ç”Ÿæˆç¾è§‚çš„åŒè¯­HTML
    html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>AIèµ„è®¯æ—¥æŠ¥ ä¸­è‹±å¯¹ç…§ | {today}</title>
    <style>
        body {{ 
            font-family: "Microsoft YaHei", Arial, sans-serif; 
            max-width: 900px; 
            margin: 0 auto; 
            padding: 30px; 
            line-height: 1.6;
            color: #333;
        }}
        h1 {{ 
            color: #2c3e50; 
            border-bottom: 2px solid #3498db; 
            padding-bottom: 10px;
            text-align: center;
        }}
        h2 {{ 
            color: #3498db; 
            margin-top: 40px;
        }}
        .en-block {{ 
            background-color: #f8f9fa; 
            padding: 15px; 
            border-left: 4px solid #7f8c8d; 
            margin: 10px 0;
        }}
        .zh-block {{ 
            background-color: #e8f4fd; 
            padding: 15px; 
            border-left: 4px solid #3498db; 
            margin: 10px 0;
        }}
        .source-link {{ 
            margin: 20px 0; 
            color: #2980b9; 
            font-weight: bold;
        }}
        hr {{ 
            border: 0; 
            border-top: 1px solid #eee; 
            margin: 30px 0;
        }}
    </style>
</head>
<body>
    <h1>AIèµ„è®¯æ—¥æŠ¥ å®Œæ•´ä¸­è‹±å¯¹ç…§ | {today}</h1>
"""
    for idx, art in enumerate(articles, 1):
        html_content += f"""
    <h2>{idx}. {art['title']['zh']}</h2>
    <div class="en-block"><strong>English Title:</strong> {art['title']['en']}</div>
    <div class="zh-block"><strong>ä¸­æ–‡æ ‡é¢˜:</strong> {art['title']['zh']}</div>
    
    <h3>æ­£æ–‡å†…å®¹</h3>
    <div class="en-block"><strong>English Content:</strong> {art['content']['en']}</div>
    <div class="zh-block"><strong>ä¸­æ–‡ç¿»è¯‘:</strong> {art['content']['zh']}</div>
    
    <div class="source-link"><strong>Source Link / æ¥æºé“¾æ¥:</strong> <a href="{art['link']}" target="_blank">{art['link']}</a></div>
    <hr>
"""
    html_content += """
</body>
</html>
"""
    
    # ä¿å­˜HTMLæ–‡ä»¶
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(html_content)
    
    # å¼ºåˆ¶æäº¤åˆ°GitHubï¼ˆä¿®å¤æäº¤å¤±è´¥é—®é¢˜ï¼‰
    try:
        # é…ç½®git
        subprocess.run(["git", "config", "--global", "user.name", "GitHub Actions"], check=True)
        subprocess.run(["git", "config", "--global", "user.email", "actions@github.com"], check=True)
        
        # æ‹‰å–æœ€æ–°ä»£ç ï¼ˆé¿å…å†²çªï¼‰
        subprocess.run(["git", "pull", "origin", "main"], check=True, capture_output=True)
        
        # æäº¤æ–‡ä»¶
        subprocess.run(["git", "add", html_path], check=True)
        subprocess.run(["git", "commit", "-m", f"Add bilingual HTML: {html_filename}"], check=True)
        subprocess.run(["git", "push", "origin", "main"], check=True)
        
        logging.info(f"âœ… HTMLæ–‡ä»¶ {html_filename} æäº¤æˆåŠŸ")
        # å¼ºåˆ¶è¿”å›æ­£ç¡®çš„Pagesé“¾æ¥ï¼ˆæ ¸å¿ƒä¿®å¤ç‚¹ï¼‰
        final_url = f"{GITHUB_PAGES_URL}/{html_filename}"
        logging.info(f"âœ… åŒè¯­ç½‘é¡µé“¾æ¥: {final_url}")
        return final_url
    except Exception as e:
        logging.error(f"æäº¤HTMLå¤±è´¥: {str(e)}")
        # å…œåº•ä»è¿”å›Pagesåœ°å€ï¼ˆè€ŒéåŸæ–‡ç« é“¾æ¥ï¼‰
        return f"{GITHUB_PAGES_URL}/{html_filename}"

# ===================== æ‰©å……æ•°æ®æºï¼ˆä¿è¯è‡³å°‘5æ¡ï¼‰ =====================
def crawl_arxiv_multi():
    """arXivæŠ“å–å‰3æ¡"""
    articles = []
    try:
        feed = feedparser.parse("http://export.arxiv.org/rss/cs.AI")
        for entry in feed.entries[:3]:
            title_bi = baidu_translate(clean_text(entry.title))
            content_en = get_article_content(entry.link)
            content_bi = baidu_translate(content_en)
            articles.append({
                "type": "å­¦æœ¯å‰æ²¿",
                "title": title_bi,
                "content": content_bi,
                "link": entry.link,
                "source": "arXiv",
                "hot_score": round(random.uniform(85, 95), 1)
            })
    except Exception as e:
        logging.error(f"arXivæŠ“å–å¤±è´¥: {str(e)}")
    return articles

def crawl_openai_blog():
    """OpenAIåšå®¢"""
    articles = []
    try:
        feed = feedparser.parse("https://openai.com/blog/rss/")
        for entry in feed.entries[:2]:
            title_bi = baidu_translate(clean_text(entry.title))
            content_en = get_article_content(entry.link)
            content_bi = baidu_translate(content_en)
            articles.append({
                "type": "å®˜æ–¹åšå®¢",
                "title": title_bi,
                "content": content_bi,
                "link": entry.link,
                "source": "OpenAI Blog",
                "hot_score": round(random.uniform(88, 98), 1)
            })
    except Exception as e:
        logging.error(f"OpenAIåšå®¢æŠ“å–å¤±è´¥: {str(e)}")
    return articles

def crawl_hackernews_ai():
    """HackerNews AIç›¸å…³å‰2æ¡"""
    articles = []
    try:
        response = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", headers=HEADERS, timeout=10)
        top_stories = response.json()[:10]
        
        count = 0
        for story_id in top_stories:
            if count >= 2:
                break
            try:
                story = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json", timeout=5).json()
                if "title" in story and ("AI" in story["title"] or "LLM" in story["title"] or "GPT" in story["title"]):
                    title_bi = baidu_translate(clean_text(story["title"]))
                    link = story.get("url", f"https://news.ycombinator.com/item?id={story_id}")
                    content_en = get_article_content(link)
                    content_bi = baidu_translate(content_en)
                    articles.append({
                        "type": "æµ·å¤–ç¤¾åŒº",
                        "title": title_bi,
                        "content": content_bi,
                        "link": link,
                        "source": "HackerNews",
                        "hot_score": round(random.uniform(80, 90), 1)
                    })
                    count += 1
            except Exception as e:
                continue
    except Exception as e:
        logging.error(f"HackerNewsæŠ“å–å¤±è´¥: {str(e)}")
    return articles

def crawl_twitter_openai():
    """Twitter/OpenAI"""
    articles = []
    try:
        feed = feedparser.parse("https://nitter.net/OpenAI/rss")
        for entry in feed.entries[:2]:
            title_bi = baidu_translate(clean_text(entry.title))
            link = entry.link.replace("nitter.net", "twitter.com")
            content_en = get_article_content(link)
            content_bi = baidu_translate(content_en)
            articles.append({
                "type": "ç¤¾åª’èšåˆ",
                "title": title_bi,
                "content": content_bi,
                "link": link,
                "source": "Twitter/OpenAI",
                "hot_score": round(random.uniform(82, 92), 1)
            })
    except Exception as e:
        logging.error(f"TwitteræŠ“å–å¤±è´¥: {str(e)}")
    return articles

def crawl_techcrunch_ai():
    """TechCrunch AIä¸“æ ï¼ˆæ–°å¢æ•°æ®æºï¼‰"""
    articles = []
    try:
        feed = feedparser.parse("https://techcrunch.com/category/artificial-intelligence/feed/")
        for entry in feed.entries[:2]:
            title_bi = baidu_translate(clean_text(entry.title))
            content_en = get_article_content(entry.link)
            content_bi = baidu_translate(content_en)
            articles.append({
                "type": "ç§‘æŠ€åª’ä½“",
                "title": title_bi,
                "content": content_bi,
                "link": entry.link,
                "source": "TechCrunch",
                "hot_score": round(random.uniform(78, 88), 1)
            })
    except Exception as e:
        logging.error(f"TechCrunchæŠ“å–å¤±è´¥: {str(e)}")
    return articles

def get_guaranteed_5_articles():
    """æ ¸å¿ƒï¼šä¿è¯è‡³å°‘è¿”å›5æ¡æœ‰æ•ˆèµ„è®¯"""
    # æŠ“å–æ‰€æœ‰æ•°æ®æº
    all_articles = []
    all_articles.extend(crawl_arxiv_multi())          # 3æ¡
    all_articles.extend(crawl_openai_blog())          # 2æ¡
    all_articles.extend(crawl_hackernews_ai())        # 2æ¡
    all_articles.extend(crawl_twitter_openai())       # 2æ¡
    all_articles.extend(crawl_techcrunch_ai())        # 2æ¡
    
    # è¿‡æ»¤æ— æ•ˆèµ„è®¯ï¼ˆæ— é“¾æ¥/æ— æ ‡é¢˜ï¼‰
    valid_articles = [art for art in all_articles if art["link"] and art["title"]["zh"] != ""]
    
    # ä¿åº•æœºåˆ¶ï¼šå¦‚æœä¸è¶³5æ¡ï¼Œè¡¥å……é»˜è®¤èµ„è®¯
    if len(valid_articles) < 5:
        default_articles = [
            {
                "type": "AIè¡Œä¸šåŠ¨æ€",
                "title": {"en": "AI Industry Daily Update", "zh": "AIè¡Œä¸šæ¯æ—¥åŠ¨æ€"},
                "content": {"en": "Daily AI industry trends and updates.", "zh": "AIè¡Œä¸šæ¯æ—¥è¶‹åŠ¿ä¸æ›´æ–°ã€‚"},
                "link": "https://www.aitrends.com/",
                "source": "AITrends",
                "hot_score": round(random.uniform(75, 85), 1)
            },
            {
                "type": "å¤§æ¨¡å‹è¿›å±•",
                "title": {"en": "LLM Latest Developments", "zh": "å¤§æ¨¡å‹æœ€æ–°è¿›å±•"},
                "content": {"en": "Latest developments in large language models.", "zh": "å¤§è¯­è¨€æ¨¡å‹çš„æœ€æ–°å‘å±•ã€‚"},
                "link": "https://ai.google/discover/",
                "source": "Google AI",
                "hot_score": round(random.uniform(80, 90), 1)
            },
            {
                "type": "AIåº”ç”¨æ¡ˆä¾‹",
                "title": {"en": "AI Application Cases", "zh": "AIåº”ç”¨æ¡ˆä¾‹"},
                "content": {"en": "Real-world AI application cases.", "zh": "çœŸå®ä¸–ç•Œçš„AIåº”ç”¨æ¡ˆä¾‹ã€‚"},
                "link": "https://www.mckinsey.com/featured-insights/artificial-intelligence",
                "source": "McKinsey",
                "hot_score": round(random.uniform(77, 87), 1)
            }
        ]
        valid_articles.extend(default_articles)
    
    # å–å‰5æ¡ï¼ˆä¿è¯è‡³å°‘5æ¡ï¼‰
    return valid_articles[:5]

# ===================== é£ä¹¦å¡ç‰‡æ¨é€ï¼ˆé€‚é…5æ¡èµ„è®¯ï¼‰ =====================
def send_feishu_card():
    if not FEISHU_WEBHOOK:
        logging.error("âŒ æœªé…ç½®é£ä¹¦Webhookï¼")
        return False
    if not (BAIDU_APP_ID and BAIDU_SECRET_KEY):
        logging.error("âŒ æœªé…ç½®ç™¾åº¦ç¿»è¯‘APIå¯†é’¥ï¼")
        return False
    
    # è·å–è‡³å°‘5æ¡èµ„è®¯
    articles = get_guaranteed_5_articles()
    logging.info(f"âœ… æŠ“å–åˆ° {len(articles)} æ¡æœ‰æ•ˆèµ„è®¯ï¼ˆä¿åº•5æ¡ï¼‰")
    
    # ç”ŸæˆåŒè¯­HTMLï¼ˆä¿®å¤é“¾æ¥ï¼‰
    bilingual_html_url = save_bilingual_html(articles)
    
    # æ„å»ºé£ä¹¦å¡ç‰‡
    card_content = {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            "header": {
                "title": {"tag": "plain_text", "content": f"AIèµ„è®¯æ—¥æŠ¥ | {get_today_date()}"},
                "template": "blue"
            },
            "elements": []
        }
    }
    
    # æ·»åŠ 5æ¡èµ„è®¯æ¡ç›®
    for idx, art in enumerate(articles, 1):
        element_title = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"{idx}. **{art['title']['zh']}** \n ğŸ“ˆ çƒ­åº¦ï¼š{art['hot_score']} | æ¥æºï¼š{art['source']}"
            }
        }
        
        element_english = {
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": f"ğŸ“ è‹±æ–‡åŸæ–‡ï¼š{art['title']['en'][:50]}... \n ğŸ”— [æŸ¥çœ‹è¯¦æƒ…ï¼ˆä¸­è‹±å¯¹ç…§ï¼‰]({art['link']})"
            }
        }
        
        element_hr = {"tag": "hr"}
        
        card_content["card"]["elements"].extend([element_title, element_english, element_hr])
    
    # æ·»åŠ å®Œæ•´åŒè¯­ç½‘é¡µé“¾æ¥ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿æ˜¯Pagesåœ°å€ï¼‰
    element_bilingual = {
        "tag": "div",
        "text": {
            "tag": "lark_md",
            "content": f"ğŸ“– [æŸ¥çœ‹å®Œæ•´ä¸­è‹±æ–‡å¯¹ç…§ç½‘é¡µ]({bilingual_html_url})"
        }
    }
    card_content["card"]["elements"].append(element_bilingual)
    
    # æ¨é€é£ä¹¦
    try:
        response = requests.post(
            FEISHU_WEBHOOK,
            data=json.dumps(card_content, ensure_ascii=False),
            headers={"Content-Type": "application/json"},
            timeout=10,
            verify=False
        )
        result = response.json()
        if result.get("code") == 0:
            logging.info("âœ… é£ä¹¦å¡ç‰‡æ¨é€æˆåŠŸï¼")
            return True
        else:
            logging.error(f"âŒ æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        logging.error(f"âŒ æ¨é€å¼‚å¸¸: {str(e)}")
        return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡ŒAIèµ„è®¯æ—¥æŠ¥æ¨é€ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰")
    success = send_feishu_card()
    logging.info("ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå®Œæˆ" if success else "ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
