#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥AIç²¾é€‰èµ„è®¯æ¨é€è„šæœ¬ï¼ˆGitHub Actionsé€‚é…ç‰ˆï¼‰
æ ¸å¿ƒï¼šæ— æˆæœ¬äº‘ç«¯éƒ¨ç½²ï¼Œæ¯æ—¥9:30ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰è‡ªåŠ¨æ¨é€åˆ°é£ä¹¦
"""
import requests
import json
import os
import datetime
import time
from bs4 import BeautifulSoup
import logging
import urllib3

# ===================== åŸºç¡€é…ç½® =====================
# å±è”½InsecureRequestWarningè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ä»ç¯å¢ƒå˜é‡è¯»å–æ•æ„Ÿä¿¡æ¯ï¼ˆGitHub Secretsé…ç½®ï¼‰
FEISHU_WEBHOOK = os.getenv("FEISHU_WEBHOOK")  # é£ä¹¦Webhook
BAIDU_TRANS_APPID = os.getenv("BAIDU_TRANS_APPID")  # ç™¾åº¦ç¿»è¯‘APPIDï¼ˆå¯é€‰ï¼‰
BAIDU_TRANS_KEY = os.getenv("BAIDU_TRANS_KEY")  # ç™¾åº¦ç¿»è¯‘å¯†é’¥ï¼ˆå¯é€‰ï¼‰

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# è¯·æ±‚å¤´ï¼ˆé˜²åçˆ¬ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# ===================== å·¥å…·å‡½æ•° =====================
def get_today_date():
    """è·å–ä»Šæ—¥æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰"""
    return datetime.date.today().strftime("%Y-%m-%d")

def baidu_translate(text, from_lang="en", to_lang="zh"):
    """ç™¾åº¦ç¿»è¯‘APIï¼ˆå¯é€‰ï¼Œæ— é…ç½®åˆ™è¿”å›åŸæ–‡ï¼‰"""
    if not BAIDU_TRANS_APPID or not BAIDU_TRANS_KEY:
        return text
    
    try:
        import hashlib
        salt = str(int(time.time()))
        sign_str = BAIDU_TRANS_APPID + text + salt + BAIDU_TRANS_KEY
        sign = hashlib.md5(sign_str.encode()).hexdigest()
        
        url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
        params = {
            "q": text,
            "from": from_lang,
            "to": to_lang,
            "appid": BAIDU_TRANS_APPID,
            "salt": salt,
            "sign": sign
        }
        response = requests.get(url, params=params, timeout=10, verify=False)
        result = response.json()
        if "trans_result" in result:
            return result["trans_result"][0]["dst"]
        return text
    except Exception as e:
        logging.error(f"ç¿»è¯‘å¤±è´¥: {e}")
        return text

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼ˆå»ç©ºæ ¼ã€æ¢è¡Œï¼‰"""
    return text.replace("\n", "").replace("\r", "").strip()

# ===================== æ ¸å¿ƒæŠ“å–å‡½æ•°ï¼ˆå¤šæºå¤´+å•æ¡è¾“å‡ºï¼‰ =====================
def crawl_basic_llm():
    """ğŸ¤– åŸºç¡€å¤§æ¨¡å‹ / å¤šæ¨¡æ€ï¼ˆä¸»ï¼šæ–°æ™ºå…ƒï¼Œå¤‡ï¼šæœºå™¨ä¹‹å¿ƒï¼‰"""
    # ä¸»æºï¼šæ–°æ™ºå…ƒ
    try:
        url = "https://www.xinzhiyuan.com/"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # é€‚é…æ–°æ™ºå…ƒé¡µé¢ç»“æ„
        articles = soup.find_all("div", class_="article-item", limit=1)
        if not articles:
            articles = soup.find_all("a", class_="title", limit=1)
        if articles:
            if hasattr(articles[0], "text"):
                title = clean_text(articles[0].text)
                link = articles[0]["href"] if "href" in articles[0].attrs else url
                if not link.startswith("http"):
                    link = "https://www.xinzhiyuan.com" + link
                return {
                    "type": "ğŸ¤– åŸºç¡€å¤§æ¨¡å‹ / å¤šæ¨¡æ€",
                    "title_zh": title,
                    "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                    "link": link,
                    "time": get_today_date()
                }
    except Exception as e:
        logging.error(f"æŠ“å–æ–°æ™ºå…ƒå¤±è´¥: {e}")
    
    # å¤‡ç”¨æºï¼šæœºå™¨ä¹‹å¿ƒ
    try:
        url = "https://www.jiqizhixin.com/"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = soup.find_all("div", class_="article-item", limit=1)
        if not articles:
            articles = soup.find_all("a", class_="article-title", limit=1)
        if articles:
            title = clean_text(articles[0].text)
            link = articles[0]["href"] if "href" in articles[0].attrs else url
            if not link.startswith("http"):
                link = "https://www.jiqizhixin.com" + link
            return {
                "type": "ğŸ¤– åŸºç¡€å¤§æ¨¡å‹ / å¤šæ¨¡æ€",
                "title_zh": title,
                "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                "link": link,
                "time": get_today_date()
            }
    except Exception as e:
        logging.error(f"æŠ“å–æœºå™¨ä¹‹å¿ƒå¤±è´¥: {e}")
    
    # æ— å†…å®¹æç¤ºï¼ˆä»…æ–‡å­—ï¼‰
    return {
        "type": "ğŸ¤– åŸºç¡€å¤§æ¨¡å‹ / å¤šæ¨¡æ€",
        "title_zh": "ä»Šæ—¥æš‚æ— ã€åŸºç¡€å¤§æ¨¡å‹/å¤šæ¨¡æ€ã€‘ç›¸å…³ä¿¡æ¯",
        "summary_zh": "",
        "link": "",
        "time": ""
    }

def crawl_industry_dynamic():
    """ğŸ¢ AI è¡Œä¸šåŠ¨æ€ / åº”ç”¨åˆ›æ–°ï¼ˆä¸»ï¼šæ™šç‚¹ï¼Œå¤‡ï¼šæ–°æ™ºå…ƒï¼‰"""
    # ä¸»æºï¼šæ™šç‚¹LatePost
    try:
        url = "https://www.latepost.com/"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = soup.find_all("article", class_="post-item", limit=1)
        if not articles:
            articles = soup.find_all("a", class_="post-title", limit=1)
        if articles:
            title = clean_text(articles[0].text)
            link = articles[0]["href"] if "href" in articles[0].attrs else url
            if not link.startswith("http"):
                link = "https://www.latepost.com" + link
            return {
                "type": "ğŸ¢ AI è¡Œä¸šåŠ¨æ€ / åº”ç”¨åˆ›æ–°",
                "title_zh": title,
                "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                "link": link,
                "time": get_today_date()
            }
    except Exception as e:
        logging.error(f"æŠ“å–æ™šç‚¹å¤±è´¥: {e}")
    
    # å¤‡ç”¨æºï¼šæ–°æ™ºå…ƒ
    try:
        url = "https://www.xinzhiyuan.com/"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = soup.find_all("div", class_="article-item", limit=1)
        if articles:
            title = clean_text(articles[0].text)
            link = articles[0]["href"] if "href" in articles[0].attrs else url
            if not link.startswith("http"):
                link = "https://www.xinzhiyuan.com" + link
            return {
                "type": "ğŸ¢ AI è¡Œä¸šåŠ¨æ€ / åº”ç”¨åˆ›æ–°",
                "title_zh": title,
                "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                "link": link,
                "time": get_today_date()
            }
    except Exception as e:
        logging.error(f"æŠ“å–æ–°æ™ºå…ƒï¼ˆå¤‡ç”¨ï¼‰å¤±è´¥: {e}")
    
    # æ— å†…å®¹æç¤º
    return {
        "type": "ğŸ¢ AI è¡Œä¸šåŠ¨æ€ / åº”ç”¨åˆ›æ–°",
        "title_zh": "ä»Šæ—¥æš‚æ— ã€AIè¡Œä¸šåŠ¨æ€/åº”ç”¨åˆ›æ–°ã€‘ç›¸å…³ä¿¡æ¯",
        "summary_zh": "",
        "link": "",
        "time": ""
    }

def crawl_ai_tech():
    """ğŸ”§ AI æŠ€æœ¯ / Agentï¼ˆä¸»ï¼šInfoQï¼Œå¤‡ï¼šæœºå™¨ä¹‹å¿ƒï¼‰"""
    # ä¸»æºï¼šInfoQ
    try:
        url = "https://www.infoq.cn/topic/ai"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = soup.find_all("div", class_="article-item", limit=1)
        if not articles:
            articles = soup.find_all("a", class_="article-title", limit=1)
        if articles:
            title = clean_text(articles[0].text)
            link = articles[0]["href"] if "href" in articles[0].attrs else url
            if not link.startswith("http"):
                link = "https://www.infoq.cn" + link
            return {
                "type": "ğŸ”§ AI æŠ€æœ¯ / Agent",
                "title_zh": title,
                "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                "link": link,
                "time": get_today_date()
            }
    except Exception as e:
        logging.error(f"æŠ“å–InfoQå¤±è´¥: {e}")
    
    # å¤‡ç”¨æºï¼šæœºå™¨ä¹‹å¿ƒ
    try:
        url = "https://www.jiqizhixin.com/"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = soup.find_all("a", class_="article-title", limit=1)
        if articles:
            title = clean_text(articles[0].text)
            link = articles[0]["href"] if "href" in articles[0].attrs else url
            if not link.startswith("http"):
                link = "https://www.jiqizhixin.com" + link
            return {
                "type": "ğŸ”§ AI æŠ€æœ¯ / Agent",
                "title_zh": title,
                "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                "link": link,
                "time": get_today_date()
            }
    except Exception as e:
        logging.error(f"æŠ“å–æœºå™¨ä¹‹å¿ƒï¼ˆå¤‡ç”¨ï¼‰å¤±è´¥: {e}")
    
    # æ— å†…å®¹æç¤º
    return {
        "type": "ğŸ”§ AI æŠ€æœ¯ / Agent",
        "title_zh": "ä»Šæ—¥æš‚æ— ã€AIæŠ€æœ¯/Agentã€‘ç›¸å…³ä¿¡æ¯",
        "summary_zh": "",
        "link": "",
        "time": ""
    }

def crawl_llm_ranking():
    """ğŸ“Š å¤§æ¨¡å‹æ’è¡Œæ¦œ / æŠ€æœ¯å‰æ²¿ï¼ˆä¸»ï¼šæœºå™¨ä¹‹å¿ƒï¼Œå¤‡ï¼šInfoQï¼‰"""
    # ä¸»æºï¼šæœºå™¨ä¹‹å¿ƒ
    try:
        url = "https://www.jiqizhixin.com/"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = soup.find_all("div", class_="article-item", limit=1)
        if not articles:
            articles = soup.find_all("a", class_="article-title", limit=1)
        if articles:
            title = clean_text(articles[0].text)
            link = articles[0]["href"] if "href" in articles[0].attrs else url
            if not link.startswith("http"):
                link = "https://www.jiqizhixin.com" + link
            return {
                "type": "ğŸ“Š å¤§æ¨¡å‹æ’è¡Œæ¦œ / æŠ€æœ¯å‰æ²¿",
                "title_zh": title,
                "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                "link": link,
                "time": get_today_date()
            }
    except Exception as e:
        logging.error(f"æŠ“å–æœºå™¨ä¹‹å¿ƒå¤±è´¥: {e}")
    
    # å¤‡ç”¨æºï¼šInfoQ
    try:
        url = "https://www.infoq.cn/topic/ai"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = soup.find_all("a", class_="article-title", limit=1)
        if articles:
            title = clean_text(articles[0].text)
            link = articles[0]["href"] if "href" in articles[0].attrs else url
            if not link.startswith("http"):
                link = "https://www.infoq.cn" + link
            return {
                "type": "ğŸ“Š å¤§æ¨¡å‹æ’è¡Œæ¦œ / æŠ€æœ¯å‰æ²¿",
                "title_zh": title,
                "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                "link": link,
                "time": get_today_date()
            }
    except Exception as e:
        logging.error(f"æŠ“å–InfoQï¼ˆå¤‡ç”¨ï¼‰å¤±è´¥: {e}")
    
    # æ— å†…å®¹æç¤º
    return {
        "type": "ğŸ“Š å¤§æ¨¡å‹æ’è¡Œæ¦œ / æŠ€æœ¯å‰æ²¿",
        "title_zh": "ä»Šæ—¥æš‚æ— ã€å¤§æ¨¡å‹æ’è¡Œæ¦œ/æŠ€æœ¯å‰æ²¿ã€‘ç›¸å…³ä¿¡æ¯",
        "summary_zh": "",
        "link": "",
        "time": ""
    }

def crawl_ai_innovation():
    """ğŸš€ AI åº”ç”¨åˆ›æ–° / è¡Œä¸šè¶‹åŠ¿ï¼ˆä¸»ï¼šçŸ¥æ½œï¼Œå¤‡ï¼šæ™šç‚¹ï¼‰"""
    # ä¸»æºï¼šçŸ¥æ½œKnowFuture
    try:
        url = "https://www.knowfuture.cn/"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = soup.find_all("div", class_="article-item", limit=1)
        if not articles:
            articles = soup.find_all("a", class_="title", limit=1)
        if articles:
            title = clean_text(articles[0].text)
            link = articles[0]["href"] if "href" in articles[0].attrs else url
            if not link.startswith("http"):
                link = "https://www.knowfuture.cn" + link
            return {
                "type": "ğŸš€ AI åº”ç”¨åˆ›æ–° / è¡Œä¸šè¶‹åŠ¿",
                "title_zh": title,
                "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                "link": link,
                "time": get_today_date()
            }
    except Exception as e:
        logging.error(f"æŠ“å–çŸ¥æ½œå¤±è´¥: {e}")
    
    # å¤‡ç”¨æºï¼šæ™šç‚¹LatePost
    try:
        url = "https://www.latepost.com/"
        response = requests.get(url, headers=HEADERS, timeout=10, verify=False)
        soup = BeautifulSoup(response.text, "html.parser")
        
        articles = soup.find_all("a", class_="post-title", limit=1)
        if articles:
            title = clean_text(articles[0].text)
            link = articles[0]["href"] if "href" in articles[0].attrs else url
            if not link.startswith("http"):
                link = "https://www.latepost.com" + link
            return {
                "type": "ğŸš€ AI åº”ç”¨åˆ›æ–° / è¡Œä¸šè¶‹åŠ¿",
                "title_zh": title,
                "summary_zh": f"æœ€æ–°åŠ¨æ€ï¼š{title[:50]}...",
                "link": link,
                "time": get_today_date()
            }
    except Exception as e:
        logging.error(f"æŠ“å–æ™šç‚¹ï¼ˆå¤‡ç”¨ï¼‰å¤±è´¥: {e}")
    
    # æ— å†…å®¹æç¤º
    return {
        "type": "ğŸš€ AI åº”ç”¨åˆ›æ–° / è¡Œä¸šè¶‹åŠ¿",
        "title_zh": "ä»Šæ—¥æš‚æ— ã€AIåº”ç”¨åˆ›æ–°/è¡Œä¸šè¶‹åŠ¿ã€‘ç›¸å…³ä¿¡æ¯",
        "summary_zh": "",
        "link": "",
        "time": ""
    }

# ===================== æ„å»ºæ¨é€å†…å®¹ =====================
def build_feishu_content():
    """æ„å»ºé£ä¹¦æ¨é€å†…å®¹"""
    # æŠ“å–5ç±»ä¿¡æ¯
    basic_llm = crawl_basic_llm()
    industry_dynamic = crawl_industry_dynamic()
    ai_tech = crawl_ai_tech()
    llm_ranking = crawl_llm_ranking()
    ai_innovation = crawl_ai_innovation()
    
    # ç»„è£…å†…å®¹
    content = f"ğŸ“® æ¯æ—¥AIç²¾é€‰ï¼ˆ{get_today_date()}ï¼‰\n\n"
    
    # éå†5ç±»ä¿¡æ¯
    for idx, item in enumerate([basic_llm, industry_dynamic, ai_tech, llm_ranking, ai_innovation], 1):
        content += f"{idx}. ã€{item['type']}ã€‘\n"
        content += f"   æ ‡é¢˜ï¼š{item['title_zh']}\n"
        if item["summary_zh"]:
            content += f"   æ‘˜è¦ï¼š{item['summary_zh']}\n"
        if item["link"]:
            content += f"   æ¥æºé“¾æ¥ï¼š{item['link']}\n"
        content += "\n"
    
    return content.strip()

# ===================== é£ä¹¦æ¨é€ =====================
def send_to_feishu():
    """æ¨é€å†…å®¹åˆ°é£ä¹¦"""
    if not FEISHU_WEBHOOK:
        logging.error("æœªé…ç½®é£ä¹¦Webhookï¼")
        return False
    
    try:
        payload = {
            "msg_type": "post",
            "content": {
                "post": {
                    "zh_cn": {
                        "title": f"æ¯æ—¥AIç²¾é€‰ï¼ˆ{get_today_date()}ï¼‰",
                        "content": [[{"tag": "text", "text": build_feishu_content()}]]
                    }
                }
            }
        }
        response = requests.post(
            FEISHU_WEBHOOK,
            data=json.dumps(payload, ensure_ascii=False),
            headers={"Content-Type": "application/json"},
            timeout=10,
            verify=False
        )
        result = response.json()
        if result.get("code") == 0:
            logging.info("âœ… é£ä¹¦æ¨é€æˆåŠŸï¼")
            return True
        else:
            logging.error(f"âŒ æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        logging.error(f"âŒ æ¨é€å¼‚å¸¸: {e}")
        return False

# ===================== ä¸»ç¨‹åº =====================
if __name__ == "__main__":
    logging.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ¯æ—¥AIèµ„è®¯æ¨é€ä»»åŠ¡")
    send_to_feishu()
    logging.info("ğŸ”š æ¨é€ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
